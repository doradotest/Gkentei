１〇重み
２〇バイアス
３〇活性化関数

４〇訓練誤差
５〇最小化
６〇汎化誤差

７〇確率的勾配降下法
８〇逐次学習
９〇ミニバッチ学習
１０〇バッチ学習

１１〇隠れ層のユニット数が増えるほど,複雑な関数を近似する能力が上がる

１２〇順伝播型ニューラルネットワーク
１３ｘ　正解：3全結合層　あなたの回答：4　畳み込み層
１４〇プーリング層

１５ｘ　正解：2　ファインチューニング
１６〇入力層に近い中間層

学習に用いる画像データが十分量集まらないときは,既存のモデルを再利用し,ファインチューニングを行う方法がある.
ファインチューニングを行う際は,ILSVRC で功績を残したネットワークの,学習済みのものを利用するなどする.
CNN の入力層に近い中間層ではタスクごとに比較的大きな差が生まれない汎用的な特徴が学習されるため,このような手法を用いることができる.

１７〇再帰構造
１８〇系列データ
１９〇情報を一時的に記憶させること

RNN（Recurrent Neural Networks）は内部に再帰構造を持つニューラルネットワークの総称であり,
この構造は系列データを扱うために開発された.系列データを扱えるようになったのは,再帰構造によって,情報を一時的に記憶させることができるようになったためである.

２０ｘ正解：2　自己符号化器　あなたの回答：3RNN（Recurrent Neural Networks）
２１〇長い系列を遡るにつれて学習が困難になる

LSTM（Long Short-Term Memory）は自己符号化器の一種である.内部にゲート構造を設けることにより,
プレーンな自己符号化器の抱える長い系列を遡るにつれて学習が困難になるという問題を解決する方法として提案されている.

