〇★教師あり学習は、正解となるラベルデータが存在する場合に用いられる方式であり、【そのラベルを目的変数】という。
Ｘ★教師あり学習は、【説明変数】の種類により回帰と分類の２種類に分けられる。回帰は目的変数が連続値となる。
Ｘ★教師なし学習は、正解ラベルを用いない学習方法であり、クラスタリングや次元削減といったタスクを行う。典型的なものに【ニューラルネットワークを用いた深層学習】がある。
Ｘ★強化学習は、ブラックボックス的な環境の中で行動するエージェントが、得られる報酬を最大化するように学習する方法であり、ルールベースと比較される機械学習の【伝統的な手法】である。

＊DBSCAN法とは（教科書では特に言及はされていないのですが）「特徴量ベクトル間の距離に着眼した手法」つまりデータ間のそれぞれの特徴を基に学習する手法です。
ですから「教師あり学習」ではなく、「教師なし学習」とするのが正しい

〇機械学習を用いずにカテゴライズや数値予測を行う手法の一つとして、ルールベースがある。ルールベースではプログラミングの条件分岐の要領でデータを容易にルール化できるが、
　パラメータの数が増えると記述が困難になる。

・指数関数★★
　　f(x)=2x
　　シグモイド関数　ニューラルネットワークでよく使われる★★★★★★
〇★★★シグモイド関数のグラフの形は、座標点（0, 0.5）を基点として点対象の、S字型曲線である。

次元追加★★★★★★★★★★
a=array([1, 2, 8])
行方向にスラッシング
a[np.newaxis,:]★★★★★★
array([[1, 2, 8]])

a[:, np.newaxis]★★★★★★
array([[1],
       [2],
       [8]])

グリッドデータの生成
m=np.arange(0, 4)★★★★
array([0, 1, 2, 3])
n=np.arange(4, 7)
array([4, 5, 6])

xx, yy = np.meshgrid(m, n)★★★★★★
xx  
array([[0, 1, 2, 3],
       [0, 1, 2, 3],
       [0, 1, 2, 3]])
yy
array([[4, 4, 4, 4],
       [5, 5, 5, 5],
       [6, 6, 6, 6]])


■欠損値の補完
  特徴量の平均値、中央値、最頻値などで補完

■カテゴリ変数のエンコーディング
 文字を数字に

■One-hotエンコーディング
# 英語のa、b、cを1、2、3に変換
〇One-hotエンコーディングでは、たとえば、テーブル形式のデータのカテゴリ変数の列について、取り得る値の分だけ列を増やして、
　各行の該当する値の列のみに1を、それ以外の列には0を入力するように変換する処理をいう。


■特徴量の正規化
特徴量の大きさをそろえる処理
分散正規化　特徴量の平均が0，標準偏差を1となるように変換★★
〇特徴量の正規化とは、たとえば、ある特徴量の値が2桁の数値（数十のオーダ）、別の特徴量の値が4桁の数値（数千のオーダ）のような場合、
　後者のオーダの特徴量が重視されやすくなるため、尺度を揃える処理をいう。
Ｘ分散正規化とは、特徴量の平均が1、標準偏差が0となるように特徴量を変換する処理であり、標準化やz変換と呼ばれることもある。

■最小最大正規化
　特徴量の最小値が0，最大値が1をとるように特徴量を正規化する
　〇★★最小最大正規化とは、特徴量の最小値が0、最大値が1を取るように特徴量を正規化する処理であり、scikit-learnでは、preprocessingモジュールの
　　【MinMaxScalerクラス】を用いて実行することができる。

■サポートベクタマシン




■ランダムフォレスト




■ モデルの評価




■交差検証（クロスバリデーション）★★★★★★★★★★★★




■予測確立の正確さの定量化
予測退会確率の高い順に並べる
為陽性率　負例全件中の負例何件を割合表示
真陽性率　正例全件中の正例何件を割合表示
指標
　ROC曲線　横　為陽性率　縦　真陽性率★★★★★★★★★★★★
　AUC　0.5に近づくほど、正例と負例を区別できない★★★★★★★★★★★★
　　　1に近づくほど、確率が相対的に高いサンプルが正例、低いサンプルが負例になる傾向
　　　
■ハイパーパラメータの最適化

ハイパーパラメータ　人が決める　決定木の個数

★グリッドサーチ
　指定したパラメーターの全ての組み合わせを試す手法。組み合わせの総数分モデルの学習を行うので、探索が終わるのに時間がかかる
ランダムサーチ

■次元削減

■主成分分析

■クラスタリング


■k-means


■階層的クラスタリング




■汎化能力
〇構築したモデルのテストデータセットに対する予測を行い、未知のデータに対する対応能力
Ｘ構築したモデルが持つ未知のデータに対する対応能力を「汎化能力」という。


〇★★分類モデルを構築するには、まず手元のデータセットを学習データセットとテストデータセットに分割する。そして、学習データセットを用いて分類モデルを構築し、
　構築したモデルのテストデータセットに対する予測を評価し、汎化能力を評価する。

〇学習データセットとテストデータセットの分割は、scikit-learnでは、model_selectionモジュールの【train_test_split関数】を用いて実行することができる。


Ｘscikit-learnのインターフェースでは、学習はfitメソッド、予測はestimateメソッドを用いて実行できる。


X, y = boston.data, boston.target 
〇boston.dataには説明変数、boston.targetには目的変数が格納されている。

ポアソン分布　交通事故の発生回数の予測、機械部品の故障予測など、稀に生じる事象についてモデル化したい場合に用いられる、離散型の確率分布である。