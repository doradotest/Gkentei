〇★教師あり学習は、正解となるラベルデータが存在する場合に用いられる方式であり、【そのラベルを目的変数】という。
Ｘ★教師あり学習は、【説明変数】の種類により回帰と分類の２種類に分けられる。回帰は目的変数が連続値となる。
Ｘ★教師なし学習は、正解ラベルを用いない学習方法であり、クラスタリングや次元削減といったタスクを行う。典型的なものに【ニューラルネットワークを用いた深層学習】がある。
Ｘ★強化学習は、ブラックボックス的な環境の中で行動するエージェントが、得られる報酬を最大化するように学習する方法であり、ルールベースと比較される機械学習の【伝統的な手法】である。

＊DBSCAN法とは（教科書では特に言及はされていないのですが）「特徴量ベクトル間の距離に着眼した手法」つまりデータ間のそれぞれの特徴を基に学習する手法です。
ですから「教師あり学習」ではなく、「教師なし学習」とするのが正しい

〇機械学習を用いずにカテゴライズや数値予測を行う手法の一つとして、ルールベースがある。ルールベースではプログラミングの条件分岐の要領でデータを容易にルール化できるが、
　パラメータの数が増えると記述が困難になる。

・指数関数★★
　　f(x)=2x
　　シグモイド関数　ニューラルネットワークでよく使われる★★★★★★
〇★★★シグモイド関数のグラフの形は、座標点（0, 0.5）を基点として点対象の、S字型曲線である。

次元追加★★★★★★★★★★
a=array([1, 2, 8])
行方向にスラッシング
a[np.newaxis,:]★★★★★★
array([[1, 2, 8]])

a[:, np.newaxis]★★★★★★
array([[1],
       [2],
       [8]])

グリッドデータの生成
m=np.arange(0, 4)★★★★
array([0, 1, 2, 3])
n=np.arange(4, 7)
array([4, 5, 6])

xx, yy = np.meshgrid(m, n)★★★★★★
xx  
array([[0, 1, 2, 3],
       [0, 1, 2, 3],
       [0, 1, 2, 3]])
yy
array([[4, 4, 4, 4],
       [5, 5, 5, 5],
       [6, 6, 6, 6]])


■欠損値の補完
  特徴量の平均値、中央値、最頻値などで補完

■カテゴリ変数のエンコーディング
 文字を数字に

■One-hotエンコーディング
# 英語のa、b、cを1、2、3に変換
〇One-hotエンコーディングでは、たとえば、テーブル形式のデータのカテゴリ変数の列について、取り得る値の分だけ列を増やして、
　各行の該当する値の列のみに1を、それ以外の列には0を入力するように変換する処理をいう。


■特徴量の正規化
特徴量の大きさをそろえる処理
分散正規化　特徴量の平均が0，標準偏差を1となるように変換★★
〇特徴量の正規化とは、たとえば、ある特徴量の値が2桁の数値（数十のオーダ）、別の特徴量の値が4桁の数値（数千のオーダ）のような場合、
　後者のオーダの特徴量が重視されやすくなるため、尺度を揃える処理をいう。
Ｘ分散正規化とは、特徴量の平均が1、標準偏差が0となるように特徴量を変換する処理であり、標準化やz変換と呼ばれることもある。

■最小最大正規化
　特徴量の最小値が0，最大値が1をとるように特徴量を正規化する
　〇★★最小最大正規化とは、特徴量の最小値が0、最大値が1を取るように特徴量を正規化する処理であり、scikit-learnでは、preprocessingモジュールの
　　【MinMaxScalerクラス】を用いて実行することができる。

■サポートベクタマシン




■ランダムフォレスト




■ モデルの評価

指標
・適合率（precision）　tp/(tp+fp)　正例予測したデータのうち、実際の正例の割合★★★★
・再現率（recall）     tp/(tp+fn) 実際の正例のうち、正例と予測したもの★★★★
・F値（F-value）　　適合率と再現率の調和平均　2/（1/適合率　＋　1/再現率）＝2＊適合率＊再現率/（適合率＋再現率）★★★★★★★★
・正解率（accuracy）予測と実績が一致したもの　(tp+tn)/(全体)

[混合行列]　予測と実績のクラスレベルの組合せを集計した表

tp;正例として予測して、実際に正例　true positive　〇

fp:正例として予測して、実際に負例　false positive　　ｘ

fn;負例として予測して、正例　true nagativeｘ

tn;負例として予測して、負例　false nagative

＊混同行列で計算する適合率、再現率、F値、正解率は、（回帰ではなく）分類モデルの評価指標★★★★
＊適合率は予想するクラスをなるべく間違えないようにしたいときに重視する指標である。★★★★★★★★★★★★
＊正解率は、正例か負例かを問わず、予測と実績が一致したデータの割合を表す。正解率は(tp+tn)/(tp+fp+fn+tn）で計算することができる。★★★★★★★★★★★★
＊F値は、適合率と再現率の【調和平均【として定義される。F値は2*適合率*再現率/(適合率+再現率)で計算することができる。★★★★
Ｘ★★機械学習を用いて構築した【回帰モデルの良し悪し】を評価する指標に適合率、再現率、F値、正解率がある。これらは【混同行列】から計算する。
〇適合率、再現率、F値、正解率は、機械学習を用いて構築した【分類モデルの良し悪し】を評価する指標であり、混同行列から計算する。

〇一般的に適合率と再現率はトレードオフの関係にある。つまり、どちらか一方の指標を高くすると、もう一方の指標は低くなる。

ｘ適合率は、たとえば病院の検診で病気の見逃し・取りこぼしがないようにしたい場合などに重視される、網羅性に関する指標である。
ｘ再現率は、間違えることをできるだけ避けたい場合に重視する指標である。一般的に適合率と再現率はトレードオフの関係にある。


■交差検証（クロスバリデーション）★★★★★★★★★★★★

データセットを学習用とテスト用に分割する処理を繰り返し、モデルの構築と評価を繰り返し、
モデルの構築と評価を複数回数行う処理
〇★★学習とモデルの評価は、学習データセットとテストデータセットの分割を繰り返し、モデルの構築と評価を複数回行う方法で行うこともできる。この方法を交差検証という。
ｋ分割交差検証　9割学習用データ、1割テストデータにする処理を10回行う＝10分割交差検証

層化ｋ分割交差検証　目的変数のクラスの割合が一定


■予測確立の正確さの定量化
予測退会確率の高い順に並べる
為陽性率　負例全件中の負例何件を割合表示
真陽性率　正例全件中の正例何件を割合表示
指標
　ROC曲線　横　為陽性率　縦　真陽性率★★★★★★★★★★★★
　AUC　0.5に近づくほど、正例と負例を区別できない★★★★★★★★★★★★
　　　1に近づくほど、確率が相対的に高いサンプルが正例、低いサンプルが負例になる傾向
　　　
■ハイパーパラメータの最適化

ハイパーパラメータ　人が決める　決定木の個数

★グリッドサーチ
　指定したパラメーターの全ての組み合わせを試す手法。組み合わせの総数分モデルの学習を行うので、探索が終わるのに時間がかかる
ランダムサーチ

■次元削減
低次元（小）-＞高次元（多）
　　「★★圧縮」するタスク
　　　計算量の削減は、（目的変数ではなく）説明変数の次元数を減らすこと★★
Ｘ次元削減は、データが持っている情報をなるべく損ねることなく次元を削減してデータを★★展開(〇圧縮)するタスクである。
Ｘ次元削減の主目的は目的変数の数を減らして計算量を削減することであるが、説明変数の削減を行うことは少ない。これは、モデルの精度を確保するためである。
特徴量の次元削減、なるべくデータの情報を落とさずに、少ない特徴量でデータを表現するために用いられる

■主成分分析
高次元のデータに対して分散が大きくなる方向（データが散らばっている方向）を探して、元の次元と同じかそれよりも低い次元にデータを変換する手法
分散・ばらつきが大きくなる方向を探して、元の次元と同じか、低い次元にデータを変換・圧縮する、次元削減の一手法★★★★
主成分分析は、scikit-learnのdecompositonモジュールのPCAクラスを用いて実行することができる。★★★★★★★★★★★★
2次元のデータに対して主成分分析を行い、どちらも重要であると確認できた場合には、1次元に次元削減できる可能性が高いとはいえません。★★

Ｘ主成分分析とは、高次元のデータに対して標準偏差が小さくなる方向を探して、元の次元と同じかそれよりも高い次元にデータを変換する手法である。

Ｘ2次元のデータに対して主成分分析を行い、新たな2変数に変換した結果、第一主成分と第二主成分がともに重要であると確認できた場合、1次元に次元を削減できる可能性が高い。

■クラスタリング


■k-means


■階層的クラスタリング

凝集型　階層的クラスタリングのうち、似ているデータをまとめていく★★
〇凝集型の階層的クラスタリングは、まず似ているデータをまとめて小さなクラスタを作り、次にそのクラスタと似ているデータをまとめ、最終的にデータが1つのクラスタにまとめられるまで処理を繰り返すクラスタリング手法である。

〇★★★★凝集型の階層的クラスタリングは、【まず似ているデータをまとめて小さなクラスタを作り】、次にそのクラスタと似ているデータをまとめ、
　　　最終的にデータが1つのクラスタにまとめられるまで処理を繰り返すクラスタリング手法である。
Ｘ凝集型の階層的クラスタリングは、最初にすべてのデータが一つのクラスタに所属していると考え、順次クラスタを分割していく手法である。

〇分割型の階層的クラスタリングは、【最初にすべてのデータが1つのクラスタに所属している】と考え、順次クラスタを分割していくアプローチであり、
　　凝集型より利用頻度が多いとは言えません。
分割型　似ていないデータ同志を分割していく手法であり計算量が多いため、凝集型より利用頻度が多いとは言えません★★
Ｘ分割型の階層的クラスタリングは、最初にすべてのデータが1つのクラスタに所属していると考え、順次クラスタを分割していくアプローチであり、
　　一般的に凝集型に比べて計算量が少なく精度が高いためよく用いられる。



■汎化能力
〇構築したモデルのテストデータセットに対する予測を行い、未知のデータに対する対応能力
Ｘ構築したモデルが持つ未知のデータに対する対応能力を「汎化能力」という。


〇★★分類モデルを構築するには、まず手元のデータセットを学習データセットとテストデータセットに分割する。そして、学習データセットを用いて分類モデルを構築し、
　構築したモデルのテストデータセットに対する予測を評価し、汎化能力を評価する。

〇学習データセットとテストデータセットの分割は、scikit-learnでは、model_selectionモジュールの【train_test_split関数】を用いて実行することができる。


Ｘscikit-learnのインターフェースでは、学習はfitメソッド、予測はestimateメソッドを用いて実行できる。


X, y = boston.data, boston.target 
〇boston.dataには説明変数、boston.targetには目的変数が格納されている。

ポアソン分布　交通事故の発生回数の予測、機械部品の故障予測など、稀に生じる事象についてモデル化したい場合に用いられる、離散型の確率分布である。