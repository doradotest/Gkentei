:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
4.人工知能の定義
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
人工知能（Artifical  Intelligence）
	推論、認識、判断など人間のような知的なふるまいをする人工的に作られた機械のこと（計算機科学の一分野）
	★ジョン・マッカーシーがダートマス会議で使った言葉（1956）ダートマス大学
人工知能のおおまかな分類
	・ルールベースの制御（制御工学）
	・古典的な人工知能（人工知能）
	・機械学習（人工知能）
	・ディープラーニング（人工知能）
	自らの判断で自律的に振る舞うエージェントに基づき分類
ルールベースの制御
	あらかじめ決められた設定に基づいてふるまう
	冷蔵庫、エアコン、炊飯器など
古典的な人工知能
	多くのデータを活用して、複雑なふるまいが可能
	→医療診断プログラム、掃除ロボット（ルンバ）　など
機械学習を取り入れた人工知能
	大量のデータからルール・関係性などを学習する
	→検索エンジン、スパムメール判定、交通渋滞予測　など
ディープラーニング（人工知能）
	何に注目すればいいか（特徴量）を自らで判断し学習する
	→画像認識、音声認識、機械翻訳など
機械学習（犬の写真）
	何に注目するべきか（特徴）を教える必要があった
	→目、口、耳など犬としての特徴を設定する必要があった
AI効果
	人工知能とみなされていた技術で課題が実現されるとそれはただの自動化であって人工知能ではないと
人工知能とロボットの違い
	ロボットは予め決められた動作を行うのが基本になる
	→産業用のロボット、運搬ロボット、などの装置・機器（体）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
5.人工知能
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
世界初の汎用コンピュータ
	アメリカで世界初の汎用コンピュータ
	【エニアック（ENIAC）】をペンシルバニア大学が開発(1946)
人工知能研究のブームと冬の時代
	第一次人工知能ブーム：1950年代後半～1960年代
	第二次人工知能ブーム：1980年代
	第三次人工知能ブーム：2000年代～
第一次人工知能ブーム（推論・探索の時代）
	迷路の謎解きなど簡単な問題（トイ・プロブレム）はコンピュータが解けるようになった
第二次人工知能ブーム（知識の時代）
	特定の専門分野の知識をもとに、専門家のように様々な推論・判断を行うシステム（エキスパートシステム）
	大量の知識を与えるコスト・管理するコストが大きかった
第三次人工知能ブーム（機械学習・特徴表現学習の時代）
	大量のデータ（ビックデータ）を用いることで人工知能が自ら知識を獲得する機械学習が実用化された
	→データの特徴を定量的に表したもの（特徴量）を人工知能が自ら学習するディープラーニングが登場

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
6. 探索・推論（探索木）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
探索木
	１つ１つの要素をノードと呼ぶ
幅優先探索
	初期のノードから見て近いノードから探索を行う
	探索したノードをすべて記憶しなければならないためメモリが必要
	最短経路を見つけ出すことができる(ステップ数が少なければ)
	深い場所に解答がある場合、途中で記憶容量がパンクする
深さ優先探索
	あるノードから行き止まりになるまで行き、行き止まりになったら、
	１つ手前のノードまで戻りまた行き止まりになるまで探索を行う
	記憶するものが少なくメモリが少なくて済む
	最短経路とは限らない
ハノイの塔
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
7. 探索・推論（行動計画とボードゲーム）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ロボットの行動計画
	プランニング
	前提条件、行動、結果を組み合わせて記述
	プランニングシステム、【STRIPS(ストリップ)】
SHRDLU(シュルドゥル)
	テリー・ウィノグラード
	コンピュータに英語で指示を出すと端末の画面上の物体を動かしてくれる（積み木）
	研究成果はCycプロジェクトに引き継がれていく
ボードゲーム
	組み合わせの数はオセロ、チェス、将棋、囲碁の順
	コスト（ステップ数）の概念を取り入れて探索を効率化しようとする
Mini-Max法
	自分が手を打つときは自分が有利に、相手が手を打つときは相手が不利
	あらかじめどのような手が有利か不利かをスコア化（コスト）
	すべての手を計算すると時間がかかるデメリットがある
αβ法
	スコアが低い手を切り捨てる
	αカット、βカット
	切り捨てた中に実は最善の手があるかもしれない
	すべてのパターンを調べる方法（ブルートフォース）の方が勝てる確率は上がる
	Mini-Max法、αβ法では「相手は自分のスコアが小さい手をとる」
	Negaとつくと、「相手が相手自身のスコアが高くばるような手をとる」
ヒューリスティックな知識
	経験による知識やルールのこと
	精度は高くないが、ある程度正しい答えを出せる方法
モンテカルロ法
	ゲームの終盤まで進んだら、あらかじめ決めたスコアを無視して、AI同士でランダムな手を打って、結局（プレイアウト）させる
	複数回プレイアウトすると、勝ちやすい手などがわかりスコアをつけていく
	人間がスコアをつけるよりも理にかなっている
	DeepMind社が開発した人工知能の囲碁プログラムAlphaGoがプロに勝った(2016年)
	ディープラーニングの技術を使っている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
8. 知識表現
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
人工無能
	チャットボットなどと呼ばれるプログラム
	人間との形式的な会話のために作られたロボット
ELIZA（イライザ）
	１９６４年、チャットボットの元祖となるELIZAが開発される
イライザ効果
	コンピュータとのやりとりを、人間とやりとりしていると錯覚する現象のこと
知識ベースの構築とエキスパートシステム
	DENDRAL（世界初のエキスパートシステム）（読-デンドラル）
	1960年代の人工知能に関するプロジェクト
	→未知の有機化合物を特定するエキスパートシステム
	知識ベースと推論エンジンにより構成
	・【マイシン（MYCIN）】1970年代
	スタンフォード大学で開発されたエキスパートシステム
	→感染した細菌を判断するためのシステム
	専門医＞マイシン（MYCIN）＞普通医
知識獲得のボトルネック
	専門家の暗黙知を聞き出すのが困難
意味ネットワーク（セマンティックネットワーク）
	「is-a」の関係：●は●である
	「part-of」の関係：●は●の一部である
	「has-a」の関係：●は●を持っている
	・is-a　継承関係を表す
	・part-of　属性を表す。本体と部品の関係
ナレッジグラフ
　　　意味ネットワークの中でも、インターネット上などから半自動で構築しているもの
	LOD（Linked　Open Data）
		ネットワーク形式で公開されているデータセット
		地方公共団体などではこのデータを活用したサービスを展開しているのも見られる
	Mercari
		ナレッジグラフを用いてレコメンドシステムにその理由を示すために使っている
Cycプロジェクト
	一般常識もデータベース化して、人間と同様の推論システムを構築することを目的とするプロジェクト
	→1984年に開始され、30年以上も入力作業は続いている
オントロジー
	概念化の明示的・形式的な仕様、概念を体系化する学問
	意味ネットワークの単語と単語をつなぐつなぎ方のルールのこと
	オントロジーに従った記法で統一すれば共有できる
	→意味ネットワークを作成しても作者によって書き方などが異なるため他人が作成したものを利用できない
	→いずれにせよ、記述する人によって記述の方法や記述の粒度がまちまちになると、そもそもの知識の共有化や再利用が難しくなるため、記述ルールの標準化が必要
概念間の関係
	→推移律（AとB、BとCに関係が成立するとき、AとCも成立）
	・part-isの関係：全体と部分の関係、タイヤは車の一部
	→part-isの関係には5種類以上の意味があり複雑
	本来なら分けて関係を示したいが分けられていない

オントロジーの構築
　  ヘビーウェイトオントロジー
	    人間がしっかりと考えて知識を記述していくという考え方
		Cycプロジェクトが長年行っている
　  ライトウェイトオントロジー
	   コンピュータが情報を理解して、自主的に関係性を見つけようという考え方
ライトウェイトオントロジー
　  ウェブマイニング
	  ウェブ上にあるデータ分析して有益な情報を抽出する手法
　  データマイニング
	  ビックデータを分析して有益な情報を抽出する手法
ワトソン
	IBMが開発した質問応答（QUESTION-ANSWERING）のシステム
	→2011年アメリカのクイズ番組で対戦して優勝
	→蓄積されている大量のデータから最適な回答を見つけ出している
	IBMは人工知能ではなく、拡張知能と呼んでいる
	→言葉を理解していない（ライトウェイトオントロジー）
東ロボくん
	東大の入試を突破することを目標にした人工知能
	→2011年から2016年まで続けられた
	→偏差値57を突破したが、東大入試を突破することは現在では不可能であると判断されプロジェクト終了した
	→読解力が必要であり、知識だけで合格が困難であった

セマンティック・ウエブ
	メタデータを活用して、コンピュータがウェブ情報を解釈し処理する技術や考え方のこと
	→メタデータ：データに付与するデータのこと
	「五十嵐すず」というデータに対して「著者」というデータを付与することでコンピュータが理解しやすくなる
	例）写真の情報(Exif情報)
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
9. 機械学習・深層学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
レコメンデーションエンジン
	→ユーザーが興味がありそうなものを提案するシステム　アマゾン、YouTube
スパムフィルター
	→迷惑メールを見つけ出すためのシステム
自然言語処理
	自然言語をコンピュータに処理させること
統計的自然言語処理
	確率推論や統計学を使って、自然言語を処理すること
　  コーパス（自然言語の文章を構造化したもの、対訳データ）を活用することで文章を正しく読み取れる確率が上がる
ニューラルネットワーク
	人間の脳を模倣してモデル化されており、入力層、中間層、出力層の複数ニューロンから構成される非常に複雑な構成
ディープラーニング
	ニューラルネットワークを多層化したもの
	→多層化しても学習精度が思ったように上がらなかった
	→誤差逆伝播法などを取り入れることで学習精度があがru
	自己符号化器（オートエンコーダ）、
	活性化関数などを活用することで問題がすこしずつ解決
ILSVRC（ImageNet　Large　Scale　Visual Recognition　Challenge）
	画像認識の精度を競う大会
機械学習
	データの特徴を定量的に表した特徴量を人間が設定
	→人間の経験と勘によって特徴量を設定していた
	→2015年から人間の認識率を超える精度を誇るようになる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
10. 人工知能分野の問題①
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
トイ・プロブレム
	人工知能は複雑な問題を解くことができない
	ルールがシンプルなもの（ゲーム）しか解けないという用語
フレーム問題
	処理能力が限られている人工知能では、現実に起こりうる全ての問題に対応することができないこと
	問題の枠に囚われてしまい、枠の外を考慮することはロボットには難しい
	自動運転車は事故を必ず起こさないようにプログラムすると、走行速度が遅すぎてうまく機能しない問題
チューリングテスト
	機械は思考できるのかという目的から行われるテスト
	→アラン・チューリングが提案したもの
	→人間が審査員となり、人工知能と人間と会話を行うどちらが人間なのかを当てるテスト
	→人工知能は人間っぽいふるまいを行う
ローブナーコンテスト
	最も人間に近い人工知能を決める
	チューリングテスト形式のコンテストのこと
強いAI
	人間のようなふるまい（思考・判断など）をするAI
	汎用型人工知能
弱いAI
　　特定の問題を解決AI
   特化型人工知能
ジョン・サールが論文で発表したAIの分類
	→弱いAIは現実可能でも、強いAIは不可能であると考えている
	→「中国語の部屋」という思考実験を使って説明されている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
11. 人工知能分野の問題2
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
シンボルグラウンディング問題
	シンボル（記号）を現実世界の意味とどのように結びつけるかという問題のこと
	→スティーブン・ハルナッドによって推論された
	→概念と記号を結びつけることは難しい
	人間の場合、縞模様の馬はシマウマだと知っていれば初見でもシマウマかもしれないと考えることができる
	→記号のシマウマと実際のシマウマが結びつく
	→コンピュータの場合、記号のシマウマと実際のシマウマを結びつけることができない
身体性
	機械が身体を持つことで高度な人工知能になるという考え方
	→身体を通して得られる情報も必要になってくる
知識獲得のボトルネック
	人間が持っている大量の一般常識をコンピュータが獲得することは極めて難しいこと
	・機械翻訳
　		・ルールベース機械翻訳：1970年代後半から
　		・統計的機械翻訳：1990年代から
　　　	性能は一気に改善されたが、意味を理解しているわけではないので翻訳が大変になる
ニューラル機械翻訳
　　ニューラルネットワークを活用した翻訳
	→精度が一気に上がり、実務でも使われるようになってきた
	TOEIC900点以上の人間と同じくらいの翻訳力をもつ
特徴量設計
	機械学習では、データの特徴を定量的に表した特徴量の選び方が大切になる
	→家賃を予想する場合、最寄り駅までの距離や築年数などが特徴量として考えられる
特徴表現学習
	特徴量をコンピュタ自身が見つけること
	→ディープラーニングも特徴表現学習の１つ
	→家賃などの情報を与え、機械が特徴量を抽出する
	人間は機械が何を特徴量にしたのかよくわからない
シンギュラリティ（技術的な特異点）
	人工知能の進化により、人類の知能を越える転換点のこと
	→人工知能が自分よりも賢い人工知能を作れるようになることで加速度的に人工知能の性能は飛躍すると予想
	→レイ・カーツワイルは、2045年にシンギュラリテイーが訪れることを予想している
	人工知能が人類よりも賢くなるのは2029年と予想している
	→シンギュラリティーが起きる年と人工知能が人類よりも賢くなる年は異なる
	→イーロンマスク、ビル・ゲイツは人工知能は脅威であると考えている
<<<<<<< HEAD
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
12. 学習の種類
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
教師あり学習
	正解が与えられたデータをもとに、ルールやパターンを学習して手法のこと
	→分類問題、回帰問題　を解くときに使われる手法になる
	・分類問題：複数のカテゴリに分ける問題
	・回帰問題：連続値（体重、温度など）を予測する問題
分類問題
　	迷惑メールか普通のメールかを分類する　など
回帰問題
　	来店数や温度、気温からビールの売上を予測する　など
教師無し学習
	クラスタリング
		購入データから顧客層に分ける　など
	次元削減
		数学、理科を理系、社会、国語を文系にまとめる　など
　　　　扱うべきデータが減り計算、分析しやすくなる
強化学習
　	与えられた環境下で、報酬が最大となる行動を行うように学習していく手法のこと
	→テレビゲームで点数が高くなるような行動をするために学習していく
	（最適な行動は何か）

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
13. 教師あり学習（線形回帰・ロジスティック回帰）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
教師あり学習の代表的な手法
・ランダムフォレスト・ブースティング
・サポートベクターマシン（SVM）・k近傍法（ケイきんぼうほう）
・ニューラルネットワーク・ディープラーニング
・時系列分析
線形回帰
	説明変数：説明するための変数、原因になっている変数
　	目的変数：予測したい変数、結果になる変数
	→アイスクリームの売上（目的変数）と気温（説明変数）
最小二乗法
　	実際の値と予想値との差の2乗値が最小になるような重みを決定する方法
擬似相関
　　　因果関係がないのに相関に見えること
重回帰分析の注意点
	多重共線性
　		相関係数が高い説明変数があること
		→計算が不安定になってしまう特徴がある
正則化とは過学習を防ぐために使われるテクニック・手法
　	過学習は訓練データを学習しすぎて
　	訓練データ以外のデータに対して精度が悪くなってしまうこと
------------------------------------
ロジスティック回帰（分類問題）
　	複数の説明変数から、ある事象が発生する確率を予想する手法
	→複数の説明変数から、再検査が必要かどうか　な
	ジグモイド関数を用いて計算されている
		→結果が2択の時に用いられる
	→結果が3択以上の場合はソフトマックス関数を利用する
　	複数のクラスに分類することができる（多クラス分類）

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
14.教師あり学習（ランダムフォレスト・ブースティング）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
決定木
	データを分割するルールを次々と作成していくことにより、分類を実行するアルゴリズム
	分類木：試合の勝ち負け　など
	回帰木：住宅の価格　など
	根ノード
	葉ノード
	分岐が多くなりすぎると1つの葉に１つのデータが対応してしまう（過学習）
	→過学習を防ぐためにも、木の深さ・幅に気を付ける必要がある
	情報利得が最大化になるように分岐させる
	・情報利得：分割前の不純度から分割後の不純度を引いたもの
				情報利得＝親ノードでの不純度　-　子ノードでの不純度
	・不純度：１つのノードに異なるクラスのサンプルがどれだけ含まれているかという割合
			　ジニ不純度★　各ノードに間違ったクラスに振り分けられてしまう確率
		エントロピー、ジニ係数によって計算される
	〇決定木でデータを分割する時は、データの分割によってどれだけ得をするかについて考える。これを情報利得と呼ぶ。
	　　情報利得は親ノードの不純度から子ノードの不純度を差し引いたものとして定義される。
	〇決定木などで利用される木と呼ばれるデータ構造は頂点であるノード（利用回数、利用間隔）とそれらを結ぶエッジ（5回以上、5回未満）から構成される。★★
	　　木の最下部にあり子ノードを持たないノードはリーフと呼ばれる。
	Ｘ情報利得は【子ノードの不純度から親ノードの不純度を差し引いたもの】として定義される。 不純度の指標としてはジニ不純度、エントロピー、分類誤差などが用いられる。
	Ｘ決定木などで利用される木と呼ばれるデータ構造はノードとそれらを結ぶエッジから構成される。木の最上部にあり親ノードを持たないノードは葉ノードと呼ばれる。

		from sklearn.datasets import load_iris
		from sklearn.model_selection import train_test_split
		from sklearn.tree import DecisionTreeClassifier　＜---重要★★
		# Irisデータセットを読み込む
		iris = load_iris()
		X, y = iris.data, iris.target
		# 学習データセットとテストデータに分割する
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
		# 決定木をインスタンス化する (木の最大深さ=3)
		tree = DecisionTreeClassifier(max_depth=3)
		# 学習
		tree.fit(X_train, y_train)

		決定木の描画
		　pip install pydotplus★★★★★★★★

		from pydotplus import graph_from_dot_data
		from sklearn.tree import export_graphviz
		# dot形式のデータを抽出
		dot_data = export_graphviz(tree, filled=True,
		                rounded=True,
		                class_names=['Setosa',
		                     'Versicolor',
		                     'Virigica'],
		                feature_names=['Speal Length',
		                               'Spal Width',
		                               'Petal Length',
		                               'Petal Width'],
		                out_file=None)
		# 決定木のプロットを出力
		graph = graph_from_dot_data(dot_data)
		graph.write_png('tree.png')

		# 予測
		y_pred = tree.predict(X_test)★★★
		y_pred

ランダムフォレスト
　	ランダムに選ばれた訓練データで学習した決定木を複数作成し、決定木で予想された結果の多数決でクラスを決める手法
	→回帰では平均値を求める
	→精度の悪いものがあったとしても複数組み合わせると精度が高いものになるという考え方にもとづく
	〇scikit-learnでランダムフォレストを実行するにはensembleモジュールのRandomForestClassifierクラスを使用する。
　		パラメータで決定木の個数を指定することもできる。
	〇★★ランダムフォレストは、ブートストラップデータを用いて決定木を構築する処理を複数回繰り返し、
　　　各木の推定結果の多数決や平均値により分類・回帰を行う手法であり、アンサンブル学習の1つである。
	〇ランダムフォレストは、データのサンプルと特徴量をランダムに選択して決定木を構築する処理を複数回繰り返し、
　		各木の推定結果の多数決や平均値により分類・回帰を行う手法である。

		from sklearn.ensemble import RandomForestClassifier
		# ランダムフォレストをインスタンス化する
		forest = RandomForestClassifier(n_estimators=100, random_state=123)
		# 学習
		forest.fit(X_train, y_train)
		# 予測
		y_pred = forest.predict(X_test)
		y_pred

アンサンブル学習
　	別々の学習器を組み合わせる学習器を作ること
	→バギング、ブースティングという手法がある
	・バギング
　　	複数のモデルを並行的に学習させていく方法（復元抽出）
		→ランダムフォレストはバギングの１つで学習時間が短いのが特徴
	ブースティング
		複数のモデルを直列的に学習させていく方法
		→前に作成したモデルを修正して新しいモデルを作成していく
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
15. 教師あり学習（サポートベクターマシン・k近傍法）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
サポートベクターマシン（SVM）
	主に分類で使われるアルゴリズム
	→マージン最大化する線を求めることで分類を行う
	マージン
　		線とクラスの各データの距離のこと
	→数式を使って求めることができる
	ハードマージン
　		マージンの中にデータが入ることが禁止
	ソフトマージン
　		マージンの中にデータが入ることを許容
	→スラック変数：どの程度強要するか調整
	〇サポートベクタマシンは、【分類・回帰だけでなく】外れ値検出にも使えるアルゴリズムであり、直線や平面などで分離できないデータであっても、
　		高次元の空間に写して線形分離することにより分類を行うことを可能にする。
	Ｘサポートベクタマシンは、分類のみに適用できるアルゴリズムである。直線や平面で線形分離できないデータも高次元の空間に写像して線形分離することで分類を行うことを可能にする。
	Ｘサポートベクタマシンで決定境界を求める際にマージンを最小にする理由は、決定境界がサポートベクタから近くなり、多少のデータが変わっても誤った分類を行う可能性を低くできると期待できるからである。
	〇サポートベクタマシンは、分類・回帰だけでなく外れ値検出にも使えるアルゴリズムであり、直線や平面などで分離できないデータであっても、
　		高次元の空間に写して線形分離することにより分類を行うことを可能にする。
	サポートベクタマシンの決定境界は、マージンを（「最小」ではなく）最大にすることで求めます。
		ハイパーパラメーターのCの値が小さいほど、マージンは大きくなる

カーネル法を組み合わせることで直線で分類できないものでも分類ができる
 	次元を拡張させる関数をカーネル関数という
	→カーネルトリックと呼ばれる方法を使うことで
　		計算を楽にすることができる（次元が増えると計算が大変）
k近傍法
	分類に使われる手法の１つ
	→未知のデータが与えられたときに近くにあるデータからクラスを分類していく手法のこと
	→ｋには個数が入る
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
16. 教師あり学習（ニューラルネットワーク・時系列分析）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ニューラルネットワーク
	人間の神経回路（ニューロン）を模倣した数理モデル
単純パーセプトロン
　	1958年に提案されたニューラルネットワーク
	→入力層と出力層からなる
	入力値の重要度を数値化したものを重みという
	モデルの自由度を上げるためにバイアス(b)を使用する
	活性化関数 次の層に渡す値を調整する関数をという
		シグモイド関数
		ソフトマックス関数
		ReLU関数　　など
多層パーセプトロン
	→非線形分類が可能になった
	→入力層、隠れ層、出力層に分かれている
	層が増えることで重みも増える
	→誤差逆伝播法を活用して重みを調整
	誤差逆伝播法
	出力層から入力層にかけて重みを調整する方法のこと
自己回帰モデル（ARモデル）
	回帰問題に使われる手法（時系列系のデータ）
	→過去のデータを使ってある時点の値を予測するモデル
ベクトル自己回帰モデル（ＶＡＲモデル）
　自己回帰モデルを多変量に拡張したもの
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
17. 教師なし学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

クラスタリング（クラスタ分析）
	データをグループ（クラスタ）分けする手法のこと
	ある基準を設定してデータ間の類似性を計算し、データをクラスタ（グループ）にまとめるタスク
	→大量のデータをコンピュータが自動的に分類してくれる
	グループ分けされたものは人間が見て解釈する必要がある
	Ｘクラスタリングは、ある基準を設定してデータ間の類似性を計算し、データをクラス分類するタスクであり、教師あり学習の典型的なタスクである。
	Ｘクラスタリングにおいては、得られたクラスタの妥当性に絶対的な答えがある。
	〇クラスタリングは、ある基準を設定してデータ間の類似性を計算し、データをクラスタ（グループ）にまとめるタスクであり、教師なし学習の典型的なタスクである。
	・K-means法
	・ウォード法
	・主成分分析
	・ｔ-ＳＮＥ法
	・協調フィルタリング
K-means法（階層なしクラスタリング）
	あらかじめ設定したクラスタの数（k個）にデータを分類方法
	→重心を活用して分類を行っていく方法
	〇k-meansは、【最初にランダムにクラスタ中心を割り当て】、クラスタ中心を各データとの距離を計算しながら修正し
	  、最終的なクラスタ中心が収束するまで再計算を行いクラスタリングする手法である。
	Ｘk-meansは、scikit-learnのclusterモジュールのAgglomerativeClusteringクラスを用いることによって実行することができる。★★
	〇k-meansは、scikit-learnのclusterモジュールのKMeansクラスを用いることによって実行することができる。
	〇k-meansは最初にランダムにクラスタ中心を割り当て、クラスタ中心を各データとの距離を計算しながら修正し、最終的なクラスタ中心が収束するまで再計算を行いクラスタリングする手法である。
	〇クラスタリングにおいては、得られたクラスタの妥当性に絶対的な答えはない。

		from sklearn.cluster import KMeans
		# クラスタの数を3とするKMeansのインスタンスを生成
		km = KMeans(n_clusters=3, init='random', n_init=10, random_state=123)　　　#n_initはk-meansの実行回数★★
		# KMeansを実行
		y_km = km.fit_predict(X)

ウォード法（階層ありクラスタリング）
	似ている組み合わせから順番にクラスター化していく方法
	途中経過を樹形図（テンドログラム）で表すことができる
主成分分析（PCA）
　	データの特徴量の関係性（相関関係）から変数をまとめてより少ない変数を要約すること
	→データを要約することを次元削減といい、計算量を減らせる
	→まとめられた特徴量を主成分と呼ぶ

	英語、理科、数学、社会、国語のテストを実行
	→それぞれの科目を分析して２つの成分にする場合、理系と文系という風に分けることができる
次元削減の方法
	主成分分析以外にもｔ分布を使って次元削減する
	t-SNE法、特異値分解（SVD）、多次元尺度構成法（MDS）が有名
協調フィルタリング
	レコメンデーションで使われる手法の１つ
	→AMAzonなどECサイトでおススメが表示される仕組みのこと
	→自分と類にしたユーザーが購入した商品を薦める
	　動画配信ならば類にしたユーザーが見た動画を薦める
	→ある程度のデータが必要になる（コールドスタート問題）
	・コンテンツベースフィルタリング
	　	商品の特徴を利用して類にした商品を薦める方法
		→コールドスタート問題を回避することができる
		→それぞれにメリットとデメリットが存在する
トピックモデル
	クラスタリングで使われるモデルの１つ
	→K-means法などと異なり複数のクラスタに分類が可能
	　潜在的ディリクレ分配法（LDA）が有名
	→ニュース記事を複数のカテゴリに分類したりレコメンダシステムなどで活用したりできる
	・ソフトクラスタリング
	　	各データが１以上のクラスタに所属するようなクラスタリング
	・ハードクラスタリング
	　	各データが１つのクラスタに所属するようなクラスタリング
半教師あり学習
　	教師なし学習と教師あり学習を組み合わせた学習
	→少量のラベル付きデータを学習し、
　		ラベルがついていない大量のデータを学習するアプローチ
	→大量のラベル付きデータを集めるのはコストがかかりすぎる
		ラベル：データに付与される正解かどうかを表す情報

