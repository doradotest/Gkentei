:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ベイズの定理
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
観測された事象（結果）が起こったもとで、背後に潜む事象（原因）がどうであるかの確率を表すのに役立つ
事後確率
事前確率　結果がまだわからない状態P(A)
尤度（ゆうど）　右辺のP(B|A)は結果に対する原因のもっともらしさ
	確率モデルにおいて、想定するパラメータが具体的な値を取る場合に観測されたデータが起こり得る確率のこと
	データからモデルのパラメータを推定する方法の１つ
ナイーブベイズ
	分類に使われる重要な機械学習モデル。ラベルが陽性である確率を0〜1の数字で予測
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
4.人工知能の定義
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
人工知能（Artifical  Intelligence）
	推論、認識、判断など人間のような知的なふるまいをする人工的に作られた機械のこと（計算機科学の一分野）
	★ジョン・マッカーシーがダートマス会議で使った言葉（1956）ダートマス大学
人工知能のおおまかな分類
	・ルールベースの制御（制御工学）
	・古典的な人工知能（人工知能）
	・機械学習（人工知能）
	・ディープラーニング（人工知能）
	自らの判断で自律的に振る舞うエージェントに基づき分類
ルールベースの制御
	あらかじめ決められた設定に基づいてふるまう
	冷蔵庫、エアコン、炊飯器など
古典的な人工知能
	多くのデータを活用して、複雑なふるまいが可能
	→医療診断プログラム、掃除ロボット（ルンバ）　など
機械学習を取り入れた人工知能
	大量のデータからルール・関係性などを学習する
	→検索エンジン、スパムメール判定、交通渋滞予測　など
ディープラーニング（人工知能）
	何に注目すればいいか（特徴量）を自らで判断し学習する
	【教師なし学習】 の一種に分類されている
	→画像認識、音声認識、機械翻訳など
機械学習（犬の写真）
	何に注目するべきか（特徴）を教える必要があった
	→目、口、耳など犬としての特徴を設定する必要があった
AI効果
	人工知能とみなされていた技術で課題が実現されるとそれはただの自動化であって人工知能ではないと
人工知能とロボットの違い
	ロボットは予め決められた動作を行うのが基本になる
	→産業用のロボット、運搬ロボット、などの装置・機器（体）
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
知能の全体像は,三つの階層で理解される.第一の階層は【パターン処理】で,環境からの情報のセンシングとそれに応じた行動というループが基本とされる.
この際に重要となるのが 「身体性」という性質で,コンピュータはハードウェアを通じて外部環境との相互作用を得る.
第二の階層は,【記号の処理】であり,人間はこれを通じて物事を抽象的に認識できるようになり,チェスなどのゲームを楽しむようになった.
そして第三の階層が,【他者とのインタラクション】であり,我々が知識を獲得していく上で不可欠な営みである.
人工知能研究は,こうした知能の全体像をなぞる形で進行している.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
5.人工知能
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
世界初の汎用コンピュータ
	アメリカで世界初の汎用コンピュータ
	【エニアック（ENIAC）】をペンシルバニア大学が開発(1946)
人工知能研究のブームと冬の時代
	第一次人工知能ブーム：1950年代後半～1960年代
	第二次人工知能ブーム：1980年代
	第三次人工知能ブーム：2000年代～
第一次人工知能ブーム（推論・探索の時代）
	迷路の謎解きなど簡単な問題（トイ・プロブレム）はコンピュータが解けるようになった
	人工ニューラルネトワークの研究は1950年代からすでに始まっている
第二次人工知能ブーム（知識の時代）
	特定の専門分野の知識をもとに、専門家のように様々な推論・判断を行うシステム（エキスパートシステム）
	大量の知識を与えるコスト・管理するコストが大きかった
第三次人工知能ブーム（機械学習・特徴表現学習の時代）
	大量のデータ（ビックデータ）を用いることで人工知能が自ら知識を獲得する機械学習が実用化された
	→データの特徴を定量的に表したもの（特徴量）を人工知能が自ら学習するディープラーニングが登場

・第一次AIブームでは,1997年にIBMが開発した【Deep Blue】というソフトがチェスの世界チャンピオンを破った
・Deep Mind社は2016年にAlpha Goを開発し,碁でプロ棋士に勝利するプログラムを作成した.

トロント大学の【ヒントン教授ら】はディープラーニングの第一人者として有名である.
ニューラルネットワークのバックプロパゲーションやボルツマンマシンなどの開発者の一人である.

Google社が2012年に大量の画像データから「猫」を抽出し,その隠れ層では猫の概念と思われる画像が抽出された.
これにより,コンピュータがディープラーニングにより意味を理解できたと考えられた.

google 社が提供するTensorFlow
これは大規模な GPU （Graphics Processing Unit） 環境での高速化に特徴があると言われている

Chainer 日本のPreferred Networksが開発したディープラーニング用ライブラリである.
PyTorch Chainerから派生して生まれたオープンソースのディープラーニング用ライブラリである.
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
6. 探索・推論（探索木）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
探索木
	１つ１つの要素をノードと呼ぶ
幅優先探索
	初期のノードから見て近いノードから探索を行う
	探索したノードをすべて記憶しなければならないためメモリが必要
	最短経路を見つけ出すことができる(ステップ数が少なければ)
	深い場所に解答がある場合、途中で記憶容量がパンクする
深さ優先探索
	あるノードから行き止まりになるまで行き、行き止まりになったら、
	１つ手前のノードまで戻りまた行き止まりになるまで探索を行う
	記憶するものが少なくメモリが少なくて済む
	最短経路とは限らない
ハノイの塔
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
7. 探索・推論（行動計画とボードゲーム）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ロボットの行動計画
	プランニング
	前提条件、行動、結果を組み合わせて１つの動作を定義する自動計画を記述する手法
	プランニングシステム、【STRIPS(ストリップ)】
SHRDLU(シュルドゥル)
	テリー・ウィノグラード
	コンピュータに英語で指示を出すと端末の画面上の物体を動かしてくれる（積み木）
	研究成果はCycプロジェクトに引き継がれていく
ボードゲーム
	組み合わせの数はオセロ、チェス、将棋、囲碁の順
	コスト（ステップ数）の概念を取り入れて探索を効率化しようとする
Mini-Max法
	自分が手を打つときは自分が有利に、相手が手を打つときは相手が不利
	自分も相手も最善手を打つ仮定のもとにスコアを逆算する
	あらかじめどのような手が有利か不利かをスコア化（コスト）
	すべての手を計算すると時間がかかるデメリットがある
αβ法
	Mini-Max法において、スコアが低い手を切り捨てる
	αカット、βカット
	切り捨てた中に実は最善の手があるかもしれない
	すべてのパターンを調べる方法（ブルートフォース）の方が勝てる確率は上がる
	Mini-Max法、αβ法では「相手は自分のスコアが小さい手をとる」
	Negaとつくと、「相手が相手自身のスコアが高くなるような手をとる」
ヒューリスティックな知識
	経験による知識やルールのこと
	必ずしも正しい答えではないが、経験や先入観によって直感的に、ある程度正解に近い答えを得ることができる思考法
モンテカルロ法
	ゲームの終盤まで進んだら、あらかじめ決めたスコアを無視して、AI同士でランダムな手を打って、結局（プレイアウト）させる
	複数回プレイアウトすると、勝ちやすい手などがわかりスコアをつけていく
	人間がスコアをつけるよりも理にかなっている
	DeepMind社が開発した人工知能の囲碁プログラムAlphaGoがプロに勝った(2016年)
	ディープラーニングの技術を使っている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
8. 知識表現
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
人工知能という言葉の定義は松尾豊准教授によると「人工的に作られた人間のような知能,ないしはそれを作る技術」と提唱している.
機械学習の先駆者であるアーサー・サミュエル氏による機械学習の定義は
	「明示的にプログラミングをしなくても学習する能力をコンピュータに与える研究分野」
としている.
人工無能
	チャットボットなどと呼ばれるプログラム
	人間との形式的な会話のために作られたロボット
ELIZA（イライザ）
	１９６４年、チャットボットの元祖となるELIZAが開発される
イライザ効果
	コンピュータとのやりとりを、人間とやりとりしていると錯覚する現象のこと
知識ベースの構築とエキスパートシステム
	DENDRAL（世界初のエキスパートシステム）（読-デンドラル）
	1960年代の人工知能に関するプロジェクト
	→未知の有機化合物を特定するエキスパートシステム
	【知識ベースと推論エンジン】により構成
	・【マイシン（MYCIN）】1970年代
	スタンフォード大学で開発されたエキスパートシステム
	→感染した細菌を判断するためのシステム
	専門医＞マイシン（MYCIN）＞普通医
知識獲得のボトルネック
	専門家の暗黙知を聞き出すのが困難
意味ネットワーク（セマンティックネットワーク）
	「is-a」の関係：●は●である
	「part-of」の関係：●は●の一部である
	「has-a」の関係：●は●を持っている
	・is-a　継承関係を表す
	・part-of　属性を表す。本体と部品の関係
ナレッジグラフ
　　　意味ネットワークの中でも、インターネット上などから半自動で構築しているもの
	LOD（Linked　Open Data）
		ネットワーク形式で公開されているデータセット
		地方公共団体などではこのデータを活用したサービスを展開しているのも見られる

Cycプロジェクト（サイク）
	一般常識もデータベース化して、人間と同様の推論システムを構築することを目的とするプロジェクト
	→1984年に開始され、30年以上も入力作業は続いている
オントロジー
	概念化の明示的・形式的な仕様、概念を体系化する学問
	意味ネットワークの単語と単語をつなぐつなぎ方のルールのこと
	オントロジーに従った記法で統一すれば共有できる
	→意味ネットワークを作成しても作者によって書き方などが異なるため他人が作成したものを利用できない
	→いずれにせよ、記述する人によって記述の方法や記述の粒度がまちまちになると、そもそもの知識の共有化や再利用が難しくなるため、記述ルールの標準化が必要
概念間の関係
	→推移律（AとB、BとCに関係が成立するとき、AとCも成立）
	・part-isの関係：全体と部分の関係、タイヤは車の一部
	→part-isの関係には5種類以上の意味があり複雑
	本来なら分けて関係を示したいが分けられていない

オントロジーの構築
　  ヘビーウェイトオントロジー
	    人間がしっかりと考えて知識を記述していくという考え方
		Cycプロジェクトが長年行っている
　  ライトウェイトオントロジー
	   コンピュータが情報を理解して、自主的に関係性を見つけようという考え方
ライトウェイトオントロジー
　  ウェブマイニング
	  ウェブ上にあるデータ分析して有益な情報を抽出する手法
　  データマイニング
	  ビックデータを分析して有益な情報を抽出する手法
ワトソン
	IBMが開発した質問応答（QUESTION-ANSWERING）のシステム
	→2011年アメリカのクイズ番組で対戦して優勝
	→蓄積されている大量のデータから最適な回答を見つけ出している
	IBMは人工知能ではなく、拡張知能と呼んでいる
	→言葉を理解していない（ライトウェイトオントロジー）
東ロボくん
	東大の入試を突破することを目標にした人工知能
	→2011年から2016年まで続けられた
	→偏差値57を突破したが、東大入試を突破することは現在では不可能であると判断されプロジェクト終了した
	→読解力が必要であり、知識だけで合格が困難であった

セマンティック・ウエブ
	メタデータを活用して、コンピュータがウェブ情報を解釈し処理する技術や考え方のこと
	→メタデータ：データに付与するデータのこと
	「五十嵐すず」というデータに対して「著者」というデータを付与することでコンピュータが理解しやすくなる
	例）写真の情報(Exif情報)
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
9. 機械学習・深層学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
レコメンデーションエンジン
	→ユーザーが興味がありそうなものを提案するシステム　アマゾン、YouTube
スパムフィルター
	→迷惑メールを見つけ出すためのシステム
自然言語処理
	自然言語をコンピュータに処理させること
統計的自然言語処理
	確率推論や統計学を使って、自然言語を処理すること
　  コーパス（自然言語の文章を構造化したもの、対訳データ）を活用することで文章を正しく読み取れる確率が上がる
ニューラルネットワーク
	人間の脳を模倣してモデル化されており、入力層、中間層、出力層の複数ニューロンから構成される非常に複雑な構成
ディープラーニング
	ニューラルネットワークを多層化したもの
	→多層化しても学習精度が思ったように上がらなかった
	→誤差逆伝播法などを取り入れることで学習精度があがる
	自己符号化器（オートエンコーダ）、
	活性化関数などを活用することで問題がすこしずつ解決
ILSVRC（ImageNet　Large　Scale　Visual Recognition　Challenge）
	画像認識の精度を競う大会
機械学習
	データの特徴を定量的に表した特徴量を人間が設定
	→人間の経験と勘によって特徴量を設定していた
	→2015年から人間の認識率を超える精度を誇るようになる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
10. 人工知能分野の問題①
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
トイ・プロブレム
	人工知能は複雑な問題を解くことができない
	ルールがシンプルなもの（ゲーム）しか解けないという用語
フレーム問題
	処理能力が限られている人工知能では、現実に起こりうる全ての問題に対応することができないこと
	問題の枠に囚われてしまい、枠の外を考慮することはロボットには難しい
	自動運転車は事故を必ず起こさないようにプログラムすると、走行速度が遅すぎてうまく機能しない問題
チューリングテスト
	機械は思考できるのかという目的から行われるテスト
	→アラン・チューリングが提案したもの
	→人間が審査員となり、人工知能と人間と会話を行うどちらが人間なのかを当てるテスト
	→人工知能は人間っぽいふるまいを行う
ローブナーコンテスト
	最も人間に近い人工知能を決める
	チューリングテスト形式のコンテストのこと
強いAI
	人間のようなふるまい（思考・判断など）をするAI
	汎用型人工知能
	精神や意識を持った AI
弱いAI
　　特定の問題を解決AI
	特化型人工知能
ジョン・サールが論文で発表したAIの分類
	→弱いAIは現実可能でも、強いAIは不可能であると考えている
	→「中国語の部屋」という思考実験を使って説明されている
	強いAI　精神や意識を持った AI
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
11. 人工知能分野の問題2
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
シンボルグラウンディング問題
	シンボル（記号）を現実世界の意味とどのように結びつけるかという問題のこと
	→スティーブン・ハルナッドによって推論された
	→概念と記号を結びつけることは難しい
	人間の場合、シマウマを、縞模様の馬かもしれないと考えることができる
	→記号のシマウマと実際のシマウマが結びつく
	→コンピュータの場合、記号のシマウマと実際のシマウマを結びつけることができない
身体性
	機械が身体を持つことで高度な人工知能になるという考え方
	→身体を通して得られる情報も必要になってくる
知識獲得のボトルネック
	人間が持っている大量の一般常識をコンピュータが獲得することは極めて難しいこと
	機械翻訳では人間の自然な言葉遣いから,出現した単語に続く単語を確率的に予測する言語モデルが用いられる.
	現在ではそれをニューラルネットで【近似】したニューラル機械翻訳モデルが利用されることも増えている.
	・機械翻訳
　		①ルールベース機械翻訳：1970年代後半から
　		②統計的機械翻訳：1990年代から
　　　		性能は一気に改善されたが、意味を理解しているわけではないので翻訳が大変になる
		③ニューラル機械翻訳
			ニューラルネットワークを活用した翻訳
			→精度が一気に上がり、実務でも使われるようになってきた
			TOEIC900点以上の人間と同じくらいの翻訳力をもつ
特徴量設計
	機械学習では、データの特徴を定量的に表した特徴量の選び方が大切になる
	→家賃を予想する場合、最寄り駅までの距離や築年数などが特徴量として考えられる
One-hotエンコーディング
	つくられた変数をダミー変数という
	〇One-hotエンコーディングでは、たとえば、テーブル形式のデータのカテゴリ変数の列について、取り得る値の分だけ列を増やして、
	  各行の該当する値の列のみに1を、それ以外の列には0を入力するように変換する処理をいう。

		from sklearn.preprocessing import LabelEncoder, OneHotEncoder
		# DataFrameをコピー
		df_ohe = df.copy()
		# ラベルエンコーダのインスタンス化
		le = LabelEncoder()
		# 英語のa、b、cを1、2、3に変換
		df_ohe['B'] = le.fit_transform(df_ohe['B'])
		# One-hotエンコーダのインスタンス化　
		ohe = OneHotEncoder(categorical_features=[1])##変換する列名を指定 categorical_features=[1]
		# One-hotエンコーディング
		ohe.fit_transform(df_ohe).toarray()
特徴量エンジニアリング
	取得済みデータから加工し抽出すること
特徴表現学習
	特徴量をコンピュタ自身が見つけること
	特徴量の加工・抽出も学習器にさせること
	→ディープラーニングも特徴表現学習の１つ
	→家賃などの情報を与え、機械が特徴量を抽出する
	人間は機械が何を特徴量にしたのかよくわからない
シンギュラリティ（技術的な特異点）
	人工知能の進化により、人類の知能を越える転換点のこと
	→人工知能が自分よりも賢い人工知能を作れるようになることで加速度的に人工知能の性能は飛躍すると予想
	→レイ・カーツワイルは、2045年にシンギュラリテイーが訪れることを予想している
	人工知能が人類よりも賢くなるのは2029年と予想している
	→シンギュラリティーが起きる年と人工知能が人類よりも賢くなる年は異なる
	→イーロンマスク、ビル・ゲイツは人工知能は脅威であると考えている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
12. 学習の種類
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
教師あり学習（Supervised Learning）
	答えが分かってる問題を教師が教えながら学習していきます
	正解が与えられたデータをもとに、ルールやパターンを学習して手法のこと
	→分類問題、回帰問題　を解くときに使われる手法になる
	・分類問題：複数のカテゴリに分ける問題
	・回帰問題：連続値（体重、温度など）を予測する問題
	分類問題
	　	迷惑メールか普通のメールかを分類する　など
	回帰問題
	　	来店数や温度、気温からビールの売上を予測する　など
教師無し学習（Unsupervised Learning）
	クラスタリング
		購入データから顧客層に分ける　など
	次元削減(特徴抽出)
		数学、理科を理系、社会、国語を文系にまとめる　など
	生成アルゴリズム
		既存データに似ている新しいデータ点を生成
　　　　扱うべきデータが減り計算、分析しやすくなる
強化学習（Reinforcement Learning）
　	与えられた環境下で、報酬が最大となる行動を行うように学習していく手法のこと
　「ブラックボックス的な環境の中で行動するエージェントが、得られる報酬を最大化するような状態に応じた行動を学習していく手法」
	→テレビゲームで点数が高くなるような行動をするために学習していく
	（最適な行動は何か）

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
13. 教師あり学習（線形回帰・ロジスティック回帰）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
・時系列分析
線形回帰
	説明変数：説明するための変数、原因になっている変数
	目的変数：予測したい変数、結果になる変数
	→アイスクリームの売上（目的変数）と気温（説明変数）
	説明変数の数が複数になると。直線から平面になる
	３次元以上の平面を超平面という （ｎー１）次空間
	正規化などの前処理を行い説明変数のスケールを揃え、回帰係数の絶対値が大きいものが重要

	from sklearn.linear_model import LinearRegression 線形回帰の実行準備
	from sklearn.datasets import load_boston
	from sklearn.model_selection import train_test_split

	# Bostonデータセットを読み込む
	boston = load_boston()
	X, y = boston.data, boston.target
	# 学習データセットとテストデータセットに分割
	★X_trainは学習用の説明変数、y_trainは学習用の目的変数である。
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
	# 線形回帰をインスタンス化
	lr = LinearRegression()
	# 学習
	lr.fit(X_train, y_train)

	価格＝100（バイアス） + 50（重み） x 部屋数

最小二乗法
　	実際の値と予想値との差の2乗値が最小になるような重みを決定する方法
擬似相関
　　　因果関係がないのに相関に見えること
重回帰分析の注意点
	多重共線性
　		相関係数が高い説明変数があること
		→計算が不安定になってしまう特徴がある
		説明変数同士の相関が高すぎると、単独の影響を分離したり、その効果を評価したりすることが困難になる
重回帰分析において,学習させて作成したモデルがどのくらい精度の良いものなのかを評価する際には幾つかの方法がある.
そのうち数値によってそれを確認する代表的な手法の一つは 【決定係数】 を用いるもので,0 から 1 の間の値を取り,1 に近いほどモデルがデータに対して当てはまりが良いことを示す.
数値を利用するその他の手法としては, 【RMSE】 が挙げられる.これは,作成したモデルによって導かれる予測値が実際の値よりどれだけずれているかという誤差の二乗の平均の平方根を求めたものである.

------------------------------------
ロジスティック回帰（分類問題）
	回帰と言う名前がついているが分類問題に適用されるモデル
　	複数の説明変数から、ある事象が発生する【確率】を予想する手法
	スコア０〜１を目的変数は０，１に分類。０．５が分かれ目
	→複数の説明変数から、再検査が必要かどうか　な
	シグモイド関数［活性化関数］を用いて計算されている
		→結果が2択の時に用いられる
	→結果が3択以上の場合はソフトマックス関数を利用する
　	複数のクラスに分類することができる（多クラス分類）
	オッズ＝事象が起こる確率／事象が起こらない確率＝ｐ／１−ｐ
	ロジスティック回帰は,ある事象の分類を 0 ～ 1 の値の確率として表現することができる.
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	このロジスティック回帰を行う際には,【最尤推定】 が用いられ,最も確からしいパラメータを獲得することを目的としている.
	このロジスティック回帰を用いるのに適切な事例としては,喫煙状況や血中のコレステロール値などをもとに病気の発症リスクについて調べるなどが挙げられる.
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

ある事象が起こる確率ｐの範囲は０〜１，１−ｐも同じ
ｐ／１−ｐの範囲は０〜∞（無限大）
log(ｐ／１−ｐ)の範囲は-∞〜∞
線形判別モデル

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★14.教師あり学習（ランダムフォレスト・ブースティング）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
決定木
	データを分割するルールを次々と作成していくことにより、分類を実行するアルゴリズム
	分類木：試合の勝ち負け　など
	回帰木：住宅の価格　など
	根ノード（root）
	葉ノード（一番下、子を持たない）
	分岐が多くなりすぎると1つの葉に１つのデータが対応してしまう（過学習）
	→過学習を防ぐためにも、木の深さ・幅に気を付ける必要がある
	情報利得が最大化になるように分岐させる
	・情報利得：分割前の不純度から分割後の不純度を引いたもの
				情報利得＝親ノードでの不純度　-　子ノードでの不純度の合計
	・不純度：１つのノードに異なるクラスのサンプルがどれだけ含まれているかという割合
			　ジニ不純度★　各ノードに間違ったクラスに振り分けられてしまう確率
		エントロピー、ジニ不純度、分散誤差などによって計算される
	〇決定木でデータを分割する時は、データの分割によってどれだけ得をするかについて考える。これを情報利得と呼ぶ。
	〇決定木などで利用される木と呼ばれるデータ構造は頂点であるノード（利用回数、利用間隔）とそれらを結ぶエッジ★（5回以上、5回未満）から構成される。
		木の最下部にあり子ノードを持たないノードはリーフと呼ばれる。
	決定木は SVM やニューラルネットワークなどの手法と比較して【分析結果を説明しやすいこと】や【データの前処理が少なく済む】という特徴があり,実務でも好んで利用される手法である.

		from sklearn.datasets import load_iris
		from sklearn.model_selection import train_test_split
		from sklearn.tree import DecisionTreeClassifier　
		# Irisデータセットを読み込む
		iris = load_iris()
		X, y = iris.data, iris.target
		# 学習データセットとテストデータに分割する
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
		# 決定木をインスタンス化する (木の最大深さ=3)
		tree = DecisionTreeClassifier(max_depth=3)
		# 学習
		tree.fit(X_train, y_train)

		決定木の描画
		　pip install pydotplus

		from pydotplus import graph_from_dot_data
		from sklearn.tree import export_graphviz
		# dot形式のデータを抽出
		dot_data = export_graphviz(tree, filled=True,
		                rounded=True,
		                class_names=['Setosa',
		                     'Versicolor',
		                     'Virigica'],
		                feature_names=['Speal Length',
		                               'Spal Width',
		                               'Petal Length',
		                               'Petal Width'],
		                out_file=None)
		# 決定木のプロットを出力
		graph = graph_from_dot_data(dot_data)
		graph.write_png('tree.png')

		# 予測
		y_pred = tree.predict(X_test)
		y_pred

ランダムフォレスト（分類と回帰）
	バギングを用いる。
	たくさんの決定木を並列で学習し、それらを統合するモデル。特徴量を重複ありでランダムでサンプリングして、各決定木で使う
　	ランダムに選ばれた訓練データで学習した決定木を複数作成し、決定木で予想された結果の多数決でクラスを決める手法
	回帰では平均値を求める
	→精度の悪いものがあったとしても複数組み合わせると精度が高いものになるという考え方にもとづく
	ランダムフォレストは、アンサンブル学習の中でもバギングを用いており、またベース学習器として決定木を用いた手法

	〇ランダムフォレストは、ブートストラップデータを用いて決定木を構築する処理を複数回繰り返し、
　　　各木の推定結果の多数決や平均値により分類・回帰を行う手法であり、アンサンブル学習の1つである。
	〇ランダムフォレストは、データのサンプルと特徴量をランダムに選択して決定木を構築する処理を複数回繰り返し、
　		各木の推定結果の多数決や平均値により分類・回帰を行う手法である。

		from sklearn.ensemble import RandomForestClassifier
		# ランダムフォレストをインスタンス化する
		forest = RandomForestClassifier(n_estimators=100, random_state=123)
		# 学習
		forest.fit(X_train, y_train)
		# 予測
		y_pred = forest.predict(X_test)
		y_pred

アンサンブル学習
　	別々の学習器を組み合わせる学習器を作ること
	→バギング、ブースティングという手法がある
	・バギング★
　　	複数のモデルを並行的に学習させていく方法（復元抽出）N個の予測値の単純平均
		→ランダムフォレストはバギングの１つで学習時間が短いのが特徴
	・ブースティング★
		複数のモデルを直列的に学習させていく方法
		→前に作成したモデルを修正して新しいモデルを作成していく
		学習器を増やしすぎると過学習を起こしやすい
		▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
		まず一つモデルを学習して作成し,それを何度も訂正することを通じて性能を高めていく手法のこと
		▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

	・スタッキング★
		あるモデルの予測値を新たなモデルのメタ特徴量とする

ブートストラップサンプリング★（Bootstrap Sampling）　重複ありで同じ大きさのサンプルを得る
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
    機械学習では,モデルの性能を向上させるための工夫として,複数のモデルを融合させる（◆アンサンブル学習◆）という手法が広く用いられている.
    （◆アンサンブル学習◆）はいくつかの種類に分けることができ,たとえば決定木を用いる場合で言えば,ランダムフォレストは（◆アンサンブル学習◆）の手法のうち（◆バギング◆）,（◆勾配ブースティング木◆）はブースティングにあたる.
    両者の違いは,複数のモデルを独立に学習させるのか,逐次的に学習させるのかの違いである.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

剪定（せんてい）
	枝刈りや木を整えるという意味があり,決定木に対し汎化性能を損なわないような条件分岐の回数の初期値を設定することを指す
	過学習を防ぐために行う
	剪定とは,決定木において過学習を防ぐために枝数を制限する手法である.

決定木利点
〇意味を解釈することが容易である
〇前処理を行う手間がかからない
x非常に柔軟で細かい分類が可能である
〇モデルの学習が早い
決定木はとてもシンプルなアルゴリズムなので実装が簡単.学習が高速,モデルの説明が簡単といった利点がある.ただし柔軟で細かな分類は苦手といった欠点がある.

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★15. 教師あり学習（サポートベクターマシン・k近傍法）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
サポートベクターマシン（SVM）（分類と回帰）
	主に分類で使われるアルゴリズム
	明らかに所属クラスが分かる観測ではなく、判別境界の付近にある判別の難しい学習データに着目する分類モデル
	→マージン最大化する線を求めることで分類を行う
	マージン
　		線（決定境界）とクラスの各データの距離のこと
	サポートベクトル
		判別境界に最も近い学習データ
	ハードマージン
　		マージンの中にデータが入ることが禁止
	ソフトマージン
　		マージンの中にデータが入ることを許容
	→【スラック変数】：どの程度強要するか調整
	〇サポートベクタマシンは、【分類・回帰だけでなく】外れ値検出にも使えるアルゴリズムであり、直線や平面などで分離できないデータであっても、
　		高次元の空間に写して線形分離することにより分類を行うことを可能にする。

	サポートベクタマシンの決定境界は、マージンを（「最小」ではなく）最大にすることで求めます。
		ハイパーパラメーターのCの値が小さいほど、マージンは大きくなる
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	SVMではマージンが最大になるような線を求めるだけではなく,カーネルトリックにより精度を向上させている.カーネルトリックを用いることで非線形問題も解くことができるようになる.
	SVM には分類問題を上手く解くための工夫が施されており,スラック変数は【一部のサンプルの誤分類に寛容になる】ための,カーネル法は【決定境界を非線形にする】ための,
	カーネルトリックは【内積計算をカーネル関数へ置換え,計算量を大幅に削減する】ための工夫である.
	SVM はカーネル法やカーネルトリックが登場してから人気が高まり,より頻繁に利用されるようになった.
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

カーネル法を組み合わせることで直線で分類できないものでも分類ができる
 	次元を拡張させる関数をカーネル関数という
	データを高次元に写像することで線形分離を可能にする
	→カーネルトリックと呼ばれる方法を使うことで
		計算を楽にすることができる（次元が増えると計算が大変になる）
k近傍法（分類と回帰）
	分類に使われる手法の１つ
	特徴量のスケールを予め揃えておく必要がある
	→未知のデータが与えられたときに近くにあるデータからクラスを分類していく手法のこと
	→ｋには個数が入る
	距離の算出には、一般的にユークリッド距離(√x²+ｙ²)が使われる
	最近傍法
		ｋ＝１のこと。予測する観測は最も近いデータの値が出力
		複雑で入り組んだものになる
		ｋが大きくなるほどたくさんの観測の平均や多数決が行われるので単純で明快なものになる
混合ガウスモデル(GMM) クラスタリングで用いられる.ガウスとは正規分布（確率分布）のこと
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★16. 教師あり学習（ニューラルネットワーク・時系列分析）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ニューラルネットワーク
	人間の神経回路（ニューロン）を模倣した数理モデル
ニューラルネットワークの深さ
	入力層を除く層の個数
	層のサイズはバイアスノードを除くノードの個数

単純パーセプトロン
	1958年に提案されたニューラルネットワーク
	→入力層と出力層からなる
	モデルの自由度を上げるためにバイアス(b)を使用する
	活性化関数 次の層に渡す値を調整する関数をという
		シグモイド関数（結果2択）
		ソフトマックス関数（結果が3択以上）
		ReLU関数 など
多層パーセプトロン
	→非線形分類が可能になった
	→入力層、隠れ層、出力層に分かれている
	層が増えることで重みも増える
	→誤差逆伝播法を活用して重みを調整
	誤差逆伝播法（バックプロパゲーション）
	  出力層から入力層にかけて重みを調整する方法のこと
自己回帰モデル（ARモデル）
	回帰問題に使われる手法（時系列系のデータ）
	→過去のデータを使ってある時点の値を予測するモデル
ベクトル自己回帰モデル（ＶＡＲモデル）
	自己回帰モデルを多変量に拡張したもの
勾配ブースティング
	多数の弱いモデルで強いモデルを作るアンサンブル学習であるブースティングの１つ
	簡単に言うと学習器にあまり高性能なものを使わずに、弱分類器という感じで、予測値の誤差を新しく作った弱学習器がどんどん引き継いでいきながら誤差を小さくしていく方法です
	中でも弱いモデルに決定木を利用したものは、テーブルデータを対象とした回帰問題や分類問題で高い精度を示す（勾配ブースティング木、人気）
勾配ブースティング木（GBDT）のライブラリ
	xgboost DMLC社
	lightgbm Microsoft社
	catboost ロシアの検索エンジンを運営するYandex社製
畳み込みニューラルネットワーク
	特に画像領域で用いられる
汎化能力
	未知のデータに対する対応能力

回帰事例
x高校数学の問題を解く
x葉っぱの高さや花びらの大きさからその植物の種類を当てる
xAlpha Go のような囲碁のプレイヤープログラム
〇ある日の気温とアイスクリームの売上高の関係性を見出す

下記の図表が表すのが最も適切な用途
ア）ヒストグラム
	ヒストグラムはある幅にどれくらいのデータ数があるかを可視化したい場合に使用する.
イ）散布図
	あるクラスの生徒の一週間に読む英語の文章量と英語のテストの得点との関係性を見たいとき
	散布図は縦軸と横軸の二つの特徴量によりデータの点を打つ図表である.散布図を使用すると2変数の相関を確認することができる.
円グラフ
	家電市場の各社の市場シェアを一覧したいとき
曲線グラフ
	植物群の丈の大きさの分布を種類ごとに比較したいとき
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
17. 教師なし学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
クラスタリングを使ってデータの行を減らして、次元削減で列の個数を減らす
クラスタリング（クラスタ分析）
	データをグループ（クラスタ）分けする手法のこと
	ある基準を設定してデータ間の類似性を計算し、データをクラスタ（グループ）にまとめるタスク
	→大量のデータをコンピュータが自動的に分類してくれる
	グループ分けされたものは人間が見て解釈する必要がある
	・K-means法
	・ウォード法
	・主成分分析
	・ｔ-ＳＮＥ法
	・協調フィルタリング
クラスタリングの特徴
	x各グループの特徴が客観的に明らかである
	〇クラスタリングの分け方には大きく階層型と非階層型がある
	〇幾つのグループに分けるかを選択できる
	〇類似度には様々な定義の仕方がある
非階層クラスタリング
	あらかじめクラスタ数を決めなければならない手法
K-means法（階層なしクラスタリング）(k平均法)
	あらかじめ設定したクラスタの数（k個）にデータを分類方法
	→重心を活用して分類を行っていく方法
	線形分離可能なデータしか分けることができません
	〇k-meansは、【最初にランダムにクラスタ中心を割り当て】、クラスタ中心を各データとの距離を計算しながら修正し
	  、最終的なクラスタ中心が収束するまで再計算を行いクラスタリングする手法である。
	〇k-meansは、scikit-learnのclusterモジュールのKMeansクラスを用いることによって実行することができる。
	〇クラスタリングにおいては、得られたクラスタの妥当性に絶対的な答えはない。

		from sklearn.cluster import KMeans
		# クラスタの数を3とするKMeansのインスタンスを生成
		km = KMeans(n_clusters=3, init='random', n_init=10, random_state=123)　　　#n_initはk-meansの実行回数★★
		# KMeansを実行
		y_km = km.fit_predict(X)
	k近傍法はデータ数、k平均法は分類数
階層クラスタリング
	〇凝集型の階層的クラスタリングは、まず似ているデータをまとめて小さなクラスタを作り、次にそのクラスタと似ているデータをまとめ、
	　最終的にデータが1つのクラスタにまとめられるまで処理を繰り返すクラスタリング手法である。
	Ｘ凝集型の階層的クラスタリングは、最初にすべてのデータが一つのクラスタに所属していると考え、順次クラスタを分割していく手法である。
	〇分割型の階層的クラスタリングは、【最初にすべてのデータが1つのクラスタに所属している】と考え、順次クラスタを分割していくアプローチであり、
	凝集型より利用頻度が多いとは言えません。
	分割型　似ていないデータ同志を分割していく手法であり計算量が多いため、凝集型より利用頻度が多いとは言えません

		from sklearn.cluster import AgglomerativeClustering
		# 擬集型の階層クラスタリングのインスタンスを作成
		ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')
		# クラスタリングを実行し、各クラスのクラスタ番号を取得
		labels = ac.fit_predict(X)
		labels

ウォード法（階層ありクラスタリング）
	似ている組み合わせから順番にクラスター化していく方法
	途中経過を樹形図（テンドログラム）で表すことができる
群平均法（階層型クラスタリング）
	デンドログラムが利用されるクラスタリング手法

デンドログラム（階層ありクラスタリング）
	ある距離を閾値として定めることでクラスタにする。
主成分分析（PCA）
　	データの特徴量の関係性（相関関係）から変数をまとめてより少ない変数を要約すること
	変数統合後の潜在変数の意味の解釈が主観的であり,客観的に一意に定まるものではないこと
	多変数の空間の中から、分散の大きい★順に、新たな軸を第一主成分、第二主成分、・・・と見つけていく手法。
	→データを要約することを次元削減といい、計算量を減らせる
	→まとめられた特徴量を主成分と呼ぶ
	カイザー基準　使用する主成分の数を決定するための基準
				絶対的な基準はない。寄与率80%以上という基準には一般的な根拠はない。結局、解釈可能な個数にすればよい。
				固有値が１以上である因子を採用する

	英語、理科、数学、社会、国語のテストを実行
	→それぞれの科目を分析して２つの成分にする場合、理系と文系という風に分けることができる
	高次元のデータに対して分散が大きくなる方向（データが散らばっている方向）を探して、元の次元と同じかそれよりも低い次元にデータを変換する手法
	分散・ばらつきが大きくなる方向を探して、元の次元と同じか、低い次元にデータを変換・圧縮する、次元削減の一手法
	主成分分析は、scikit-learnのdecompositonモジュールのPCAクラスを用いて実行することができる。
	2次元のデータに対して主成分分析を行い、どちらも重要であると確認できた場合には、1次元に次元削減できる可能性が高いとはいえません。
	Ｘ主成分分析とは、高次元のデータに対して標準偏差が小さくなる方向を探して、元の次元と同じかそれよりも高い次元にデータを変換する手法である。

次元削減の方法
	主成分分析以外にもｔ分布を使って次元削減する
	t-SNE法、特異値分解（SVD）、多次元尺度構成法（MDS）が有名
	低次元（小）-＞高次元（多）
　　「圧縮」するタスク
　　　計算量の削減は、（目的変数ではなく）説明変数の次元数を減らすこと
	Ｘ次元削減は、データが持っている情報をなるべく損ねることなく次元を削減してデータを展開(〇圧縮)するタスクである。
	Ｘ次元削減の主目的は目的変数の数を減らして計算量を削減することであるが、説明変数の削減を行うことは少ない。これは、モデルの精度を確保するためである。
	特徴量の次元削減、なるべくデータの情報を落とさずに、少ない特徴量でデータを表現するために用いられる
	不動産分析で、延床面積、敷地面積、間口を広さとする
ｔ分布
	t分布は、標準正規分布に非常によく似ているのですが、
	標準正規分布に比べて、t分布の方が少しだけ、

	•ベル型の頂点の位置が低く、
	•左右に広がる裾が厚い。

t-SNE
	次元圧縮のために利用される手法であり、特に高次元での距離分布が低次元の距離分布にも合致するよう非線形型の変換を行う方法
	自由度1のt分布を用いて高次元データを2次元や3次元に落とし込むため非線形の分析手法をt-SNEといいます。

協調フィルタリング
	レコメンデーションで使われる手法の１つ
	→AMAzonなどECサイトでおススメが表示される仕組みのこと
	→自分と類にしたユーザーが購入した商品を薦める
	　動画配信ならば類にしたユーザーが見た動画を薦める
	→ある程度のデータが必要になる（コールドスタート問題）
	・コンテンツベースフィルタリング
	　	商品の特徴を利用して類にした商品を薦める方法
		→コールドスタート問題を回避することができる
		→それぞれにメリットとデメリットが存在する
コールドスタート問題
	新商品、行動履歴がなくてレコメンドできない問題をどうする
	「新登場のアイテムは行動履歴がある程度付くまでレコメンドの対象にならない」というもの
トピックモデル
	クラスタリングで使われるモデルの１つ
	代表的な手法として,テキストデータに特異値分解を適用した潜在意味解析 (LSI) や,その後に開発されディリクレ分布という確率分布を利用した【LDA(潜在的ディリクレ配分法)】が知られている.
	→K-means法などと異なり複数のクラスタに分類が可能
	　潜在的ディリクレ分配法（LDA）★が有名
	→ニュース記事を複数のカテゴリに分類したりレコメンダシステムなどで活用したりできる
	・ソフトクラスタリング★
	　	各データが１以上のクラスタに所属するようなクラスタリング
	・ハードクラスタリング★
	　	各データが１つのクラスタに所属するようなクラスタリング
潜在的ディリクレ分配法（LDA）
	１つの文書には複数の潜在トピックが存在すると仮定する。また、具体的な単語などのデータがそれぞれ生成される確率はトピックごとに異なる
半教師あり学習
　	学習の途中まで教師あり学習で、その後、教師なし学習を行う学習
	→少量のラベル付きデータを学習し、
　		ラベルがついていない大量のデータを学習するアプローチ
	→大量のラベル付きデータを集めるのはコストがかかりすぎる
		ラベル：データに付与される正解かどうかを表す情報
	半教師あり学習は大きく分けて識別モデルと生成モデルでがある。

敵対的生成ネットワーク(GAN)
	主に画像生成するためのネットワーク
	GANはデータから特徴を学習することで、実在しないデータを生成したり、存在するデータの特徴に沿って変換できます。
	GANのネットワーク構造は、Generator（生成ネットワーク）とDiscriminator（識別ネットワーク）の２つのネットワークから構成されており、互いに競い合わせることで精度を高めていきます。
	例えるならば、「偽物を作り出す悪人（Generator）」と「本物かどうか見破る鑑定士（Discriminator）」のような役割をネットワーク内に組み込み、競争させるような形で学習させます。（そのため「敵対的」と言われます）
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	GAN は尤度ではなく,識別器により生成器のよさを評価する
	測る距離が,VAEはKLダイバージェンス,GANはJSダイバージェンスを用いている.
	収束性,モードコラプス,勾配消失などの問題点がある.モードコラプスとは生成器が似た出力しか出さないようになることである.
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
	ディープラーニングの画像認識に関する研究分野において,2014 年に 【イアン=グッドフェロー】によって提唱された画像を生成する手法,生成敵対ネットワーク,通称 【GAN】 は衝撃を持って迎えられた.
行と列を同時に減らす
	行列分解　レコメンデーションを生成するのに広く使われている、
	特異値分解　画像の圧縮に使われている

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★★18. 強化学習（Reinforcement Learning）（RL）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
バンディットアルゴリズム
	活用 （exploitation）と 探求 （exploration）の２つから構成される。活用は現時点で最もよいとされている腕を使うことであり、
	探求は候補の腕の中でどれが最も優れているかをテストすることだ。A/Bテストは全てのユーザを探求に割り振り、活用は行わない。
	A/Bテストは探求ー＞終盤に活用、バンディットアルゴリズムは序盤から活用しながら、適度に探索もし続ける
強化学習
	データを一切与えないでタスクを実行、データの代わりに、環境 (解決すべき問題) と行動するエージェント(学習アルゴリズム)を与える
	→状態、行動、報酬（ペナルティ）が大切なキーワードになる
	強化学習ではエージェント(学習アルゴリズム)が環境上の状態において行動します。その結果、次の状態に遷移し、報酬を得るという相互作用の枠組みを設定います。
	強化学習では環境と学習目標を設定する。行動主体であるエージェント(学習アルゴリズム)が環境内で学習目標を達成するように状態に対する最適な行動選択の学習を行う。
		また、行動選択の結果、エージェントは報酬を得る。学習目的に近づく行動選択であったか報酬に基づいて評価することで行動選択を改善する
	強化学習では一連の「ｘ行動の組み合わせ」「○行動選択の仕方」を学習する
		強化学習では一連の行動選択から得た報酬を参考に個々の行動選択を改善する
	強化学習のアルゴリズム
	・QーLearning　次の状態でとりえる移動可能な選択肢の中で最大の期待値を新しいQ値とする
	・Sarsa（サルサ）1つずつ行動した結果でQ値を更新
	・モンテカルロ法　アクション毎にQ値を更新する上記2法と違い、報酬にたどり着いた時点でそこまでで得た価値を使って一気にQ値を更新する

｛エージェント｝　　　←　状態　　　｛環境｝(解決すべき問題)
　(学習アルゴリズム)　　　行動　→
　　　　　　　　　　　←　報酬
	→短期的な報酬だけで行動を決定しているわけではない
	　行動に対して毎回報酬がなくても学習可能（囲碁や将棋など）
	・探索と活用のジレンマ
　		探索：知っていること以外の行動を行う
　		活用：既存情報を使って最適な行動を行うこと
		→活用だけだと、良い行動があるかもしれないが発見できない
　		　探索だけだと、必要のない行動が多くなり時間がかかる

	・ε-greedy方策：一定確率で探索を選ぶ
				これは一定確率で価値によらずにランダムに行動を選択することで選ばれる行動の可能性を広げようとする手法である.
	・UCB方策：探索では今まで選択されていない行動を選択

マルコフ決定過程（MDP）モデル
	次に起こる事象の確率が、【これまでの過程】と関係なく、【現在の状態】によってのみ決定される確率過程のこと
	過去を振り返らない
	マルコフ性（マルコフ連鎖）
		以前の状態に依存しない性質のことを「マルコフ性」といいます
		→本来ならば過去の状態を考慮して行動を選択するが計算が複雑になってしまう
方策
	エージェントが持つ行動選択のルールのこと
　	最適な方策を見つけ出すことは難しい
	1.状態や行動の価値を計算して最適な行動を選択（価値ベース）（モデルフリー）
	　方策を設定して得られた報酬をもとに、計算した状態や行動の価値を参考にして行動することで間接的に方策を改善する
	2.方策をパラメータのもった関数と定義して方策の価値が最大になるようにパラメータを学習させる（方策ベース）（モデルベース）
	　設定した方策に従って行動した場合の、すべての状態の価値を計算し、その値を参考に方策を改善する
	これを方策が改善されなくなるまで行う。状態遷移確率や報酬などの情報が事前にわかっているときに適応できる手法である
価値関数
	将来的に見込める報酬を求める関数
	・行動価値関数
	・状態価値関数
	→未来の報酬はもらえるのかどうかわからないので割り引いて考える
	（割引率：0～1）
行動価値関数
　	ある状態において行動した結果、将来的に得られる報酬の合計を返す関数
状態価値関数
　	ある状態において、将来的に得られる報酬の合計を返す関数
P値(P-Value)
	対立仮説「IT業界にB型が多い」
	帰無仮説「そんなに変わらない」を測る尺度がP値
Q値(状態行動価値)
　	行動価値関数から返ってきた値のこと
	「ある状態 s においてある行動 a を取った時の価値」がわかれば、その価値の一番高い行動を選択すればよい。Qボードに記録する
	Q値は「報酬」ではなく「価値」である
	Q値とは短期的な報酬ではなく、長期的な意味での価値を値として持っている関数
Q学習
　	Q値が最も高くなるように行動をするように学習すること。
	Q値を学習するためのアルゴリズムのひとつがQ学習
方策勾配
　	方策勾配法
　　	方策をパラメータのもった関数と定義して
　　	方策の価値が最大になるようにパラメータを学習させていくことで最適な方策を見つけるようにする方法
		方策を設定して一連の行動を実行し、得られた報酬によって方策を評価して直接方策を改善する。連続的な行動も選択できる。
　	ＲＥＩＮＦＯＲＣＥ
　　	アルファ碁にも活用されているアルゴリズム
	→方策ベースの手法
	・Actor-Critic
　　	行動（ACtor）と評価するCriticで構成されている
		→Criticが行動を評価して方策を更新していく
	→価値ベースと方策ベースの手法
		→Actor-Criticを応用したA3Cがある
深層強化学習
状態行動空間の爆発　
	強化学習の課題で、状態と行動の組に対して定義される値を保存するための
	領域が極端に必要になってしまう
	その問題を解決するのがDQN。代わりに関数で近似する方法でQ値を管理。関数近似にニューラルネットワーク技術を適用
疎の学習  勝敗のように最後に報酬が得られる報酬
Optimism in face of unsertainly
	不確定な時は楽観的に

SAC(Soft Actor-Critic)
	連続値制御の深層強化学習モデルである.
	SACは連続値制御の深層強化学習モデルである.方策関数(Actor)とsoftQ関数を,ニューラルネットワークで実装する.

報酬成型(Reward Shaping)
	通常の報酬値に,追加の値を加えることで学習速度を向上させることができる.
	報酬成型(Reward Shaping)は,通常の報酬値に,追加の値を加えることで学習速度を向上させることができる

Deep Q Network(DQN)
	状態と行動の価値をこれまで得た報酬で近似するQ関数をニューラルネットワークで表現する。
	ニューラルネットワークの入力は状態であり、出力層の各ノードは各行動の価値となる。
	Q関数以外の行動選択の部分はQ学習と変わらない
	ニューラルネットワークを用いて価値関数を表現し、行動の選択は価値を参考にする。
	ニューラルネットワークのパラメータの更新は誤差関数に関するパラメータの勾配で行う

代表的な手法としては,DQNなどが挙げられる.これは強化学習における行動価値関数を,従来とは異なり CNN に置き換えて近似を試みたモデルである.
この深層強化学習の分野の応用例で最も有名な事例は,google 社傘下の DeepMind 社が開発した囲碁対局ソフト Alpha Goが世界チャンピオンを打ち破ったものが記憶に新しいだろう.

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
    ディープラーニングの登場により,ロボットや機械の世界で（◆目◆）に相当する機能が向上した。
    ロボットによって,片付けや調理,あるいは農業や建設などの作業の実現を可能にしたのはディープラーニングによる（◆目◆）に相当する画像認識の精度向上に加えて（◆強化学習◆） との連携が可能になってきたこと
    が大きな変化としてあげられる.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★19. モデルの評価手法
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
→データを訓練データ・テストデータに分割して検証することを交差検証と呼ぶ
交差検証（クロスバリデーション）の種類
	・ホールドアウト検証
	・ｋ-分割交差検証
	・１つ抜き（Leave　One　Out）法
ホールドアウト検証（Hold-out法）
	準備したデータを訓練データとテストデータに分ける
	→テスト用データを活用してモデルの性能を評価
	→データが少ない場合、評価が高くなってしまうことがある
	弱点
		・学習用データが減る
		・パラメータの変更などを何度も行うことより、評価用データに過学習する恐れがある
		・データの分割の仕方によって、結果が変わる
ｋ-分割交差検証
	準備したデータを複数個に分割し、それぞれのデータを使い学習と評価を行う方法
	→データ数が少ない場合にたまたま評価が高くなってしまう可能性を防ぐために使われる検証方法になる
	9割学習用データ、1割テストデータにする処理を10回行う＝10分割交差検証

		from sklearn.svm import SVC
		from sklearn.model_selection import cross_val_score
		# サポートベクタマシンをインスタンス化
		svc = SVC()
		# 10分割の交差検証を実行
		cross_val_score(svc, X, y, cv=10, scoring='precision')　　適合率（precision）　

		＊引数cvに、10分割の交差検証を指定

１つ抜き（Leave　One　Out）法
	データ全体のうち１つをテストデータにする方法
	→訓練データとテストデータを入れ替えてテストを行う
	→データ数分繰り返し行う（ジャックナイフ法とも呼ばれる）
データリーケージ
	未知のデータが入ってしまうこと
	訓練データ・検証データ、テストデータにすべき
	→時系列データをもとにモデルを作るときは
　	未来のデータが入るのはよくない（過去のデータの学習）
二乗平均平方根誤差（RMSE）(Root Mean Squared Error)
	回帰問題の評価指標。パイ　1/n　Σ（正解ー予測）2
	大きな誤差を許容したくない場合に使う
平均二乗誤差（MSE）
	平均二乗誤差の性質として外れ値に対して敏感であることが挙げられます。
	外れ値を含むデータに平均二乗誤差を用いてモデルを構築すると、予測結果が不安定になります。
	1/n　Σ（正解ー予測）2
平均絶対誤差（MAE）
	外れ値に強い
二乗平均平方根対数誤差（RMSLE）
	目的変数と予測値の比に着目する経済指標。対数化した指標
交差エントロピー誤差（cee）
	分類問題に用います。
決定係数
	回帰問題によって求められた予測値や当てはめの値が、正解ラベルとどの程度一致しているかを表す指標
	正解ラベルと予測値の相関係数を二乗した決定係数は、ランダムな予測より良いモデルに対して0〜1の範囲をとる
	また、その数値は正解ラベルの変動のうち、モデルによって説明できる変動の割合
logloss
	分類問題の代表的な評価指標
	高い確信度をもった予測が誤ったときにペナルティが大きくなる経済指標
訓練データ：検証データ：テストデータ
		60:20:20 80:10:10
		検証データ どのモデルを選択するのかの決定に使う

どのようなモデルが優れたモデルであるのかを検証する為には様々なアプローチがある.
ある予測や分類を行うモデルを作る際,基本的にはやみくもに変数を増やすと精度が落ちたり,変数が多いことによって他の未知のデータに転用すると汎用性が低かったりということが起こる為,
出来るだけ少ない本質的な変数を選びモデル化することが必要となる.

モデルのパラメータ数を増やすと精度が高くなるが,同時に過学習の可能性も増えてしまう.したがってパラメータ数は精度の変わらない限りなるべく下げたい.
そこで使われるのがAIC（Akaike's Information Criterion / 赤池情報量基準）である.
AIC（Akaike’s Information Criterion / 赤池情報量基準）とは、モデルの当てはまり度合いを表す指標で、AICの値が小さいほど「よい」モデルとみなせる指標
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
☆20.分類モデルの評価
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
→正解・不正解をまとめた表を混同行列という

画像を犬か、それ以外に分けるテストを行った
１．「犬」の画像を、正しく「犬」と判断（真陽性）TP
２．「犬」の画像を、間違って「犬以外」と判断（偽陰性）FN
３．「犬以外」の画像を、間違って「犬」と判断（偽陽性）FP
４．「犬以外」の画像を、正しく「犬以外」と判断（真陰性）TN
                    		 予測のクラス
						犬と判断		犬以外と判断
犬の画像（Positive）     1.真陽性 TP 1o   2.偽陰性 FN 0x
犬以外の画像（Negative）  3.偽陽性 FP 1o   4.真陰性 TN 0x
  本当のクラス
                    		 予測のクラス
						病気と判断		健康と判断
病気の人（Positive）     1.真陽性 TP 1o   2.偽陰性 FN 0x
健康な人（Negative）     3.偽陽性 FP 1o   4.真陰性 TN 0x
  本当のクラス
  偽陰性>偽陽性 重大　高再現率モデル

                    		 本当のクラス
						病気の人（Positive）健康な人（Negative）
病気と判断     			1.真陽性 TP 1o   		3.偽陽性 FP 1o
健康と判断     			2.偽陰性 FN 0x  		4.真陰性 TN 0x
  予測のクラス

                    		 予測のクラス
						スパムと判断		ハムと判断
スパム（Positive）     1.真陽性 TP 1o   2.偽陰性 FN 0x
ハム（Negative）     3.偽陽性 FP 1o   4.真陰性 TN 0x
  本当のクラス
  偽陰性＜偽陽性 重大　高適合率モデル
正解率（accuracy）＝(TP+TN) / (all)
	全データのうち正しく予測できた割合
陽性判定 正例を予測する
適合率=TP / (TP+FP)
	precision 適合度，精度
	陽性と予測したデータのうち、正しい予測の割合
	陽性判定のうち、実際に疾患を有する人の割合
	適合率を用いるケース
		Amazon による商品のレコメンドの精度
		レコメンドではなるべく推薦結果がユーザーの嗜好に合致しないものをなるべく除外した方が望ましいため適合率の高さが求められている.
再現率=TP / (TP + FN)
	recall 感度 全部のがんに対して正解率
	陽性データの中で、正しい予測ができた割合
	疾患を有する人のうち正しく陽性判定できた割合
	病院の疾病診断のような見落としが許されないケースでは再現率の高さが求められている.
F値= (２ｘ適合率ｘ再現率) / (適合率＋再現率)
	適合率と再現率の調和平均 適合率と再現率の両方良いかどうか
	両方高ければ、F値も高くなる
特異度  TN / (FP + TN)
感度 TP / (TP + FN) 再現率と同じ
評価したい項目に合わせて評価指標を選ぶことが大切
→値が高いからといって性能が高いわけではない
→不良品率0.1％を判別するモデルを作り、正解率99.9％だった
	すべてを正しい製品だと判別し、不良品を見つけていなかった

	＊適合率は予想するクラスをなるべく間違えないようにしたいときに重視する指標である。
	＊正解率は、正例か負例かを問わず、予測と実績が一致したデータの割合を表す。正解率は(tp+tn)/(tp+fp+fn+tn）で計算することができる。
	＊F値は、適合率と再現率の【調和平均】として定義される。F値は2*適合率*再現率/(適合率+再現率)で計算することができる。
	〇適合率、再現率、F値、正解率は、機械学習を用いて構築した【分類モデルの良し悪し】を評価する指標であり、混同行列から計算する。
	〇一般的に適合率と再現率はトレードオフの関係にある。つまり、どちらか一方の指標を高くすると、もう一方の指標は低くなる。
	ｘ適合率は、たとえば病院の検診で病気の見逃し・取りこぼしがないようにしたい場合などに重視される、網羅性に関する指標である。
	ｘ再現率は、間違えることをできるだけ避けたい場合に重視する指標である。一般的に適合率と再現率はトレードオフの関係にある。

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★21. 過学習    正則化
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
過学習（overfitting　オーバーフィッティング）
	過学習とは、特定のデータへの対応にのみ長けてしまうことです。
	これはモデルが訓練データに対して過剰に学習したため、はずれ値やノイズまで学習してしまったと考える
	訓練に使ったデータを完全に記憶してしまうと、処理の練習になりません。結果的に、未知のデータにはまったく対処できない状態になります。
	過学習が起きる原因としてデータ数が少ない、変数が多い、パラメータが大きすぎるといったことがあります。
	モデルの予測値とテストデータ（学習に用いてないデータ）との間の誤差はだんだん増加していく
汎化性能
　	未知のデータに対する精度のこと（精度は汎化誤差で測る）
	測定はホールドアウト法、交差検証法
バリアンスが大きい
	過学習で汎化性能が大きくなること
バイアスが大きい
	学習不足だけど、モデルが単純すぎて汎化性能が大きくなること
アクティブラーニング
	人間がチェックしてNGで、人間が修正したデータを使って、追加で学習させること
訓練誤差：訓練データに対する予測と正解の誤差
汎化誤差：未知データに対する予測と正解の誤差
過学習を防ぐテクニック
	・アンサンブル学習
	・ドロップアウト・早期終了・バッチ正規化
	・データ拡張
　　	データのバリエーションを増やす（データの水増し）
		▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
		きれいな画像を汚し,ロバスト性を高める
		ロバスト性　外乱の影響があった時でも持続できる対応能力
		水増しする訓練データの加工は、加工前後で認識結果が変わらない程度にとどめなければならない
		▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
	・スパース化
　　	説明変数（特徴量）を減らす
	・正則化
	など
正則化
　	過学習を防ぐために、パラメータに制限をかけること
	学習の目的関数にペナルティとなる項目を追加することで、パラメータが極端な値になることを防ぐ
	回帰式が複雑な形状になった場合にペナルティを設けることで、回帰式をなるべくシンプルな形状に保つというテクニックの一つ
	高次元データを扱う高次元学習問題においては，モデルの複雑さが上がる傾向にあり，特に正則化法が効果を発揮する．
	高次元学習問題においては，モデルの自由度をなるべく低次元に抑えるスパース正則化がよく用いられる．
	係数の数が多いモデル、係数の値が大きいモデルは複雑と見る
	→正則化しすぎると未知のデータに対する精度が低下する
　	未学習（学習不足）になってしまうので注意が必要
	→正則化にはL1正則化、L2正則化などがある
	L1正則化／ラッソ回帰（Lasso）
		モデルの複雑さ＝係数の絶対値の合計
		余分な説明変数を省くことを目的とした手法
　　	特定のパラメータの値を０にして、選択
		正則化パラメータを大きくするに従い、各回帰係数が０になるものが増える
		ラッソ回帰＝回帰誤差＋モデルの複雑さ
		モデルの計数の個数が少なくなる。つまり正則化で計数のいくつかが0になる
	L2正則化／リッジ回帰（Ridge）
		モデルの複雑さ＝係数の2乗の合計
		モデルの過学習を防ぐことで精度を高めるために用いられます。
　		パラメータの大きさに応じて０に近づけて滑らかなモデルにする
		リッジ回帰＝回帰誤差＋モデルの複雑さ
		モデルの計数の値が小さくなる。0になることはない
	Elasti Net★
　　	ラッソ回帰とリッジ回帰を組み合わせたもの
	スパースデータ★　要素の０が多くなるデータのこと
	L1が推奨されるのは、データセットの特徴量の個数が多く、０にしたい場合
	L2が推奨されるのはデータセットの特徴量の個数が少なく、それらの値を小さくしたいが０にしたくない場合

Lassoなどのスパースモデリングは「あらゆるものごとに含まれる本質的な情報はごくわずかである(=スパース性　オッカムの剃刀)という仮定に基づき、データそのものではなく同士の関係性に注目することで、
少量のデータでも分析可能とする技術」のことです。
これはオッカムの剃刀の「ある事柄を説明するためには、必要以上に多くを仮定するべきでない」という考え方を背景にしています。
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★22. ROC曲線とモデル解釈・選択
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ROC曲線(受信者操作特性)
	真陽性率（TPR）と偽陽性率（FPR）の関係を表した曲線
	・真陽性率（TPR）＝TP /（TP＋FP）　再現率と同じ
	・偽陽性率（FPR）＝FP /（FP＋TN）
	→横軸をFPR(偽陽性率)、縦軸をTPR(真陽性率)

ROC曲線とAUC
	        Positiveと判断 Negativeと判断
	Positive    TP           FN
	Negative    FP           TN

AUC
 	ROC曲線よりも下の面積のこと
	→面積が１に近いほどモデルの性能は高いとされる
モデルの解釈
　	モデルが何かを予測したとき、どのような根拠を持って予測したのかを知ることも大切になる
	→モデルが複雑になればなるほど根拠はわかりにくくなる
	→モデルの局所的な解釈を可能にするアプローチがある
　	LIME、　SHAPなどの手法が考えられている
LIME★
	複雑なモデルを解釈しやすいモデル（線形回帰）に近似させる。SHAPの前に注目
SHAP★
　　協力ゲーム理論におけるShapley値を利用してそれぞれの特徴量が予測にどの程度を与えたかを算出
	ブラックボックスなモデルを解釈する手法の1つ。協力ゲーム理論を応用している
モデルの選択と情報量
	タスクに対して最適なモデルを選択していく
	→複雑なモデルを選択すれば複雑なタスクを実行できる可能性は上がるがコストも増大してしまう
	→「ある事柄を説明するためには、必要以上に多くを仮定するべきではない」（オッカムの剃刀）

・赤池情報量基準（AIC）
　　「モデルの複雑さと、データとの適合度とのバランス」を知るときに使われる指標の１つ
	→モデルをどの程度複雑にすればいいのか目安になる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★23. ニューラルネットワークとディープラーニング
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ニューロン

パーセプトロン
	パーセプトロンは入力された信号（複数あり）を処理して一つの値を出力します
	w0+w1x1+w2x2 ≥0 -> 1 w0はバイアス、w1,w2は重み
単純パーセプトロン
　	1958年提案されたニューラルネットワーク
　	活性化関数としてステップ関数が用いられる
	例えば、身長、体重といったデータを入力として受け取り、男性であるか女性であるかを判定する分類器を作ることができます。
		1を出力したときは男性、0を出力したときは女性
	一般の問題ではデータが線形分離可能であることはまずありませんので，パーセプトロンは残念ながら実用には向いていません。
多層パーセプトロン＝ニューラルネットワーク
　	多層化したニューラルネットワークのこと
	→非線形分類が可能になった
　	単純パーセプトロンは線形分類だけ
	→入力層、隠れ層、出力層に分かれている
　	隠れ層は中間層とも呼ばれている
	多層化する場合に新たに問題になるのは、予測値と実際の値の差分である誤差を最小化する、いわゆる最適化問題が複雑化することです。
ディープラーニング
	大規模なラベル付されたデータとニューラルネットワークの構造を利用して学習する
　	隠れ層を増やしたニューラルネットワーク（深層）を活用
　	層を深くすることで解ける問題が増えると考えたが、思ったような結果が得られなかった
	→信用割当問題・勾配消失問題など様々な問題を抱えていたため
	→問題が解決されたことで解ける問題の幅は広がった
信用割当問題
	最終的な結果に対して、どのパラメーターに責任があるのか、
	どのパラメーターを修正すればいいのかという問題
	→誤差があったとき何を修正すればいいのかという問題
	→誤差逆伝播法により信用割当問題は解決されたとされている
勾配消失問題
	ニューラルネットワークを多層化すると、誤差逆伝播法においてそれぞれの層で活性化関数の微分がかかることから、
	勾配が消失しやすくなり、学習が進まなくなる問題
	・誤差逆伝播法
　　　出力された結果と実際の結果の誤差を最小化するために出力層から入力層にかけて調整を行っていく方法

	→パラメーターを修正することで正しい値を高確率で導き出せるように
　		層が増えることで、入力層付近の調整が上手くいかない
	→誤差逆伝播法でパラメータを変更するとき活性化関数を微分した値を使用する
	→シグモイド関数の特性が影響している（0～1）
	→シグモイド関数の微分した値の範囲は（0～0.25）

	層が増えることで修正する値が小さくなってしまう
	→入力層付近では実質的に修正ができない状態になってしまう

	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋
    （◆勾配消失◆）は極端に層の多いニューラルネットワークで発生する問題である.（◆勾配消失◆）はニューラルネットワークの（◆重み更新◆）に影響を与える.このため,学習が停止したり,速度が著しく低下する.
    （◆ReLU◆）や 2015 年に提案された（◆バッチ正規化◆）はこの解決策の例である.しかし,1,000 層以上のような極端に層の多いニューラルネットワークにおいてもこの問題が解決されるのは,（ResNet.）が登場してからだった.
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★24. ディープラーニングのアプローチ
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
事前学習
	ニューラルネットワークの初期値に、オートエンコーダで学習させたものを用います。事前学習を行うことで、勾配消失による学習速度低下を防止します。
	多層化したニューラルネットワークはパラメータを効果的に更新することができないことから入気がなかった
	→ジェフリー・ヒントンが問題を解決する手法を提唱した
	→オートエンコーダ（自己符号化器）と呼ばれる方法になる事前に層ごとに学習を進める手法を事前学習という
オートエンコーダ(事前学習)
　	可視層と隠れ層の2層から構成されるネットワーク
	オートエンコーダでは、重要度の高い情報を洗い出し、それ以外の部分を削ぎ落します。次元削減の手法として注目されているが、生成モデルとしても有名
	たとえば、体格を表すのに、身長と体重の関係を二次元グラフ上のプロットで示したり、プロット結果を近似した一本の直線（一次元）で表したりすることがあります。
	→可視層は入力層と出力層を合わせたもの
	→データが可視層（入力層）から隠れ層、隠れ層から可視層（出力層）へ伝わっていく
	→可視層よりも隠れ層の次元は少ない
		入力と出力される情報が同じになるネットワーク
	→入力された情報が隠れ層で圧縮され、出力されるときに入力時と同じ情報になる
　		簡単に言えば、情報量を減らすこと（特徴をつかむ）
　		圧縮したデータはある程度もとに戻すことができる
	→入力層から隠れ層への処理：エンコード
　		隠れ層から出力層への処理：デコード
	入力ー＞エンコード            ->圧縮された特徴（潜在変数）→＞デコーダー             ->出力
           圧縮／ダウンサンプリング                          復元／アップサンプリング
	オートエンコーダは事前学習として使われていたが、それぞれのアルゴリズムで次元削減処理が含まれているので、最近では使われなくなった。
	今では次の用途で使われる
	・学習データのノイズ除去に使われる
	・異常検知の異常箇所特定
積層オートエンコーダ(事前学習)
	オートエンコードを何層も重ね合わせたもの
	→一気に学習させるのではなく、入力層から近い層から順番に学習していく方法になる
	→順番に学習していくことを事前学習という
		オートエンコーダを積み重ねても出力と入力は同じ値になるようにしているため教師あり学習にはならない
	→教師なし学習の状態になっている
	→オートエンコーダを積み重ねた最後に予測を行うための層を追加することで教師あり学習ができるようになる
	分類問題では、出力は入力データがどのようなクラスに属するのかを表す確率（0〜1の範囲）となる
	分類問題を解く場合、二項分類はシグモイド関数、多項分類の場合、ソフトマックス関数の出力層が追加される
	回帰問題の場合は出力層に線形回帰層が用いられる
	このように新たに出力層を追加した場合、出力層の重みを調整するためにネットワーク全体を学習して調整するファインチューニングが必要になる
ファインチューニング
	学習済みのモデルを利用し,新しく訓練したいモデルの訓練を行うことである
	ファインチューニングの場合,学習済みの層の学習率を低く設定することで,すでに学習済みのモデルの重みの変更幅を小さくしている.
	既存のモデルを使って,学習させたいモデルに対し,効率よく精度の高い学習をさせる方法である
	ファインチューニングとは,学習済みのモデルの最後の層のみを新しいタスクに適応するように学習することである.
	転移学習とは学習済みのモデルの重みを使用して別のモデルで再度学習させることである.
	ロジスティック層を最後に追加する★
	→シグモイド関数・ソフトマックス関数による出力層
	→回帰問題ならば、線形回帰層を最後に追加する
	→教師あり学習が可能になる
		オートエンコーダを積み重ねた最後に予測を行うための層を加えモデル全体で重みについて再学習させること（重みの調整）
	→最後の仕上げのことをファインチューニングと呼ぶ
　		出力層あたりの重みを調整するため誤差伝播法が有効
	→学習済みのモデルの学習率は低く設定しておく
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	学習に用いる画像データが十分量集まらないときは,既存のモデルを再利用し,◆ファインチューニング◆を行う方法がある.
	ファインチューニングを行う際は,ILSVRC で功績を残したネットワークの学習済みのものを利用するなどする.
	CNN の[◆入力層に近い中間層◆]ではタスクごとに比較的大きな差が生まれない汎用的な特徴が学習されるため,このような手法を用いることができる.
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
	入力層に近い中間層は全体的な特徴を捉え,出力層に近い中間層は個別の特徴を捉える.
	入力層は学習が行われる層ではない.
	出力層は学習が行われる層ではない.
深層信念ネットワーク★
　	オートエンコーダの代わりに制限付きボルツマンマシンを積み重ねたもの
ボルツマンマシン
　　可視層・隠れ層の各ユニットが全て互いに結合されているネットワーク
　	深層信念ネットワークは積層オートエンコーダと同様にジェフリー・ヒントンによって提唱された
	→現在は事前学習は計算量が膨大になるためネットワーク全体を一気に学習させるようになった
次元の呪い　扱うデータの次元が高くなるほど、計算量が指数関数的に増えていってしまうこと
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
転移学習やファインチューニングでは既存のモデルを使用し,　◆出力層の再学習◆　を行う.
転移学習では、新たに追加した出力層以外の再学習は行っていない。
事前に学習済みのモデルの重みを使用することで,精度を高めることを目的としている.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
25. ディープラーニングを実現するには
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ハードウエア
	人工知能の理論などは数多く存在したが、計算コストが高いことから実施できないことが多かった
	→半導体の性能が向上したことで計算コストは低くなった
ムーアの法則（ゴードン・ムーアが提唱）
	半導体の集積密度は18か月で2倍になるという経験則

CPUとGPU
　	CPU（Central　Processing　Unit）
　	パソコンの頭脳と言われており、汎用的な演算処理を行う装置
	複雑な命令を逐次計算
　	GPU（Graphics　 Processing　Unit）
　	画像処理が得意な装置（コア数が多い）
	→並列的な演算処理がCPUよりも得意

GPGPU
　	ディープラーニングでは同じような計算が大量になされている
	→GPUは並列的に処理をするのが得意であるため、ディープラーニングにとって相性が良いと考えられるが、GPUは画像処理に特化していたため改良する必要があった
	→GPUを画像処理以外の演算処理に応用するための技術が必要
	General-Purpose　Computing　on　Graptics　Processing　Unitｓ
	→NVIDIA社などがディープラーニング向けのGPUを開発している
　		汎用並列コンピューティングプラットフォーム（CUDA）を提供
　
TPU（Tensor　Processing　Unit）
　	Googleが開発した機械学習に特化した演算処理装置

ディープラーニングのデータ量
　	学習に必要なデータ量は明確に決まっていない
	→経験則としてモデルのパラメータ数の10倍は必要と言われている
　（バーニーおじさんのルール）
	→パラメータ数が1万ならば、データ量は10万が必要
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
ディープラーニングは特に画像認識の研究分野で大きなブレイクスルーをもたらした.
画像認識に利用されるモデルの中でもデファクトスタンダードとされつつある手法が Deep CNN である.
この特徴は,【畳み込み層】 を幾重にも内部に組み込んだニューラルネットワークを形成することであり,これにより【変形に対してより頑強な特徴表現を得ること】  に成功している.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
ディープラーニングはそれを実装する為に様々なソフトウェアのフレームワークが AI 研究に力をいれるIT企業によって開発されており,それらを用いて研究・実装することが一般的である.
Chainer
	日本国内においても PFN 社が提供する Chainer は人気を得ており,その特徴である Define by Run は高く評価されている.
	Define by Run  計算グラフの構築と同時にデータを流して処理を並行して実行する方式.データ構造によってモデルを変えやすい

TensorFlow
	google 社が提供
CNTK
	Microsoft 社が提供
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
ディープラーニングはロボット学習への応用も盛んに行われている.
中でもロボット学習へもたらした影響が大きいのは,【End to End Learning】 という方法が可能になったことであり,
これはある一連の動作を学習する際に従来は一つ一つの動作をステップバイステップで学習していたものを,
一つのネットワークとして表現することで全ての動作を一気に学習することができるようになったのは非常に大きいとされている.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
26. 活性化関数
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
活性化関数
	モデルの表現力を増すために用いられる。活性化関数を用いて変換を行うと様々な値の出力が行えます。
	その結果として高度な識別問題を解決できるようになる
	流行り　ステップ関数→シグモイド関数→ReLU関数
	出力層で用いられる活性化関数は出力を確率で表現するために、特定の活性化関数が用いられる
	ReLU関数は出力を確率として表現できない、分類問題の出力層に用いられない
	ステップ関数(階段関数)
		ステップ関数は1と0しか出力できません。そのため、情報のロスが激しく表現力に乏しいのが欠点です。
		y′≥0 => 1
		y′<0 => 0
	シグモイド関数（Sigmoid）
		2値分類で活用される（迷惑メールかどうかなど）。非線形な関数。
		座標点（0, 0.5）を基点（変曲点）として点対称となるS字型の滑らかな曲線で、「0」～「1」の間の値を返す
		微分できないステップ関数の代わりに、微分可能（differentiable）な「シグモイド関数」が活性化関数として採用された
		シグモイド関数はニューラルネットワークで用いられる活性化関数ですが現在はあまり使われていません。
		シグモイド関数を活性化関数として用いると勾配消失という現象が起こり、学習が停滞してしまうことがあるためです。
	ソフトマックス関数
	　	多クラス分類のときに使われる関数
		→モデルの出力の総和は１になる(0.5+0.3+0.2)

	恒等関数★
		回帰問題のときに使われる関数
		恒等関数は計算結果をそのまま出力する活性化関数である.
	tanh関数（ハイパボリックタンジェント関数、双曲線正接関数）
		誤差逆伝播法では活性化関数の微分を活用する
		→シグモイド関数の微分の範囲は0～0.25と値が小さいため層が深くなってしまうと学習が進まなくなってしまう
		　微分係数をかけるたびに伝播していく誤差の値は小さくなってしまう。その結果、入力に近いほど伝承すべき誤差がなくなる（勾配消失問題）
		→シグモイド関数以外の微分が可能な関数を使用すれば問題が解決されるのではないかと考えられた。
			-1から1の範囲を取る関数
		→【微分の最大値は1】であるため勾配が消失しにくい。あくまで最大値が大きいだけで根本の解決でない
		→シグモイド関数を【線形変形】したもの

	ReLU関数（Rectified Linear Unit関数）
		隠れ層の活性化関数
		Tanh関数はシグモイド関数よりも勾配消失が起きにくいが微分した値は１以下なので勾配は小さくなっていく
		→ReLU関数を活用することで勾配が消失しにくくなる
		→０以下の時は０，０以上のときは入力値をそのまま出力
		→従来の活性化関数よりも精度向上に貢献した
		y > 0 =>  y
		y <= 0 => 0

	Leaky ReLU関数★
		ReLU関数を改良した関数の１つ
		→入力値が０より小さい場合、入力値をα倍した値を出力、０以上の場合には入力値と同じ値を出力する関数

		微分した値が０にならないため勾配消失が起きにくい
		→他にReLU関数から派生した関数も存在する
		どれを使うかはケースバイケース

		・Parametric ReLU：αを学習によって決める
		・Randomized ReLU：αを範囲内でランダムに変化させる

	しかし,1,000 層以上のような極端に層の多いニューラルネットワークにおいてもこの問題が解決されるのは,ResNet（Microsoft）が登場してからだった
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
27. 学習の最適化（勾配降下法）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
最適化
	「関数の最小点を見つける」こと
勾配降下法
	関数の勾配にあたる微分係数に沿って降りていくことで、最小値を求める手法のこと。
	xから微分係数を引く
	xが収束する可能性がある値 大域最適解、局所最適解、停留点
	ニューラルネットワークでは、予測値と実際の値の誤差を近くするように学習をしていく
	→重みやバイアスなどのパラメータを最適化していくこと
	→誤差を誤差関数（損失関数）によって求め、誤差を最小化する
	　平均二乗誤差、交差エントロピー誤差などがよく使われる
	関数の最小の値を見つけるときに使われるのが微分
	誤差関数には多数の変数（パラメータ）が含まれているため微分ではなく、偏微分である（微分した値が勾配）
	→変数が増えると計算の難易度が一気に上がる
	→勾配が最小になる変数を見つけ出すアルゴリズムの１つが勾配降下法
	イテレーション
		パラメータを更新する回数のこと
		→学習してパラメータを更新したらイテレーション数が１★
	学習率
　		パラメーターを調整する値のこと。歩くときの歩幅のようなもの。
		勾配に沿って一度にどれだけ降りていくかを設定するもの★
		パラメーターz（更新後）＝パラメーターz（更新前）ー学習率＊パラメーターz（更新前）における勾配
		→学習率は大きすぎても小さすぎても良くない
		→学習率の設定は専門家が決めることが多い
		応用上ではより効率的な計算を行うため、学習の初めでは学習率を大きくし、学習が進むにつれて小さくするような工夫が行われる

	勾配降下法の種類
		１．	最急降下法
		２．	確率的勾配降下法（SGD）

バッチ学習
　	全てのデータの誤差をもとにモデルを学習

最急降下法
	学習データの全ての誤差合計からパラメータを更新する方法
	→学習データが多いと計算が多くなり時間がかかる
	→学習データが増えるたびに再計算する必要がある
	バッチ学習に相当する

確率的勾配降下法（SGD）手動
	ミニバッチ学習、オンライン学習に相当する
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	SGDはパラメータの更新ごとの計算量がとても少なく,古くからあるアルゴリズムだがシンプルで軽い.
	このためSGDは,SGDをベースにSGDの欠点を回避する工夫がなされたアルゴリズムも含め,現在でも非常によく利用されている.
	なお,現実的な時間で学習を完了するため,　◆ミニバッチ◆　とその並列計算を併用するのが普通である.
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
ミニバッチ学習
  （ランダムに抜き出す）データを小さなグループに分割し、グループの誤差をもとにモデルを学習

オンライン学習（逐一学習）
  （ランダムに抜き出す）１つずつのデータの誤差をもとにモデルを学習（くり返す）
	→時間がかかってしまう方法になる

    ディープラーニングでは,得られたデータの中からサンプリングした一部のデータのみを学習に利用するという手法が用いられる.
    また,この工程はイテレーションという単位で繰り返され,そのたびにサンプリングは新たに行われる.
    このような手法は（確率的勾配降下法）と呼ばれ,イテレーションごとにただ1 つのサンプルを利用する（逐次学習）や,
    一定数のサンプルを利用する（ミニバッチ学習）がそれに含まれる.（ミニバッチ学習）は,（逐次学習）よりも正しい解に収束しやすい傾向にある.
    また,データすべてを利用する手法は（バッチ学習）と呼ばれる.

モーメンタム（Momentum）
	SGDを改善。パラメータの更新に感性的な性質を持たせ、勾配の方向に減速・加速したり、摩擦抵抗によって減衰したりしていくようにパラメーターを更新
	物理の慣性の考え方が含まれて居る

AdaGrad (自動)=> RMSPro => Adam

エポック
	全ての訓練データを学習させた回数のこと
	例）2000個の学習データについて、バッチサイズ*500個でミニバッチ学習する場合、
	(勾配降下法などによる)重み更新を4イテレーション繰り返すと1エポック。

誤差関数（損失関数）
	損失関数は予測と実際の値のズレの大きさを表す関数でモデルの予測精度を評価します。
	損失関数の値が小さければより正確なモデルと言えます。
	この損失関数の評価をもとにモデルのパラメータを算出します。ニューラルネットワークをはじめとした機械学習モデルは損失関数の値が最小となるようなパラメータを様々な方法で求めます。
	例）平均二乗誤差（MSE）
誤差関数を二乗誤差としたとき,回帰のモデルの汎化誤差は【バリアンス（variance）】,【バイアス（bias）】,【ノイズ（noise））】の三つの要素に分解できる.
【バリアンス（variance）】はモデルが複雑になりすぎて過学習の状態にある時に高くなり,
【バイアス（bias）】は逆に単純すぎて未学習の状態にある時に高くなる.
【ノイズ（noise））】はデータ自体に混入しておりモデルの種類や学習方法を工夫しても取り除くことは困難.
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
28. 学習の最適化（問題点と対策）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
勾配降下法の問題と改善
	勾配降下法で勾配の最小値が見つからない場合も存在する
	・局所最適解
　		局所的な最小値
　		停留点の影響を受ける
	・大域最適解
　		本当の最小値
	局所最適解を防ぐ方法として、学習率の値を大きくする
	→小さな山を飛び越えることができるため、大域最適解を見つけられる可能性が高くなる
	→学習率の値が大きすぎると大域最適解を飛び越えることも
　	最適なタイミングで学習率を小さくする

	微分・偏微分を使用するため局所最適解以外にも停留点の影響で学習が進みにくくなることもある
	→停留点は局所最適解・大域最適解ではないが勾配が０になってしまうため

鞍点（あんてん）　
	パラメータが多くなると、ある次元では最小にも関わらず、別の次元から見ると最大になっている点のこと、
	３次元以上の関数に対して、勾配降下法を用いると、ある次元からみると極大点で、他の次元から見ると極小点となる点
	→鞍点の近くは平たんになっていることが多く、学習が進みにくくなってしまう
	→【プラトー】と呼ばれる現象★

モーメンタム（Momentum）
	鞍点問題に対するために考えられた手法
	→慣性の考え方を適応し、学習の停滞を防ぐとされる
		前回の更新量を、現在の更新量に反映させる
	→モーメンタムよりも効率的なアルゴリズムが考えられた

局所最適化を避ける最適化アルゴリズムの1例
	Adam：RMSpropを改良した手法
	AdaBound：Adamを改良した手法
	AMSBound：AdaBoundと同様のアイデアをAMSGradに適用した手法
	AdaGrad：確率的勾配降下法（SGD）の改良した手法、勾配降下法で学習率を自動で更新
	AdaDelta：AdaGradを改良した手法
	RMSprop：AdaGradを改良した手法
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
ディープニューラルネットワーク（DNN）のパラメータ更新の際,最適化アルゴリズムとして　◆確率的勾配降下法（SGD）◆,RMSProp,Adamなどが用いられる.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

ハイパーパラメータ
	機械学習モデルにおいて人間があらかじめ設定するパラメータ
	→学習率、ニューラルネットワークの層の数など
	→予測の精度に大きな影響を与える
	人が経験で決めることが多いが、最適な設定ができるように自動化する方法が考えられている
	ランダムサーチやグリッドサーチにもとづいて実験を繰り返し学習率などを決定していく
	・ランダムサーチ
		指定された分布に従ってランダムにパラメータを抽出し学習を行って、最適なパラメータを探す方法
	・グリッドサーチ
		あらかじめパラメータの候補地を設定し、パラメータの組み合わせから学習を行って、最適なパラメータを探す方法
		→ランダムサーチよりも日値が関与できる幅が広い
		指定したパラメーターの全ての組み合わせを試す手法。組み合わせの総数分モデルの学習を行うので、探索が終わるのに時間がかかる
	ハイパーパラメータ例
		学習率　訓練時に使うステップのサイズ
		エポック数　訓練時に使うステップの数
		勾配降下法　訓練プロセスに一度に与えるデータ点の量。データ点を1つづ与える確率的勾配降下法、いくつかに分けて与えるかミニバッチ勾配降下法、全て一度に与える（バッチ配降下法）
		層の数、ノード数、活性化関数の選択
		正則化パラメータ
		ドロップアウト確率数
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★29.ドロップアウト
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ドロップアウト
	→過学習を防止する方法としてドロップアウトがある
	→学習時に、【ランダム】でユニット（ノード）を除外して学習する方法
	一時的にネットワークのかたちを変化させるので、形の違う複数のネットワークを学習することになる
	ランダムにユニットを無効化させて学習させていくためアンサンブル学習と同じような効果が期待できる
	→ネットワーク全体で訓練データに最適化されすぎるのを抑えることができる方法になっている

早期終了（arrly stopping）
　	過学習が起きる前に学習を終了する方法（汎用性が高い,どんな形状のネットワークでも適用化）
	→汎化誤差が増加し始めたら学習をやめていく
	訓練誤差：訓練データに対する予測と正解の誤差
	汎化誤差：学習に使っていないデータに対する予測と正解の誤差
	・二重降下現象
　		汎化誤差が増えても、そのあと汎化誤差が減っていく現象
　		どのようなタイミングでやめるのかは難しい問題
	→早期終了は過学習を抑えるテクニックとしてかなり優秀
ノーフリーランチ定理
　	あらゆる問題を効率的に解く汎用的な方法はないということ
	→早期終了を「Beautiful FREE LUNCH」と表現している。ノーフリーランチ定理に反している

データの正規化・重みの初期化
　	正規化
　		効率的に学習が行えるようにデータを調整する
		→各データの最大値や最小値が大きく異なってしまうとパラメータに偏りが生じてしまうとパラメータに偏りが生じてしまう
		→データのスケールを合わせていく作業になる（体重と身長）
		それぞれの特徴量を最大値で割って特徴量を0～1の値へ

　	標準化
		各特徴量の平均を０，分散が１になるように変換すること
		→正規化よりも高い効果があるとされている

	白色化
　		各特徴量を無相関化し、標準化すること
		→計算コストが大きくなるので標準化を使うケースが多い
		無相関：データ間の相関をなくすること。斜めの相関線が横線の無相関になる
重みの初期値
	ディープニューラルネットワークについて
		データを正規化・標準化しても層を伝播していくうちにデータの分布が崩れてしまい良くない結果になってしまう
		→データの分布に影響が少ない重みの設定が大切
		→最適な重みはわからないのでランダムな値になってしまう

	シグモイド関数　：Xavierの初期値
	ReLU関数　：Heの初期値　が良いとされる

バッチ正規化
	隠れ層に入力する値を正規化する方法。前の方から伝わってきたデータに対してもう一度正規化を行う。
	バッチ正規化は各層において活性化関数をかける前に伝播してきたデータを正規化する処理である.
	正規化することで極端に小さな値をクリッピングできるので,勾配消失の解決例と考えられる.
	→過学習が起きにくくなるとされている
	→重みの初期値を工夫するよりも各層に伝わるデータを正規化しようと考え方
	→活性化関数の前にデータを正規化する
	バッチ正規化を行うと勾配消失が起こりにくく,学習を高速化する.
	バッチ正規化を行うと初期値への依存が少なくなる
	各層の出力を正規化するので,外れ値が少なくなり過学習が起こりにくい.
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★30. CNNの基本用語
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ニューラルネットワーク
	ニューラルネットワークは画像や時系列データなどそれぞれに特化したものが数多く存在する
	→画像データを扱う場合は
　		畳み込みニューラルネットワーク（CNN）がよく使われる
	→言語処理、音声処理などは他のニューラルネットワークが使用
	隠れ層のユニット数が増えるほど,複雑な関数を近似する能力が上がる

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
隠れ層の数を増やすと（層を深くすると）,訓練誤差は必ず小さくなる
隠れ層のユニットが増える分,ユニットに接続される重みの数が増えるため,表現能力が向上する.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
ＣＮＮの基本形
	ＣＮＮは人間の視覚野にヒントを得て作成されたネオコグニトロンをベースに持つ.
	ＣＮＮ（畳み込みニューラルネットワーク）は画像処理の分野に特化したニューラルネットワークである
	→OCR、パターン認識など幅広い分野で使用されている
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	 CNN（Convolutional Neural Networks）は　順伝播型ニューラルネットワーク　の一種であり,
	 入力画像の各位置と結合して積和計算や活性化関数に	よる変換などを行う畳み込み層や
	平均値や最大値などを用いて入力データをサブサンプリングするプーリング層を持つことが特徴である.
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

OCR（光学的文字認識）
	手書きや印刷された文字を読み取りデジタル化する技術

パターン認識
	画像や音声など様々なデータから一定のパターンを見つけ識別すること
	→音声認識、画像認識、文字認識などで使われている
	→ＯＣＲも文字認識の１つになる
	画像は2次元データ（厳密には3次元データ）なので2次元データのまま処理をする方が望ましい
	2次元を１次元にすると、画像に映っている位置情報がが失われる。
	そのため、畳込み演算を用いることで、2次元のまま入力できるようにしている
	（周辺のピクセルの位置関係で位置がわかるため）
	→画像は上下左右、位置が重要な意味を持つため
		一列にしてしまうと画像の持つ意味を失ってしまう
	→画像は赤色、緑色、青色の３色の情報を持っている
	人間の視覚野に関する神経細胞をもとに作られたモデル

	単純型細胞（S細胞）：画像の特徴を抽出する　画像の濃淡パターンを検出する
	複雑型細胞（C細胞）：物体の位置が変わっても同一の特徴とみなす
畳み込み演算
	画像    カーネル    処理後特徴マップ
	123      0  -1  =>  2 2
    456      1   0     -2 4
    789
ネオコグニトロン★
	→単純型細胞、複雑型細胞のはたらきをもとに、ネオコグニトロンと呼ばれるモデルが作られる（福島邦彦氏）
 	S細胞層とC細胞層を組み込んだモデル（初期のCNNモデル）
	S細胞層とC細胞層が交互に組み合わせた構造
	勾配計算を行わないadd-if silentと呼ばれる方法によって中間層の学習が行われる

LeNet★ニューヨーク大学
	1998年に、ニューヨーク大学のヤン・ルカンによって誤差逆伝播法を学習に用いた【LeNet】と呼ばれるCNNのモデルが発表される
	→入力層・畳み込み層・プーリング層（サブサンプリング層）・全結合層・出力層で処理

	ネオコグニトロンと似た構造をしている
	・Ｓ細胞層：畳み込み層
	・Ｃ細胞層：プーリング層　に対応している
	→学習方法は少し異なる、
	ＣＮＮは誤差逆伝播法を用いる

	特定のユニット同士のみが結合している特徴
	→局所結合と呼ばれる
	→重みの共有がされておりパラメータ数が少なくて済む
確率的ニューラルネットワーク
	確率的ニューラルネットワークには制限ボルツマンマシンやDeep Bielif Network(DBN)などが存在する.
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★31. 畳み込み層・プーリング層・全結合層
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
畳み込み層
	フィルタ（カーネル）を使って画像の特徴を抽出する層
	→畳み込み層と言われている
	→カーネルは基本的に画像サイズよりも小さいサイズのものを使う
		カーネルのサイズをカーネル幅と呼ぶ
	フィルタの値は学習することによって変化していく（重み）
	→全体の特徴を取り出すため、対象の位置がズレていても同じ対象として扱うことが可能になる
	→畳み込み層があることで位置のズレに強いモデルになる
	視覚野の局所受容野に対応している
	カーネルは1枚でなく、実際は複数用意する
	・ストライド
		カーネルをスライドさせる量のこと
	・パディング
		作成する特徴マップの大きさを調整するために、画像データを周りに値を入れること
プーリング層
	畳み込み層で出力した特徴マップをルールに従って小さくする（圧縮）
	→ダウンサンプリング、サブサンプリング　と言われる
	→プーリングの操作には
		マックスプーリング（最大値プーリング）と
		アベレージプーリング（平均値プーリング）という操作がある
プーリング演算
	特徴マップ       最大値プーリング
	12 36       =>  6 9
    56 69           8 9
                   平均値プーリング
    78 91          3.5  8
    35 21          5.7 3.2
	物体の位置が異なっていたとしても似た特徴量になるため位置のズレに強くなる
	→【畳み込み層と異なり重みはない】ため学習しても変化はしない
	プーリング層では機械的に計算が行われている
	そのため学習によって最適化されるパラメータ（重み）を持っていない
全結合層
	抽出した特徴マップを一列に並べる処理を行う層
	→画像に対する答えを出力するために
		特徴を2次元から一次元に変更する必要があるため
		そのままでは１次元の正解ラベルと比較ができない
	→シグモイド関数などを使って識別を行っていく
	 多層パーセプトロンに用いられている層と同じ構造をしている
	全結合層は出力サイズを調整するために使用されることが多い.

グローバルアベレージプーリング（GAP)(Global Average Pooling）★
	最近では全結合層を使わず、特徴マップの平均値を使う処理がよく使われている
	2次元の特徴マップの平均をとり１次元にし、分類したいクラスと1対1とし、誤差を平均できるようにする手法
	→全結合層を使用するよりも【パラメータの数が少なくて済み】
	過学習が起きにくいとされている

畳み込み層は単純型細胞群に対応し,データの特徴を異なる特徴マップとして表現する.
複雑型細胞群に対応するプーリング層は,畳み込み層で得た特徴の位置をぼかすことで,特徴の出ている位置が少々ずれていても許容できるようにする.
この,特徴量の抽出と特徴出現位置のぼかしを繰り返すことで,変形などにも強い特徴表現ができる.
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★32. データの拡張と発展
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
データ拡張
	精度の高いモデルを作っていくためにはデータのバリエーションを豊富にする必要がある
	→物体の角度、光の当たり方など、集めることは難しい
	→画像から別の画像を作成していく方法がある（水増し）
	コントラストを変更、左右逆転させるなど
	Cutout
		画像の一部の値を０にする。正方形領域を固定値0でマスクする手法。ランダムな値ではない。

	Random Erasing
		画像の一部を【ランダムな値】にする
	Mixup
		2枚の画像を合成して新しい画像を作る
	CutMix
		CutoutとMixupを組み合わせたもの。CutOutの改良版であり，画像を2枚用意し，一つの画像からもう一つの画像へコピーすることで学習サンプルを増やす手法

	データのバリエーションを豊富にすることは大切だが現実に存在しないデータを生成しても意味がない
	→現実味のあるデータを生成して学習させることが大切
	→手書きの文字６を１８０度回転させるような変換は不適切
CNNの発展形
	画像認識では、畳み込み層とプーリング層を繰り返して精度を上げるアプローチがとられている
	→画像認識の精度を競うコンテスト（ILSVRC）でAlexNetが圧倒的な精度を出し、優勝した（トロント大学）

AlexNet(2012 1位)トロント大学
	畳み込み層→プーリング層→畳み込み層→プーリング層→畳み込み層→畳み込み層→畳み込み層→プーリング層→全結合層→全結合層→全結合層（出力層）
	2012 年の ILSVRC で従来手法のモデルに大差をつけて優勝し,深層学習が注目されるきっかけを作ったモデル
	5 層の畳み込み層と 3 つの全結合層を持つ CNN で,ReLU 関数を用いることで,シグモイド関数の問題（勾配消失問題）を回避している他,過学習対策としてドロップアウトが使われるなどしている.

GoogLeNet(2014 1位)
	ILSVRC の分類部門と検出部門で優勝で優勝したモデル（22層、Google社）
	→インセプションモジュール(Inceptionモジュール)と呼ばれる小さなネットワークを積み上げた構造

VGGNet(2014 2位)オックスフォード大学
	ILSVRCで準優勝2位したモデル（16層、19層）オックスフォード大学が提出したモデル.
	サイズの小さな畳込みフィルタを用いて計算量を減らしている

インセプションモジュール(Inceptionモジュール)
	複数のフィルターサイズで畳み込む処理を並列で行う処理
	層を増やすと誤差が逆伝播しにくくなっていった

ResNet(2015 1位)
	ILSVRCで優勝したモデル（Microsoft）
	→スキップコネクション(Skipconnection)を加えることで層が深くなっても学習が上手くいくようになった
	様々な長さのネットワークが存在するため、アンサンブル学習になっている
	1,000 層以上のような極端に層の多いニューラルネットワークにおいてもこの問題が解決されるのは,ResNetが登場してからだった.
	最大１５２層

スキップコネクション(Skipconnection)
	層を飛び越えた結合

MoblieNet
	層が増えることでパラメータの数が増えた計算量が増加
	→スマートフォンなどでも利用できるようにパラメータ数などを削減するために作られたモデル。小型端末専用ではない
	→Depthwise　Separable　Convolutionという手法が使われている
	Googleによって発表された
	【畳み込みの計算を分割することで，計算量の減少を達成した．】
	ハイパーパラメータとしてwidth multiplier と resolution multiplier を用いる．

Neural　Architecture　Search（NAS）
	ネットワーク構造を自動的に最適化すること
	→カーネル、カーネルのサイズなど専門家が決めていたものを深層強化学習などを用いて最適な値を見つけ出す
	→人が最適なネットワーク構造を探し出すのは困難
	学習によってネットワーク構造を探し出す
	ベイズ最適化や遺伝アルゴリズムはパラメータ最適化を行う．NASはパラメータ最適化の前段階であるネットワーク構造の最適化も行う
	ニューラルネットワークの構造自体がパラメータと重みを最適化する．
	膨大な計算量が必要な点が改善点とされている．
	AutoMLを実現するための理論である．

NASNet
	ResNetのResidual　BlockをベースにしたNAS
	CNNの畳み込みやプーリングをCNNセルと定義し，CNNセルの最適化を行う

MnasNet
	モバイル機器での計算量も考えられたNAS
	Googleによって発表された
	AutoMLを参考にしたモバイル用のCNNモデル設計である
	速度情報を探索アルゴリズムの報酬に組み込むことで，速度の制約に対処した．

EfficientNet
	Google社が発表したモデル
	→ネットワークの幅・深さ・解像度を１つの複合係数を使い少ないパラメータ数で精度の高い処理を行う
	モデルの深さ、広さ、入力画像の大きさをバランス良く調整しているのが特徴

Deep Residual Learning
	残差ブロックを導入することで勾配消失問題に対処している.
	Deep Residual Learning(ResNet)とは,2015年のILSVRCで優勝したモデルであり,152層ものニューラルネットワークで構成されている.
	残差ブロックを導入することで勾配消失問題に対処し,飛躍的に層を増やすことに成功した.

Depthwise Separable Convolutionでは通常のConvolutionに比べて，計算量が少ないことで知られている．
Nチャネル数のデータに一辺のサイズがKのM個のフィルタを畳込み演算するときのパラメータ数はM×K×K×Nである．

一方，Depthwise Separable Convolutionでは（M＋K×K）Nとなる．32枚のフィルタで3×3畳込み演算を行う場合，
Depthwise Separable Convolutionのパラメータ数は通常のConvolutionの約何倍になるか．
1/9
Depthwise Separable Convolutionは（1/（K×K）＋1/M）だけパラメータ数がへるので，約1/9となる．

転移学習とファインチューニング
	・転移学習
		ある領域で学習させたモデルを他の領域で活用する方法
		→学習済みのモデルの後ろに新しいモデルを追加する　など
		→０からモデルを構築するのはコストがかかりすぎるため
		すでに学習済みのモデルを活用する方が効率的
		基本的にニューラルネットワークでは入力層に近い中間層では全体的な特徴を捉え、出力層に近い中間層は個別の特徴を捉える
		そのため転移学習やファインチューニングではこの部分をそのまま利用し、出力層の後に新たな層を追加したり、出力層を置き換えて調整することで
		目的の問題に適応させると効果的だと考えられる
		→入力層に近い中間層ではタスクごとに大きな差は生まれにくいため転移学習が可能になる
	・ファインチューニング
		学習済みのモデルに新しいモデルを追加したとき、パラメータを調整するために学習すること
	→画像認識などの分野では学習済みのモデルが公開されている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★33. 深層生成モデル
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
生成モデルの考え方
	・識別モデル
		画像などを分類するモデルのこと
	・生成モデル
		画像など新しいデータを生成するモデルのこと
		→新しいデータを作り出すタスクを生成タスクと呼ぶ
深層生成モデル
	ディープラーニングを取り入れた生成モデル
	→ディープラーニング以前から考え方は存在したが、
	 複雑なデータを生成することは難しかった
	→変分オートエンコーダ（VAE）、敵対的生成ネットワーク（GAN）が代表的なモデルになる
画像の生成は訓練データから画像が持っている潜在空間を学習し、ベクトルに格納
	・潜在空間★
		画像の特徴量を分布させた空間のこと
変分オートエンコーダ（VAE）「教師なし学習」
	オートエンコーダを活用した深層生成モデル
	オートエンコーダは同一の入力に対して完全に同一の潜在変数を獲得するのに対して、変分オートエンコーダでは、入力に対して【確率的に】出力が決まります。
	通常のオートエンコーダは、入力画像を圧縮し、特徴をとらえたより低次元のベクトルで表現することができるもの
	VAEでは画像を生成する潜在変数（入力データの特徴を圧縮したベクトル）の分布を学習し、入力画像を【平均と分散】に変換します
	変分ベイズ推定法の一種
	→統計分布に変換するのが特徴（平均と分散で表現）
	→入力データをエンコードが乱数などを使って潜在変数に変換、潜在変数をもとに新しいデータを生成する
敵対的生成ネットワーク（GAN）「教師なし学習」
	生成モデルであり、データの特徴を抽出して学習し、実在しないデータを生成できる。
	変分オートエンコーダやボルツマンマシンより鮮明な画像を生成
	ジェネレータとディスクリミネ－タで構成される生成モデル
	・ジェネレータ ：ランダムなベクトルを入力し、画像データを生成する
	・ディスクリミネータ：画像の真偽を予測
	ジェネレータとディスクリミネ－タで競い合わせて、より精度の高い画像を生成する
	GANは学習にはミニバッチを使う

・DCGAN 畳み込みニューラルネットワークを利用したモデル
	ジェネレータは畳み込みと逆のことをしている。復元し、生成する

・Pix2Pix
	ベクトルではなく、画像データを入力して条件にもとづいて別の画像に変換する
	→元の画像、変換した画像、実際の画像（正解とする画像）から真偽を判定する（多く画像が必要になる）
	「白黒写真をカラー写真に変換する」は、Pix2Pix（Cycle GANより）がむいている
・Cycle GAN
	CycleGANは教師なし学習であり，1対1のサンプルを集めづらい学習に向いている
	2組の画像を使い、一方の画像から他方の画像を生成し、他方の画像から一方の画像に戻した時に（サイクルした時に）精度が高くなるように学習させます。
	→変換した画像の真偽を判定する
	  元の画像と再変換した画像が一致するように学習
	→画像のペアが必要のない方法
	「冬にとった風景写真を夏のものに変換する」「馬の画像をシマウマに変換する．」「絵画を風景写真に変換する．」はCycle GANが（Pix2Pix）むいている

制限付きボルツマンマシン（RBM）
	同一層では情報を伝達しない。入力層と隠れ層の２層のみ。出力層なし。もし制限をつけないと、複雑になりすぎる。
WaveNet	音声生成で大きな影響を与えたモデル
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★★34. 画像認識分野①
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
画像認識分野
	入力した画像を識別する以外にも一般物体検出・画像セグメンテーション・姿勢推定などがある
ディープラーニングの画像認識手法は
以下の5種の分野に大別して考えられる.
・クラス分類："犬","猫"など単一のラベルを出力する.
・物体検出：領域とラベルを出力する.
・物体セグメンテーション：画素ごとにラベルを出力する.
・画像キャプション生成：画像の意味合いを理解し,キャプション（脚注）を出力する.
・画像生成：画像そのものを生成する.

クラス分類
	層を沢山重ねた深い層であってもうまく学習ができるように出力を入力と入力からの差分の和としてモデリングしたネットワークの枠組み ResNet（Microsoft）が提案され高い精度の識別性能を誇っている
物体セグメンテーション
	対象とする物体とその周囲の背景を境界まで切り分けるようなタスクを行うもの
	画像内に表示されている女性を認識し,「青い服を着てスマートフォンをいじっている」などのようにその対象が何をしているかを表示させることができるようになりつつある
画像生成
	2015 年に google 社が通常の画像をまるで夢に出てくるかのような不思議な画像に変換して表示する Deep Dream というプログラムを発表し話題を呼んだ
物体認識タスク
　	入力した画像が何かを識別するタスクのこと
	→確信度が高い物体クラスを結果として出力する
	→犬の画像を入れた場合、
　		犬の確率80％、オオカミの確率10％のように確信度が出力される
	２０１２年ILSVRCでディープラーニングを活用したモデル（AlexNet）が圧倒的な成績を残し優勝した
		→コンテストではImageNetを使っている
		→ImageNetには数百万の画像データがあるが、全てのラベルが正しいわけではない、中には誤りのラベルも
ResNet（Microsoft）
　	人間の識別精度を超えた（限られた条件）
	→カーネル数を増やしたWide　ResNet、スキップコネクション（スキップ結合）を改良したDenseNetなどが有名なResNetの派生モデルである
DenseNet
	前方の各層からの出力すべてが後方の層への入力として用いられるのが特徴でDense　Blockと呼ばれる構造をもつ
SEＮet(Squeeze-and-Excitation Networks)
	畳み込み層が出力した特徴マップに重み付けする
　　Attention機構を導入したモデル
	→Attention機構は汎用的なアイディアであり、VGG,ResNetなど多くのモデルに導入が可能
物体検出タスク
	画像に写っている物体を検出するタスクのこと
	画像内に含まれるとある物体を取り囲むようなボックスを推定するタスクを行うもの
	→物体が写っている領域を囲み（矩形領域（バウンディングボックス）、四角形）
	  物体クラスを識別するタスク
	→ディープラーニングにより、複数の物体クラスを同時に見つけることが可能になった
	・一般物体検出の代表的な手法
　		・R-CNN
　		・Fast　R-CNN
　		・Faster R-CNN
 　		・FPN
		・YOLO
		・SSD
R-CNN
　	セレクティブ・サーチと呼ばれる手法を活用し画像から物体候補領域を抽出する
	→候補領域の画像を一定の画像サイズにしてCNNに入力して特徴マップを出力
	→特徴マップをSVMによりクラスの識別を行う
	R-CNNは 画像の中にたくさんのボックスの候補をリストアップし(region proposal、max2000)、 それらのどれかが実際に物体に対応しているかどうかを調べます。

Fast　R-CNN
	R-CNNより高速化されたモデル
	→R-CNNと異なり画像全体をCNNに入力して特徴マップを作成、特徴マップごとにクラスを識別
	→候補領域の数だけCNN処理するのは時間がかかっていた

Faster R-CNN
	Faster R-CNNでは候補領域を抽出する方法としてセレクティブ・サーチを利用している
	→Region　ProPosal　Network（CNN）に置き換えたモデル
	→処理がさらに高速化された

YOLO
	候補領域を見つけ出し、物体クラスを予測する方法だった
	→画像をCNNに入力するだけで、物体の認識と物体領域の切りだしを同時に行う
	検出できる物体の数は2つ

SSD
	YOLOと同じように物体の認識と物体領域の切りだしを同時に行う手法になる
	→特徴マップを上手いこと活用することで精度を高めている

セレクティブ・サーチ
	今はつかわれていない古い技法

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★★35. 画像認識分野②
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
セグメンテーションタスク
	画像を画素（ピクセル）単位で複数の領域に分類・識別を行うタスクのこと

	・セマンティックセグメンテーション★
		同一ラベルでも個が区別されないという特徴がある
		画像上のすべてを対象にする
		インスタンスセグメンテーションでは検出が困難な空や道など,形が不定の対象であっても検出可能となる場合がある.
		→同一クラスの物体は１つにまつめられる
		→猫が複数匹写っていても、個々の物体として区別されない
　			クラスごとに識別される
		画像内の有界距系として出力

	・インスタンスセグメンテーション★
		矩形を基本とした境界ボックス単位で物体を抽出する手法であり,
		YOLOから派生したYOLACT++はこの手法を採用した物体検出用の深層学習モデルである.
		同一ラベルであっても個が区別される
		画像に写っている物体（検出した物体）を対象にする
		→同一クラスでも、別の物体として識別することができる
		→猫が複数匹写っていたら、個々の物体として区別できる

	・パノプティックセグメンテーション★
		googleチームより発表された
		ビルや道路にはセマンティックセグメンテーション
		人などにはインスタンスセグメンテーション　を行う

セグメンテーションの代表的な手法
	・セマンティックセグメンテーション
		FCN、Segnet、U-net、PSPNet　など

	・インスタンスセグメンテーション
		Mask　R-CNN　など

FCN（完全畳み込みネットワーク）
	一般的なCNNは全結合層を用いるが、全結合層を畳み込み層に置き換えたネットワーク
	→畳み込み層により特徴マップは小さくなるため入力画像サイズまで拡大する処理が必要（アンサンプリンク）
セグネット（Segnet）
	画像のセマンティックセグメンテーション用に開発された深層学習モデルの一つである.
	モデルの構造について,エンコーダ－部分はCNNモデルであるVGG16の一部が採用されたシンプルな作りとなっている.
	デコーダー部分では,エンコーダーによって,インプット時よりも画素数が小さくなった画像を【Up sampled】層と呼ばれる層を通過させることで,インプットと同じ大きさに復元をする.
	全結合層を持たず、畳み込み層で構成されている
	→特徴マップを小さくするエンコードと大きくしていくデコーダが対照的に配置されている
Up sampled層はプーリング層と対照的な振る舞いを与える層であり,指定したカーネル周りの画素の中で最大の値を有するセルを特定し,その周りをパディングする仕組みを有している.

U-net
	U-Netでは,【Skipconnection】と呼ばれるエンコーダー部で取得した画像の特徴マップを,デコーダー部で再活用することのできる構造が採用されているため,
	プーリングや畳み込みといった層を通過する際の情報削減に強い特徴を有したセマンティックセグメンテーションモデルである.
	U-Netは,ネットワーク図を描くとエンコーダー部とデコーダー部で行われているタスクがきれいに左右対称に写り,アルファベットのUに見えることから名のつけられたことでも知られている.
	エンコーダとデコーダ構造のネットワーク
	→デコーダで特徴マップを拡大するとき対応するエンコーダで作成される特徴マップを利用する

PSPNet
	エンコードとデコードの間にPyramind　Pooling　Moduleが設けられたモデル
	→特徴マップを【複数の解像度】でプーリングする

DeepLad
	Atrous　convolution (Dilated convolution、Dilation convolution)を導入したモデル
	・Atrous　convolution
　		入力画像のピクセルの間隔をあけて畳み込み処理を行う
		→計算量、パラメータが少なく広い範囲の情報を集約できる

Dilation convolution
	通常の畳み込み層では畳み込む先のフィルターは密な構造を定義するが,Dilation convolutionでは,畳み込み先に疎な部分がある構造を定義する.
	疎な部分はできるものの,同一のパラメータ数を有する通常の畳み込み層とくらべ,一度にスキャンできる領域が大きいことや総計算量が削減できる利点がある.
	カーネル自体は畳み込み層と同様に行列を定義するが, スキャンの際に一定の間隔で画素を読み込まないことで結果的に広範囲の領域を一度にスキャンすることができる仕組みである.

DeepLad　V3＋
　　ASPPを採用したモデル

ASPP
　　PSPNetのように複数解像度の特徴マップを使う手法

Mask　R-CNN
	物体検出とセグメンテーションを同時に行うことができる
	→物体検出はFaster　R-CNNを活用し、セグメンテーションはFCNを活用している
　	インスタンスセグメンテーションの手法になる
	→１つのモデルが複数のタスクをこなすことをマルチタスク
姿勢推定
	人間の頭や足などの関節位置を推定すること
	→監視カメラの映像から不審な行動を発見する　など
	→Open Poseが代表的なモデルになる

Open Pose
	【Open Pose】はカーネギーメロン大学のZhe Caoらが2016年に論文発表した,2D画像の複数人物の姿勢を可視化し,効率的に推定するモデルである.

	手法の概要として,まず入力画像から部位の位置の推定（S・confidense maps）と,部位の連関を表す（L・【Part Affinity Fields（PAFs）】）を算出し,
	その後confidense mapsと【Part Affinity Fields（PAFs）】の集合から同じ人物の部位を組み合わせ、姿勢の状態を出力する.
	複数の人間の関節位置を推定できる（口や鼻なども分かる）
	Parts　Affinity　Fieldsと呼ばれる処理がなされている

	人間の姿勢推定には大きく分けてTop-downとBottom-upの2つのapproachがある.
	Open Poseはカーネギーメロン大学のZhe Caoらが2016年に論文発表した,2D画像の複数人物の姿勢を可視化し,効率的に推定する,Bottom-up approachのモデルである.


マルチタスク学習
	マルチタスク
　	複数のタスクを単一のモデルで課題を解決すること
	・Mask　R-CNN
　　物体検出とセグメンテーションを同時に行うことができる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★★36. 音声データの扱い方
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
データの扱い方（時系列データ）
	時系列データ
	気温、株価の動き、人口動態など時間的に変化した情報を保有しているデータのこと
	→ある時点の情報は、その時点よりも前の情報に影響を受ける
　	データを時系列にそって学習させる必要がある
データの扱い方（音声データ）
	音声認識
　		人間が発したアナログなデータをデジタルデータを変換
		→アナログデータからデジタルデータに変換することを
　		　アナログデジタル変換（A-D変換）
		→一般的にパルス符号変調（PCM）でデジタルに変換
		　音声データから音声認識で必要な特徴量を取り出す

パルス符号変調
	パルス符号変調は,アナログ信号のデジタル化に用いられる変調方式であるが,自然界の現象のアナログ信号をデジタル信号に変換する際,以下の三つの工程を踏む.
	標本化→量子化→符号化
	標本化：アナログ信号を一定時間ごとに区切り、その値を読み込むこと（サンプリングとも呼ぶ）
	量子化：標本化し読み込んだ値をデジタル信号に変換できるように加工すること
	符号化：量子化された値を指定された二進コードなどで符号化すること

高速フーリエ変換
		周波数成分として分解することをなるべく少ない計算量で行うのが高速フーリエ変換である.
		あらゆる周期の関数は正弦波の足しあわせで表現できるという特性を利用し、周波数成分に分解することが可能であるが,
		これをなるべく少ない計算量で行うのが高速フーリエ変換（FFT/Fast Fourier Transformation）である.
		→高速フーリエ変換（FFT）により
　		　音声信号を周波数スペクトルに変換する
		スペクトル：複雑な信号を成分に分解したもの
		音色は音の違いを認識する上で大切

音声信号スペクトル
	音声スペクトルから得られる情報として,声帯の振動に対応した、その人個人が持つ「声の高さ」である【音声信号スペクトル】と
	声道・鼻腔における共振・反共振特性に関連した各音韻ごとの違いである【音声信号スペクトル】の2種類に分けられるが,
	音声認識などに使用する音韻の識別には【音声信号スペクトル】のみが必要である.
		→同じ高さの音でも音色によって違う音と感じる（楽器）
		→音色の違いは周波数スペクトルのスペクトル包絡の違いと解釈
　		　スペクトル包絡を知ることが大切とされる

メル周波数ケプストラム係数（MFCC）
　	スペクトル包絡を求めるために使われる方法
	メル周波数ケプストラム係数は音声認識や音楽ジャンル検索などで使われる特徴量であり,人間の聴覚特性を考慮した周波数スペクトルの概形を表している.
	メル周波数ケプストラム係数は楽器音に対して音色に対応しており、音色が異なるとメル周波数ケプストラム係数の形状は異なることが予想される.
	メル周波数ケプストラム係数（Mel-Frequency Cepstrum Coefficients: MFCC）という, 音声認識の特徴量を示す係数である.

ケプストラム
	周波数スペクトルを信号と見なしフーリエ変換して得られる信号である（スペクトルのスペクトル）
	音声の波形をフーリエ変換し, 対数を取り, 逆フーリエ変換した結果である.

メル尺度
	心理学者のStanley Smith Stevensらによって提案された, 人間が感じる音の高さに基づいた, 音高の知覚尺度
	人間が感じる高音の変化の尺度のこと
	→人間は高音になればなるほど音の高さの変化を感じにくい
	例えば人間が低音域の差は見抜けくことができるが, 高音域の違いを見抜くのが難しい. メル尺度も同様の考えであり, 周波数が高いほどメル尺度が増加しにくくなる.

フォルマント
	言葉を発してできる複数の周波数のピークを【フォルマント】といい,声道の共振周波数を【フォルマント周波数】と呼ぶ.
	なお，【フォルマント】は，周波数の低いものから順に第1【フォルマント】周波数，第2【フォルマント】周波数，…と呼ばれる．
	母音を区別するときに重要なのは，第1，第2，第3【フォルマント】周波数であるといわれている．
	周波数スペクトルに現れる周波数のピークのこと
	→その周波数をフォルマント周波数と呼ぶ
	→音韻（意味の区別を示す音）が同じならばフォルマント周波数は近くなる


音韻(おんいん)
	特定の言語の音の体系のことである（例：日本語の「あ」は「あ・い・う・え・お」という体系の一部）
	同じ音として識別するグループのこと
	→言語によって音韻は異なる
	→日本人は「lock」「rock」の違いが判らない
	同じ「ロック」として認識してしまう（音韻が異なるため）
	、日本語の「デンキ」「デンパ」の「ン」は音声としては異なるが、同じ音韻だと認められる。
	音韻論とは,音素について研究する言語学の一部門であり,音韻とはある特定の言語の音の体系のことを指す。

		・隠れマルコフモデル(HMM)
			音声認識でよく使われていたモデル
			→音素を利用して単語を捜索（音素と単語を対応）
			hidden Markov model,つまり隠れマルコフモデルのことであり,統計モデルの1種である.
音素
	意味の違いに関わる最小の音声的な単位を音素（phoneme）といい,音声認識では音声データから特徴量を抽出するために,音素の抽出やノイズ除去のようなデジタル信号の波形に変換を行う.
	個別言語の中で同じとみなさせる音の集まり
	さんま、テント
	→同じ「ん」だが実は発音は異なっているが日本語では同一の「ん」として扱う
	音韻よりも小さい概念とされている

WaveNet
	音声認識や音声合成に利用できるモデルで, 量子化された音声を1つずつ, 1秒間に16000個の音声データを学習データとして学習するモデル
	DeepMind社が開発した音声認識・音声合成を行う
	→ディープラーニングを活用したモデル
	→人間に近い自然な話し方をする
	スマホやスマートスピカ―で活用されている

音声認識エンジンの仕組みは,大まかに次の流れとなる.
	①元の音声データから特徴量を抽出するために,アナログ信号からデジタル信号の波形に変換し,【音素】の抽出やノイズ除去を行う.
	②抽出された特徴量が,どの【音素】に近いのかを見つける（この作業を「音響分析」という）
	③【音素】と発音辞書と連結して、単語単位に組み立てる.
	④あらかじめ蓄積したデータから使用する単語の出現率を算出し,確率が高い文章に整形する.
	ディープラーニングの技術を用いた音声認識エンジンは,一連のプロセスを1つのニューラルネットワークモデルで実装している.

Microsoft社はCNNとLSTMを組みあわせるモデルで音声認識の性能を向上させた.これは Windows 社の Cortana などに搭載されている.
CNNは画像処理に利用されるイメージが強いが,音声データの波形を画像と考えれば,CNNが音声認識に利用できることは理解できるだろう.
また,音声データは時系列データであることから,　◆RNN◆　の一種であるLSTMとの親和性は高い.
現在,音声認識には,音響モデルと言語モデルで異なるアルゴリズムを採用,併用するのが一般的だが,最近ではLSTMのみで音声認識を行う研究も進められている.
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
音声を入力とし,単語列を出力するモデルを
生成するプロセスは以下のように分けることができる.
■プロセス1：雑音・残響抑圧 (入力：音声　出力：音声）
  音声を認識したい対象以外の雑音を分離する.
■プロセス2：音素状態認識 (入力：音声　出力：音素）
  音声の周波数スペクトル,すなわち音響特徴量をインプットとして,音素状態のカテゴリに分類する.
■プロセス3：音素モデル (入力：音素　出力：文字列）
  音素がどの文字であるかを推定する.
■プロセス4：単語辞書 (入力：文字列　出力：単語）
  認識した文字列から単語を特定し認識する.
■プロセス5：言語モデル (入力：単語　出力：単語列）
	あらかじめ蓄積したデータから使用する単語の出現率を算出し、単語を文章化するモデルである.
  単語系列仮説の尤度を評価する.
  プロセス1～5を纏めて,すなわち,入力：音声　出力：単語列とする学習手法のことをEnd-to-End音声認識と呼ぶ.音響特徴量から音素,音素から文字列,文字列から単語列に直接変換して言語モデルを学習するアプローチ
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
ディープラーニングは音声認識の逆過程である音声合成においても利用されている.
従来,音声合成,あるいは音声認識の分野においては 【HMM】などの統計的手法により,大量のデータを集め,多数のコーパスを用意するものが一般的であった.
しかしながら,2016 年に DeepMind 社により発表されたニューラルネットワークのアルゴリズム 【WaveNet】 は従来に比べて圧倒的に高い質での音声合成に成功し,
AI　スピーカーが人間に近い自然な言語を話すことなどに大きく寄与している.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
コーパス
	単語検索対象となる文書全体のことである
正規表現
	文字列内で文字の組み合わせを照合するときの表現方法である.

2018 年 8 月現在において,音声認識技術は長 年の研究成果が花開き,成熟を見せている.
音声認識技術の一般的な評価尺度としては,【WER】 が用いられており,この値をいかに下げるかということを目標に研究開発競争が繰り広げられてきた.
この値はディープラーニング技術の向上により飛躍的な精度向上を果たしている.
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★37. テキストデータの扱い方
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
単語N-gram
	N-gramは任意の文字列やテキストをいくつかの単語や文字のペアでn個ずつ分割し, ペア内の文脈を考慮してテキストを解析する手法である.
	1つの漢字を読み取るときに単語の前後関係から読みを解析するときなどに利用できる.
	文字列をｎ個の単語で分割する方法
	１つの単語で分割する場合：ユニグラム(unigram) n=1
	２つの単語で分割する場合：バイグラム(bigram)  n=2
	３つの単語で分割する場合：トライグラム(trigram)n=3

			今日｜は｜良い｜天気

	・ユニグラム：今日｜は｜良い｜天気
	・バイグラム：今日は｜は良い｜良い天気
	・トライグラム：今日は良い｜は良い天気

BoW（Bag-of-Words）
	任意の文字列やテキストに対して, 記載内容の特徴を分析するため, 単語の出現回数を読み取る手法
	記述内容が肯定的なのか否定的なのかの判定などに利用できる.
	単語の順番は考えず、単語の出現回数をカウントする
	単語を数値に変換して処理を行う（ベクトル計算など）
	各単語にIDをつけて、ワンホットベクトルに変換する
	今日｜は｜晴れ＝＞今日は：１，は：２、晴れ：３
	・今日は＝＞［1，0，0］
	・は	＝＞[0，1，0]
	・晴れ＝＞［0，0，1］
	ベクトルの形で表すことでテキスト間の関係性や特徴などを計算して求めることができる（コサイン類似度）

ワンホットベクトル（1つの概念を1つの成分で表すので局所表現という）
	１つの値が１，他の値が０になっているベクトル

Bag-of-ｎ-grams
	BoWとn-gramを組み合わせたもの

TF-IDF
	文章内に出現する単語について, 単語の出現頻度と単語が使われている文の割合の逆数で単語の重要度を示す手法
	TF-IDFは, 単語の出現頻度(TF)と単語が使われている文の割合の逆数(IDF)で単語の重要度を示す手法である.
	単語の重要度を計算する方法
	TF（単語の出現頻度）
	IDF
	文章に含まれている単語がどれだけ珍しいか
	その単語が存在する文書の割合の逆数の対数

局所表現(1,0,0,0,1,0,･･･････,0)
	ある単語とベクトルの関係を1対1で表現する考え方である.他の概念とは完全に独立しているため, 単語間の関係性をとらえることができない問題がある.
	ワンホットベクトルのように１つの概念を１つの成分で表すような表現
	→似た概念を考慮することができない以上、次元が高くなる
	→問題を解決したのが分散表現（単語埋め込み）という方法
	自然言語処理の分野で, 単語とベクトルの関係を1対1で表現する考え方を【局所表現】と呼び, 単語をベクトル空間上で点として表現する考え方を【分散表現】と呼ぶ.

分散表現（単語埋め込み）(1,3,2,8)
	文字や単語をベクトル空間に埋め込み、その空間上で点として扱う考え方である.他の概念と関連する場合は紐づけながらベクトル空間上に表現していく.
	１つの単語を複数の成分で表現する方法
	→単語間の関係などを計算することができる上に次元数を低くすることができる
	→word3vecが有名な手法になる

・Word2vec　単語をベクトルにする。分散表現
	推論ベースの手法
	分散仮説（【単語の意味は周囲の単語によって決まる】）とニューラルネットワークを用いた手法
	→Queen（”王様” – “男” – “女” = “女王”）　というように表すことができる
　	ベクトルを使うことで様々な概念を説明できる

	１．CBOW（Continuous　Bag-of-Words）
		周辺の単語を与えてある単語を予測する
		周囲の単語からある単語を推測するモデル.単語周辺の文脈から中心の単語を推定
	２．スキップグラム（skip-gram）
		ある単語を与えて周辺の単語を予測する
	　　ある単語を与えて周囲の単語を推測するモデル。逆に中心の単語からその文脈を構成する単語を推定
		精度がいいが時間がかかる

fastText
	文章を扱うモデルとして, 個々の単語を高速でベクトルに変換しテキスト分類を行う
	個々の単語を高速でベクトルに変換しテキスト分類を行うモデルである.
	word2vecとの違いとして、単語の表現に文字の情報を含めて, 存在しない単語を表現しやすくすることができる.
	word2vecを開発したトマス・モコロフが開発したモデル
	学習内容に部分文字列（文字列の一部）も含めることで未知語にも強いモデルになる
ELMo
	文脈から意味を演算する
	ELMoは, 文脈から意味を演算するモデルである.双方向のLSTMで学習することにより, 同じ単語でも文脈によって異なる表現を獲得することができる手法である.
	双方向リカレントニューラルネットワークを活用する手法
	→複数の意味を持つ単語でも前後文から適切な意味を推測
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★38. RNN（LSTM・GRU）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
リカレントニューラルネットワーク（RNN）再帰ニューラルネットワーク
	自然言語処理は時間の概念が必要となり、入出力のデータサイズも可変となる
	音声処理、自然言語処理、時系列データなどの学習に用いられるニューラルネットワーク
	→過去の情報から未来の情報を予測する（過去の情報が大切）
	過去の入力になる隠れ層（中間層）を再帰的に接続し、現在の入力情報のために使う
	→機械翻訳では過去の情報が現在・未来に影響を与えている
	最近は RNN の学習における問題点であった勾配消失,勾配爆発を回避する方法が定着してきた.LSTMを用いることで勾配消失を回避することができる.
	また、勾配爆発を回避するために、勾配クリッピングという手法が用いられている.
	近年は自然言語処理だけでなく,音声処理にも利用されるようになってきている
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	2014 年には,CNN の最終畳み込み層の結果をRNN へ接続することで,画像キャプション生成なども可能になった
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
    RNN（Recurrent Neural Networks）は内部に（◆再帰構造◆）を持つニューラルネットワークの総称であり,
    この構造は（◆系列データ◆）を扱うために開発された.（◆系列データ◆）を扱えるようになったのは,（◆再帰構造◆）によって,（◆情報を一時的に記憶させること◆）ができるようになったためである.
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

	・言語モデル
		人間が用いる自然な言葉使いを確立などを用いてモデル化したもの

	パラメータの更新はニューラルネットワークと同じように誤差逆伝播法がつかわれている
	→RNNでは過去のパラメーターも更新していく必要があるため時間軸をもとに誤差を反映させていく
	→通時的誤差逆伝播 BPTT（BackPropagation Through―Time）と呼ばれる手法
	 過去の時系列を遡りながら誤差を計算する
	 時系列が古いほど勾配消失しやすい

Backpropagation Through Time(通時的誤差逆伝播 BPTT)
	Backpropagation Through Time（BPTT）は、elman networkの学習にも使用されている,ある種のRNNを学習するための勾配ベースの手法である.
	（ア）は目的変数の誤差が小さくなるように、各ニューロンの重みを最適化する手法であるが,
	誤差が時間をさかのぼり,勾配を計算するためにすべてのタイムスタンプを使用することをBackpropagation Through Timeという。
	なお,Backpropagation Through Timeは，進化的最適化アルゴリズム等の汎用的な最適化手法に比べて，RNNの学習が大幅に速くなる傾向があるが,一方で局所最適が苦手である.

単純RNNの長期依存性問題
	長期記憶を対応できない。短期記憶のみ対応
	単純RNNは前セルの出力（Recurrent）と入力が合わさって出力されている単純モデル

CTC（Connectionist Temporal Classification）
	音声認識において入力された音声の数と認識すべき音素数が不一致になることがあるが,空文字を追加することにより問題を解決する手法
	音声認識において,入力された音声を音素として出力したいが,その音素数が必ずしも入力された音声と一致するわけではなく矛盾が生じる場合がある.
	これを解決する手法がCTC（Connectionist Temporal Classification）である.
    文字認識や音声では一つの文字の横幅や、一つの音素の時間長さが可変です。
	そこで、▽▽▽ デコーダ側で同じ文字が連続した場合に消し込むことでこの可変性の問題を解決 △△△します。

	ニューラルネットワークでは、入力１に対して出力は１になる特徴がある
	→入力数と出力数（音素）が合わないことがある（音声認識）
	→以上の問題を解決する手法がCTCになる

	RNNは時間軸にそって重みを調整するため層が深くなりやすい特徴がある
	→勾配消失問題が発生しやすい（学習が進まない）

	一般的に関係のある入力は重みが大きく、関係のない入力は重みを小さくすることが原則である
	入力の場合重みを大きくする（入力重み衝突）
	→出力でも同じような現象が起きてしまう（出力重み衝突）

	▽▽▽ 勾配消失問題、入力重み衝突や出力重み衝突のような問題を回避するために考えられた △△△のが、LSTM（Long Short-Term Memory）と呼ばれる手法になる
	→隠れ層に工夫を加えることで問題を解決しようとする手法
LSTM（Long short−term memory 長短期記憶）
	▽▽▽ 短期記憶と長期記憶を別々のラインで保持 △△△
	LSTMブロックと呼ばれる機構を導入することで時系列情報を保持することができる
	CECという情報を記憶する構造とデータの伝搬量を調整する3つのゲートを持つ構造
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	LSTM（Long Short-Term Memory）は◆RNN◆の一種である.内部にゲート構造を設けることにより,
	プレーンな◆RNN◆の抱える　◆長い系列を遡るにつれて学習が困難になる◆　という問題を解決する方法として提案されている.
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

	→LSTMブロックはニューラルネットワークのユニットに対応
	・CEC
	  情報を記憶するセル
	  誤差を内部にとどめ勾配消失を防ぐ、セルとも呼ぶ
	・入力ゲート、出力ゲート：重み衝突に対応する
	・忘却ゲート：過剰な誤差をリセットする。CECから削除する情報を決めたゲート

打ち切り型通事的逆転搬
	RNNは記憶する範囲をちょっと前までに限定していて、それ以前のものは切り捨てている
	現実社会ではもう少し前までの情報が必要で、改良したものがLSTM

GRU LSTMは複雑なため計算量が多くなってしまう問題に対応化した軽量モデル
	リセットゲートと更新ゲートという2つのゲートを用いた構造
	→ゲートの数を減らし計算量を削減したモデル
	→リセットゲート、更新ゲートが入力ゲート、出力ゲート、忘却ゲートの役割を果たす
入力重み衝突
	現在の入力に対し過去のじょうほうの重みは小さくなくてはならないが将来のために大きな重みでなければならないという矛盾のこと
出力重み衝突
	現在の状態を次時刻の隠れ層へ出力するときに発生する
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★★39. RNN（双方向RNN・Attenntion・エンコーダ-デコーダ）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
双方向RNN（Bidirectional　RNN、BiRNN）
	過去のデータだけでなく、未来のデータからも学習できるようにしたモデルのこと
	データを時系列通りに学習し、もう一方は時系列を逆順に並び替えて学習するモデル
	過去と未来の両方の情報を踏まえた出力ができる
	→事前言語処理の場合なら、過去の単語だけではなく、未来の単語からも意味を推測した方が精度が高くなる

（ア）はある状態の中間層の出力値を次の状態に順伝播するネットワークであるが,
【Bidirectional RNN（双方向RNN,BRNN)】は,中間層の出力を、未来への順伝播と過去への逆伝播の両方向に伝播するネットワークである.
Seq2Seq等の機械翻訳のモデルでは（ア）の仕組みを使うが,順伝播するネットワークであるため,翻訳の計算時間が長くなる性質がある.
そのため,同じく機械翻訳のために提案された【Transformer】は計算速度向上のため,（ア）を使わずに,【Attention】メカニズムを使って並列計算を可能とするモデルである.
【Transformer】は（ア）等に比べて非常に自由度の高いモデルであり、大量のデータを学習することにより、さらに性能が向上している.

Sequence To Sequence( Seq2Seq )エンコーダーデコーダ
	機械翻訳などの用途で,入力だけでなく出力も時系列にそった出力を行いたい場合,エンコーダとデコーダを用いることによりこの問題を解決したモデル
	機械翻訳のように,入力位置と出力位置が1対1に対応していない場合,出力も別のRNNを用いることによりそれぞれ別の時系列として扱うことができる.
	このモデルをSeq2Seq（sequence to sequence）と呼ぶ.
	今までのRNNは時系列データを入力すると出力が１つだった
	→出力も時系列データで予測したい場合もある（Seq２Seq）
	この問題を解決するモデルがRNNエンコーダーデコーダ
	→機械翻訳などでよく使われている手法になってくる
	「私はペンを持っている」という文章を、「I have a pen」という英文に翻訳
	この際,特に入力と出力（検索）が異なる場合に入力と出力の関連を求める仕組みを【Source-Target Attention】と呼ぶ.
	また【Source-Target Attention】は,構造に着目して【Encoder-Decoder Attention】と呼ばれることもある.
	Attentionは単語間（入力,出力間）の1対1の対比ではなく,入力されたベクトルの内積で関連度を求めている.
	Attentionの中で,過去の内容からsource（入力）とtarget（出力）の内容が違うものの関連を求めるものをSource-Target Attentionという.

RNNエンコーダーデコーダ
	エンコーダでは入力された時系列データから【固定長のベクトル】に変換する
	seq2seqの問題に対応したモデル
	デコーダでは固定長のベクトルから時系列データを出力
	ある時刻の状態が、どれだけ他の時刻の状態に影響を与えるかわからない
	→長い時系列データになると固定長のベクトルだと情報が入りきらないという問題が起きてしまう
	Encoderが入力データをDecoderが出力データをそれぞれ処理する。
    出力が時系列であり、全ての出力を一気に行うことができないため、Decoderの出力を、次のステップの入力として受け取って処理することになる。
	Source-Target AttentionはEncoderとDecoderを橋渡ししており,Encoder-Decoder Attentionとも呼ばれている.

Attention
	Attentionメカニズムとは、RNNが覚えきれない過去の情報を記憶にキャッシュするモデルである
	RNNエンコーダーデコーダの固定長で入りきらないという問題に対応
	入力データと出力データにおける重要度のようなもの（アライメント）を計算する手法
	時系列データにおいては「過去の入力のどの時点がどれくらいの影響を持っているか」を直接的に求めることでデータの対応関係を求めている

Imaging　Captioning
	入力した画像の説明文を生成するタスクのこと
	→人が座っている画像ならば、
　	人が座っていますというように説明文が生成される

	RNNによって時系列データを高い精度で扱えるようになった
	→並列的に計算することができないため処理速度が遅い
	→入力データが長くなると単語間の関係性を正しく反映させることが難しかった
	→以上の問題を解決する方法としてトランスフォーマーが提案

トランスフォーマー登場する前はエンコーダとデコーダを使用
	Transformerとは,機械翻訳のために提案され,計算速度向上のため,RNNを使わずに,Attentionメカニズムを使って並列計算を可能とするモデルである.
	→入力文と出力文の単語間の関連度を計算するAttention構造を
　	Source-Target Attention , Encoder-Decoder Attentionという
	→トランスフォーマーはRNNからエンコーダ・デコーダをなくし
　	Self- Attentionを導入、Source-Target Attentionを使用

Self- Attention（自己注意機構）
	ニューラル機械翻訳の新たなモデルとして提案されたトランスフォーマーは,従来式のRNNの代わりに【Self- Attention】と呼ばれるネットワーク機構を採用することにより並列計算処理がしやすくなり,
	RNNと比べて正確かつ高速にモデルの学習が行えるようになった.
	背反として語順情報を考慮することができないため,【位置エンコーディング】を付加することにより間接的に単語の位置情報や位置関係を考慮することができるようにする.
	RNNを利用したニューラル機械翻訳では,時間軸にそって順にデータを読み込む必要があったため並列処理ができなかった.
	これを解決するためにSelf-Attentionと呼ばれるネットワーク構造を利用することにより並列処理を可能にした.
	Self-Attentionを導入することにより並列処理は可能になるが,単語の位置関係が把握できなくなる.これを解決するため位置エンコーディングと呼ばれる単語の出現位置情報を付加することにより,
	間接的に単語の位置情報や位置関係を考慮することができるようになった.
	入力文内、出力文内の単語間の関連度を計算するAttention構造
	→並列に計算ができるようになり処理速度が速くなった
	→単語の位置を考慮する位置エンコーディングにより位置情報を保有することができる

SOTA（State-of-the-Art）
	製品や科学などの、ある特定の専門技術領域において現時点での最先端レベル（＝最高／最良）の性能（＝機械学習では正解率などのスコア／精度）を達成していることを表す、
	一般的な用語である。
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
40. 自然言語処理（NLP）の概要
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
自然言語処理の研究においても,ディープラーニングを利用した研究が目立ちつつあるが,その成果は分野においてまちまちである.
例えば,【機械翻訳】 や画像説明文生成においては大幅な性能向上が見られる一方で,【構文解析】 や意味解析のように連続的な精度向上は見られるものの基本的な手法は大きく変わらないもの,
そして 【文脈解析】 や常識推論など現在のアプローチでは実用的な精度は見込めないものなどがある.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
自然言語処理
	人間が使っている言語をコンピュータに処理させること
	１．形態素解析
	２．構文解析
	３．意味解析
	４．文脈解析

１．形態素解析
	文章を意味をもつ最小の単位に切り分けて解析を行う
	→今日は良い天気です
　	今日｜は｜良い｜です
	→日本語は英語と異なり切り分ける処理が難しい
２．構文解析
	自然言語を単語単位に分割し,その関係を何かしらのデータ構造として表現すること
	これは英語のように単語ごとに空白で区切られている言語では簡単であるが,日本語のように単語ごとに区切られていない言語であっても意味をなす最小単位の要素に分解することが必要である.
	このように最小単位の要素に分割し,それぞれの属性を明らかにしていく処理を【形態要素解析】という.
	自然言語処理をするにあたり,まず自然言語を単語単位に分割する構文解析を実施する.
	自然言語を単語単位に分割することは,単語がスペースで区切られている英語などでは容易であるが,日本語のように単語の区切りがない言語では容易ではない.
	そのため,意味をなす最小単位（形態素）に分割し,それぞれの品詞など属性を明らかにする処理を形態要素解析（形態素解析）という.
　	形態素解析で分解した単語の関係性を解析すること
	→構文木などを使って分析結果が表示される
３．意味解析
	構文解析行った文の意味を正しく解釈するめに行う解析
４．文脈解析
　	文章全体の意味を解析すること
	代名詞（それ・あれなど）を推定する照応解析
	関連した一連の文の意味的な関係性を推定する談話解析
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
一文の意味解釈を超え,文章全体の意味を解釈しようという研究も盛んに行われて来た.その例として代表的なものは 【照応解析】 である.
これは,文章内に存在する代名詞などが何を指し示しているのかを突き止めさせる解析である.
こうした意味解釈や文脈解析の精度を上げていこうと研究が進められていた自然言語処理の分野にもディープラーニングは影響を与えている.
最も注目すべきは,言語の 【分散表現学習】 の成功である.代表的なものとして,google 社のミコロフは,【Word2Vec】 という枠組みを提案し,これによってこれはある種の意味的な演算が行えることを示した.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
自然言語処理においては,学際的な研究が進められている.その一つとして,機械が知能を持っているか否かを判断することを目的とした知能テストが多数考案されており,【常識推論】 タスクとして注目を浴びている.
【常識推論】 タスクの著名なものの一つとして,南カリフォルニア大学の Andrew Gordon の研究グループが提案した COPA （Choice of Plausible Alternatives） というものがある.
これは「知能を持つ」ことが事象間の因果関係を理解することだと捉え,これを計測しようとしたものである.
他の 【常識推論】 のタスクとしては,Ernest Davis と Hector Levesque の研究グループが定式化した 【WSC】 があり,
「統語的手がかりだけでは解けないような照応解析の問題が解けること」が知能を持つこととしてこれをテストの形にした.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

現在このテストは AI のトップ会議でコンペティションが開かれるなどしており,まだまだ精度は低いものの注目を浴びつつある.
この 【WSC】 の演算の事例として最も適切なものを選択肢から選べ.ただし,選択肢における [V] はベクトルを指し示す.
V（Prince） - V（Male） + V（Female） ≒ V（Princess）

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
COPA の知能テスト
	前提となる文を与えられた際に,その文章から必然的に考えられる結果として適切な文章を選ばせる
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
自然言語分野の研究において,近年実社会でもその活用が著しい分野の一つとして,自然言語の生成技術が挙げられる.例えば,記事の要約を行うサービスや,機械による自動翻訳などはその顕著な事例である.
この躍進を支える仕組みが,「言語モデル」の研究であり,人間が話そうとする内容に基づいて単語同士が綺麗に接続されるように単語を順々に選択しているであろう様を模倣したものである.
大きく n-gram 言語モデル　と　リカレントニューラルネットワーク言語モデル（以下,RNNLM）　の二つに分かれており,近年では後者の系譜が採用されることが多い.
現在の主流の機械翻訳システムは,この RNNLM 型デコーダを活用し,中間データを介してエンコーダとデコーダが相対する形で構成された 【エンコーダ・デコーダモデル】 と呼ばれるモデルが改良されているものである.
現在機械翻訳の精度が高いとして評価が高いのが,google 社が2016 年に発表した 【GNMT】 であり,従来のモデルに比べて言葉の流麗さが増したと言われている.
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
文中の「言語モデル」の本質を説明
	過去の入力系列を利用して次回の出力単語の分布を求める確率モデル
▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

様々なインプットを受け入れることが可能になり,機械翻訳などの分野で大幅な精度向上をもたらした RNNLM 型デコーダであったが,従来型の言語生成システムでは見られなかった新たな問題を生じさせることがわかっている.
特に主要な問題として二つ,過生成と生成不足が指摘されている.
	過生成の事例
		入力：I got a book and a pen
		出力：私は本と本と本とペンを手に入れた
	生成不足の事例
		入力：I got a book and a pen
		出力：私は本を手に入れた

単語埋め込みモデル
	自然言語処理の分野で単語の意味をベクトルで表現するモデル
	単語埋め込みモデルは, 単語の意味をベクトルで表現するモデルである。複数の単語の関係性をベクトルの演算によって導き出すことに使われている(例. 「王様」-「男性」+「女性」=「女王」).
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
41. 自然言語処理と画像処理
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
自然言語処理
	・事前学習と移転学習
		自然言語処理においてGPT、BERTが代表的なモデル

	→大量のデータで学習したモデル（事前学習）を使い、移転学習を行うことで高精度のモデルを開発できる

GPT
	大規模なコーパスを用いて事前学習を行うモデルの一つにOpenAIが開発した【GPT】がある.【GPT】はトランスフォーマーに似た構造のネットワークを用いることにより事前学習を行う.
	【GPT】は1億前後のパラメーターを持つが,2019年には15億のパラメーターを持つ【GPT-2】,2020年には1,750億ものパラメーターをもつ【GPT-3】と後継モデルが発表されており,
	従来に比べ事前学習のみでさまざまなタスクに高精度に対応できるようになった.
	自然言語処理において,トランスフォーマーににた構造のネットワークを利用して事前学習を行うものにGPT（Generative Pre-Training）がある.
	GPTの後継としてGPT-2が発表されたが,発表当初はあまりにも自然な文書作成ができることが危険だとして小規模モデルしか公開されなかった.（現在はすべて公開済みである）
	GPT-2の後継としてGPT-3が発表された.しかし,パラメーター数が多く安価なGPUでは学習に300年以上もかかるといわれていたり,
	GPT-3自体は善悪の判断ができないため不適切な発言をする可能性があるなど,新たな問題も指摘されている.

	OpenAIが開発した自然言語処理モデル
	→大規模なコーパスを使用して事前学習を行い、ファインチューニングすることで様々な場面で活用
	→パラメータ数を増やしたGPT-2、GPT-3などがある

BERT
	自然言語処理において,事前学習したモデルを基に転移学習することにより従来より高精度にタスクを解くことができる.
	その中でもMLM（Masked Language Model）とNSP（Next Sentence Prediction）という2つのタスクにより事前学習を行う学習モデルを【BERT】という.
	またこのようなモデルの,言語理解タスクをベンチマークするための【GLUE】と呼ばれるデータセットが公開されている.
	BERT（Bidirectional Encoder Representation from Transforms）はトランスフォーマーのエンコーダーを利用するため通常の言語モデルによる学習は行えず,MLMとNSPというタスクにより事前学習をおこなう.
	BERTなどによる言語理解タスクのベンチマーク用データセットとしてGLUE（General Language Understanding Evaluation）などのデータセットが公開されている.
	Googles社が開発した自然言語処理モデル
	→MLM（Masked Language Model）、NSP（Next Sentence Prediction）という事前学習を行っている
	→RNNの代わりに、双方向トランスフォーマーを採用している

MLM
	文章の一部を隠して見えない状態で入力し、前後文から隠した文章を推測する
	複数箇所が穴になっている文書の単語予測
NSP
	２つの文章を入力し、連続した文章を推測する
	・事前学習モデル
		パラメータ数を減らしたALBERT、DistilBERTなどが提案

パラメータ数を増やすモデルも提案
	・Megatron―LM
	・Turing-NLG（Microsoft）
	・GPT-2、GPT-3 など

	・画像処理分野
		トランスフォーマーが一部の画像処理にも使われる
		→CNNを使わない方法
		→Vit（Vision Transformer）と呼ばれる事前学習モデルが提案

Vision Transformer
	自然言語処理の分野で発展してきたTransformerの技術を,画像処理に応用したもの
	Vision Transformerは自然言語処理の分野で発展したTransformerを画像処理に流用したものである.
	Vision Transformerは画像を単語のように分割することによりCNNを使用せず,Transformerに近いモデルを使用している.

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
42. 自然言語処理と画像処理
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
深層強化学習の基本的な手法と発展
	・深層強化学習
		状態や行動の組み合わせが多い場合の学習を可能にしました
		【強化学習とディープラーニング】の手法を組み合わせたもの
		状態と行動の組み合せが多いとき、ディープラーニングで集約する
		→DeepMind社が開発したDQN (Deep Q-Network) が
		  基本的な手法になっている
		→Q学習に畳み込みニューラルネットワークを組み込んだもの
		  Q学習の場合、画像の少しのズレでも違う状況だと
          判断してしまうため考えられる状況が多いと言う問題があった

		→ニューラルネットワークを活用したことで
		  状況の重要な情報のみを処理できるようになった

		経験再生、ターゲットネットワークと言われる
		学習手法が導入されている

	・経験再生(Experience replay)
		経験データを蓄積しておき、学習するときにランダムにデータを抜き出し、学習に使う手法のこと
		DQNを用いる際、行動系列から得られた報酬をもとにネットワークの重みを更新する
		重みの更新は状態遷移に強い相関を受けるため学習が安定しない
		この問題に対して状態、行動と報酬の組を記録しておいたものからランダムサンプリングして重みを更新する
		→訓練データ間の時間的な偏りを少なくし、学習の安定化を図る

	・ターゲットネットワーク
		学習を安定させるためにターゲットになる固定した
		過去のネットワークを教師のように見立てて学習

		→固定していないと学習が収束しにくくなる

	・DQNを発展させた方法
		・ダブルDQN
		・優先度付き経験再生
		・デュエリングネットワーク
		・カテゴリカルDQN
		・Rainbow

	・ダブルDQN(Double DQN)
		行動価値に対する学習とディープラーニングを組み合わせた深層強化学習は,様々な拡張手法が発表されている.
		例えば深層強化学習を二重化することによって偶然行動価値が大きくなり,誤った学習をしてしまう現象を防ぐ【ダブルDQN】や,
		行動によらず学習ができるように行動価値関数を状態と行動に分割する【デュエリングネットワーク】などの手法がある.
		行動価値（Q値）に対する学習と,ディープラーニングの組み合わせはDQN（Deep Q-Network）と呼ばれる.
		DQNはたまたまQ値が高いところを学習してしまう場合があり,それを防ぐ手段にDQNを二重化したダブルDQNがある.
		ターゲットネットワークを活用して学習を行う
		Q関数の更新には、更新を行うQ関数の状態の次の状態のQ関数のうち、最大の値をもつものを目標値とする
		そのため、誤差の上方向に影響を受ける傾向がある。これを防ぐためにQ関数の更新を行うネットワークと次の状態のQ関数のうち更新に用いるQ関数を選ぶネットワークを使う

	・優先度付き経験再生
		行動と報酬を記録したものから頻度の少ない行動などを優先的にサンプリングを行い重み

	・デュエリングネットワーク(Dueling Network)
		デュエリングネットワークは行動価値を状態価値関数（状態）とAdvantage関数（行動）に分割することにより,行動にかかわらず状態を学習することができるようにしたものである.
		状態から計算できる部分と行動から計算できる部分を分けて計算することで、効率的に学習を行うことができる
		Q関数をニューラルネットワークで近似する際に、状態のみからで計算できる部分と行動のみから計算できる部分を別々に計算することで、効率よくQ関数を学習できる

	・カテゴリカルDQN
		期待収益や離散確率分布を活用することで効果的に学習を行う
		Q関数は、ある状態とある行動により今後得られる期待収益値であるが、期待収益を離散確率分布とすることで学習を安定させたり、期待収益の分散や多峰性分布が表現できる

	・Rainbow
		ダブルDQNや優先度付き経験再生など様々な手法を組み合わせた手法
		2017年に,過去にDQNに関して提案されてきた7種類のアルゴリズムを統合したアルゴリズムである【Rainbow】が発表された.
		RainbowはDQN,Categorical DQN,Multi-Step RL,Double DQN,Prioritized Experience Reply,Dueling Net,Noisy Netという7種類のアルゴリズムを統合したアルゴリズムである.

	Reword Clipping
		ニューラルネットワークのパラメータは報酬の値に大きく影響を受ける
		報酬の値を１、ー１，０のみに限定したり−１から１の範囲に限定したりすることでニューラルネットワークの学習を安定化させる

	noisy network(ノイジーネットワーク)
		noisy networkはネットワークそのものに学習可能なパラメータと共に外乱を与え,それも含めて学習させていく手法である.
		DQNでは常にその時点で価値の高い行動をとり続けた場合,別の行動をとる可能性がなくなってしまう.
		この問題点をネットワークそのものに学習可能なパラメータと共に外乱を与え,
		それも含めて学習させていくことでより長期的で広範囲に探索をすすめることで改善するという方法を【noisy network(ノイジーネットワーク)】という.

深層強化学習とゲームAl
	・AlphaGo
		DeepMind社が開発した囲碁のプログラム
		→人間のプロ棋士に勝利した
		モンテカルロ木探索と深層強化学習を組み合わせたもの
		モンテカルロ木探索　価値の高い状態を起点としたシミュレーションによって効率的に手の探索を行うアルゴリズム
		AlphaGoではCNNを用いて４つのニューラルネットワークを学習させる。全ウチの手からランダムに選出
		SL Policy  		人間の棋譜の繊維関係を学習させることで１つ次の手を予測し勝ちに繋がりやすい状態を列挙する
		Rollout Policy 	SL Policyの予測性能を落とす代わりに計算速度をあける
		RL Policy  		SL Policyのネットワーク同士の対戦による学習で予測性能を向上させた
		Value Network  またある基盤からRollout PolicyとRL Policyのネットワークを用いて勝敗がつくまでゲームを進めこの勝敗をもとに盤面の勝利確率を学習させ、予測する
	・AlphaGo Zero
		AlphaGoを発展させた囲碁プログラム
		→人間の棋譜データを使わずに深層強化学習を行った
		人間の知識を使わずに0から学習、AlphaGoよりも強くなった

	・Alpha Zero
		将棋やチェスなどの世界最高峰であったAlに勝ち
		AlphaGo Zeroよりも強くなった
		→人間の棋譜データを使わずに自己対戦で学習を行った


	・マルチエージェント強化学習
		複数エージェントによる強化学習のこと
		あるエージェントが報酬を得た時に、他のエージェントにも間接報酬を与える手法
		マルチエージェント機械学習は複数の強化学習エージェントが同時に学習をして行動し、相互に影響を与える.
		→100人が1になるまで生き残り続けるゲームの場合
		  99人の動きなどを考慮した上で行動する必要がある

		・OpenAl Five
		・AlphaStar

		マルチエージェント強化学習を活用したゲームAIlとして
		以上2つが代表的なものになる

	マルチエージェントシミュレーション
		人工知能研究の応用分野の一つとして,「マルチエージェントシミュレーション」は,社会現象を研究する上で有用であるとして経済学の領域などで利用されてきた.
		▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
		エージェントと呼ぶ小さな AI プログラムを人間とみなして,エージェントが多数存在して相互作用することによって生じる現象を模倣し分析しようとするもの
		▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
		社会応用例
			〇街中や施設内における人々の動きのシミュレーション
			〇交通渋滞のシミュレーション
			〇電力市場の送配電システムシミュレーション
			x津波による浸水地域シミュレーション

	・OpenAl Five
		2019年に,「Dota2」という対戦型リアルタイムストラテジーゲームにおいて、【OpenAl Five】というAIソフトが2018年度の世界王者を破って勝利した.
		非営利団体Open AIが作ったAIソフトであるOpenAI Fiveが,2019年にDota2というコンピュータゲームで2018年度の世界王者を破って勝利した
		対戦型リアルタイムストラテジーゲーム「Dota 2」の
		世界大会覇者OBに勝利した

	・AlphaStar
		2019年に,リアルタイムストラテジーゲーム「スタークラフト2」でDeepMind社により開発されたAIである【AlphaStar】がプロのゲームプレイヤー相手に大勝利した.
		DeepMind社の作った人工知能ソフトであるAlphaStarが,2019年にスタークラフト2というコンピュータゲームで人間のプロ2人を相手にして,10連勝を果たした.
		DeepMind社が開発したAlで、リアルタイムストラテジ
		「スタークラフト2」でトップレイヤーに勝利した

深層強化学習の応用先
	〇将棋のプレイヤープログラム作成
	x特徴に基づく植物の分類プログラム
	〇ファイナンスにおける消費者の信用予測
	〇広告配信の最適化
人工知能研究を行う英国のベンチャー企業DeepMind社を買収したGoogle社
中国のインターネット検索最大手であるバイドゥ社 は,スタンフォード大学でGPU （Graphic Processing Unit） を利用しディープラーニングの研究を進めていた
Andrew Ng を招き 2013 年にディープラーニング研究所を設立するなど,精力的な動きを見せている.
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
43. 深層強化学習と実システム制御への応用
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
・実システム制御への応用

	深層強化学習を実世界で応用するときの課題が存在する
	→強化学習では、離散値の状態や行動が扱われきた
　	実世界では連続値を扱うため離散化していくことが大切

	・連続値 : 切れ目なく連続している値のこと (体重、速度など)
	・離散値 : 連続していない値のこと (整数、人数など)

	・次元の呪い
		意味もなく適当に離散化してしまうと
		指数関数的に扱うべき状態・行動が増えてしまう
		→扱うべきことが増えるため学習が困難になる

	・状態表現学習
		環境の状態をあらかじめ学習しておくことで深層強化学習の学習効率を高める手法
		状態表現学習は,深層強化学習において環境の状態をあらかじめ学習しておく手法である.
		状態に対する特徴表現学習のこと
		→低次元で状態をどのように表現するのかが大切
		→ディープラーニングの特徴表現学習を活用していく

	・連続値制御
		出力される行動は離散値であるが、
		ロボットを扱う場合は連続値 (速度など) である
		→どのように連続値を扱っていけばいいのか など考える必要

	・報酬成形
		目標を達成するために学習するときの報酬関数を
		作っていくことが大切になる
		→報酬の与え方によって、方策は大きく変わってしまうため
		  適切に報酬を与えることが大切になる

	・課題に対する解決策
		・オフライン強化学習
		・模倣学習
		・シミュレータの利用
		・残差強化学習
		・環境モデルの学習

	・オフライン強化学習
		医療・ロボティクスなどの分野での活躍を期待されている.
		オフライン強化学習は,強化学習をオフラインで過去に蓄積されたデータのみで学習を行う手法である.医療・ロボティクスなどの実環境との相互作用へのリスクの大きい分野で期待されている.
		事前に収集されたデータ (オフラインデータ) で学習する
		→強化学習ではデータを収集しながら方策を学習するが
		  実世界の場合データを収集しながら方策を学習するのは困難
		→時間的な問題で十分なデータを収集できない、
		  危険な操作により損害を与えてしまう など

	・模倣学習
		人間がロボットをコントロールして
		期待する行動を学習させる方法

	・シミュレータの利用
		パソコン上でシミュレーションを行い方策を学習させる

	・sim2real (simulated-to-real)
		シミュレーションを用いて方策を学習し,その学習した方策を現実に転移させる手法である.
		Sim2Real(Simulation-To-Real)とは,シミュレーションを用いて方策を学習し,その学習した方策を現実に転移させる手法である.
		シミュレータで学習したモデルを実世界に移転させること
		→シミュレータと現実世界にはギャップが存在し、
		モデルの性能が低くなってしまう
		→対策としてドメインランダマイゼーションが活用

	・ドメインランダマイゼーション(Domain Randomization)
		ランダム化されたプロパティを使用して様々な学習用のシミュレーション環境を作成することが出来る.
		ドメインランダマイゼーション(Domain Randomization)とは,ランダム化されたプロパティを使用して様々な学習用のシミュレーション環境を作成する手法である.
		これらすべての環境で機能するようにモデルを学習していく.
		学習するときにランダムに異なる条件を設定する
		→様々な条件に対応することで実世界でも
		同じような性能を保つことが期待されている

	・残差強化学習
		従来型の制御システムの出力と
		最適な方策との差分を強化学習で学習する手法

	・環境モデル
		状態がどのように遷移するのかをモデル化したもの
		→ある行動を選択後、別の状態に遷移する確率・報酬がわかる

	・世界モデル
		エージェントが環境から得た情報を使って予測するモデル

	・モデルフリー (強化学習)
		環境モデルを使用しないアルゴリズム
		方策ベースの手法  ある状態に対してある行動を起こす確率をパラメトリックな関数fで表現する
		                この関数を用いて一連の行動による報酬の期待値を求め、期待値が大きくなるように関数fのパラメータの変化させることを繰り返して、報酬和を最大化する
		価値関数ベースの手法
		                ある状態である行動をとった後得られる報酬の期待値を算出し、期待値の大きい状態に到達するように、個体値の大きい行動をとるようにすることで得られる報酬和を最大化する
	・モデルベース (強化学習)
		環境モデルを使用するアルゴリズム
		状態遷移確率と報酬関数が既知の場合に最適ベルマン方程式を用いて最適価値関数を求め報酬和を最大化する
		→モデルフリーの手法よりもサンプル効率が向上するとされる

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
44. モデルの解釈性とその対応
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ディープラーニングのモデルの解釈性問題
	モデルが何か予測したとき、どのような根拠を持って
	予測したのか知ることも大切になる
	→モデルが複雑になればなるほど根拠は分かりにくくなる

	→モデルの局所的な解釈を可能にするアプローチがある
 	LIME、SHAP などの手法が考えられている

	ディープラーニングのモデルは複雑なため
	ブラックボックス化になりやすく、根拠が分かりにくい

	→ディープラーニングのモデル自体に根拠を持たせる
	アプローチが考えられるようになってきた

Grad-CAM
	CNNの判断するときの根拠を可視化するもの
	→画像のどの部分を見て判断したか分かる (勾配を利用)

Guided	Grad-CAM
	より詳細にどういった特徴を抽出しているのか可視化

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
45. モデルの軽量化
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
エッジAI
	エッジデバイス (インターネットに接続された端末) に学習したAlを搭載しんたもの
 	AIが搭載されたクラウドにデータを送信するクラウドAIに対して、エッジAIとはIoT機器やセンサーなどの端末にAIを搭載し、
	端末が学習・推論を行う技術のことです。 端末で収集したデータを基に端末内で推論を処理し、瞬時に判断を出します
	→車 (自動運転) 、製造ロボット などで使われることが期待
	→クラウドAlと異なる

モデル圧縮の手法
	学習済みモデルを端末など機器に搭載する場合
	大規模なモデルだと計算量が多くなってしまうため
	モデルの軽量化が必要になってくる (処理能力に限界がある)
	→モデルを軽量化 (モデル圧縮) する代表的な手法は
	燕留、量子化、プルーニング がある

蒸留
	学習済みモデルに与えた入力とそれに対する出力を学習データとして,新たなモデルを訓練すること.
	蒸留とは,学習済みモデルに与えた入力とそれに対する出力を学習データとして,新たなモデルを訓練すること.必要な計算リソースを削減することができる.
	学習済みモデルの入力と出力を学習させ
	小さなモデルを作成する方法
	→学習済みモデルを教師モデル、新しいモデルを生徒モデル
	蒸留はモデル生成手法の１種であり,複雑な学習済みモデルで予測した結果をシンプルなモデルで学習しなおす手法を指す.
量子化
	機械学習において演算に用いる数値の表現を浮動小数点数から整数に変換すること
	量子化を行うことでモデルサイズの縮小と推論時間の短縮を行うことができる.
	量子化とは,機械学習において演算に用いる数値の表現を浮動小数点数から整数に変換することである.
	重みなどのパラメータを小さいビットで表現し
	モデルを軽量化させる方法
	→全体のメモリ使用量や計算量を削減することができる
	→32ビット長の変数を8ビット長に置き換えることで4分の1へ
	計算の精度が低くなってしまうデメリットが存在する

プルーニング
	ニューラルネットワークにおいて機械学習アルゴリズムを最適化する方法の一つにプルーニングがある.
	プルーニングは,ニューラルネットワークのレイヤー間のつながり(パラメータ)を削除し,パラメータの数を減らして計算を高速化することができる.
	プルーニングとは,ニューラルネットワークの重み(パラメータ)の一部を取り除く手法である.
	ニューラルネットワークのユニットの中で
	重みが小さい接続を削除して、パラメータ数を削除すること
	→パラメータが少なくなることで計算量は減るが
	精度が落ちてしまうデメリットが存在する
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
x46. AIと社会
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
AIのビジネス活用と法・倫理
	AIによる経営課題の解決と利益の創出は重要なポイントになる
	→AI技術を普及させるためには
	  ビジネスで利益を生み出せるかが大切になるため

	→利益が上がらない場合はAlの活用に対して消極的になる
	  上手いこと活用すればイノベーションが生まれる

	Alを活用していく上で法の遵守・ 倫理の遵守も大切になる
	→AIの責任について法律で定めること
	→Alが入った機械が人を傷付けたとき誰が責任を取るのかなど
	  法律で定めておかないと被害者が泣き寝入りしてしまうことに

	インターネット普及により大量のデータ (ビックデータ) を
	獲得することができるようになったことでAIも進歩した
	→loT・RPAの普及によりさらに多くのデータが集まると予想

loT : モノがインターネットに接続されること (家電など)
RPA : 人間の代わりにコンピュータが定型作業を自動化

デジタルではデータの書き換えが容易であるため
ブロックチェーンを活用して改ざんを防ぐことも必要である

ブロックチェーン (分散型台帳)
	過去に行った全ての取引データを保有した台帳のこと
	→台帳は多くの人が持っているため誰かが改ざんすると分かる

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
47. AIプロジェクトの進め方
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
Alプロジェクト進行の全体像
	・CRISP-DM
		データマイニングのための指針となる手法のこと
		→6つのステップ (ビジネスの理解、データの理解、データの準備、モデリング、 評価、展開) から構成されている
		→それぞれのステップを行き来して作業を行う
	・MLOPs (Machine LearningとOperationsの造語)
		Alを本番環境で開発しながら運用していく考え方のこと
		→開発して終わりではない (継続的に開発し続ける)
		→DevOps (DevelopmentとOperationsの造語) から派生した言葉 (開発しながら運用することを意味する)
Alプロジェクトの進め方
	・プロジェクト内でAlを使うべきなのか検討を行う
		プロジェクトによってはコスト・利益の面からAlを使わない方が良い場合もある
	・データを蓄積して改善できるのか
		最初から精度の高いAlを開発するのは困難である
	・BPR (Business Process Re-engineering)
		ビジネスプロセスを再設計すること
		→Alを活用するためにビジネスプロセスを再設計する

	Web APIを提供する方法

	エッジデバイス (インターネットに接続された装置) に
	モデルをダウンロードし、更新し続ける方法

開発計画を策定する

	Alを学習させる上でどのようなデータを使って学習させていくかが大切になる

	→コスト面を考えながら、精度を高めるためには技術者の経験、知識などが必要になってしまう

	→Alの精度に合わせて運用方法を変えていくことも視野に入れる

プロジェクト体制を構築する

	Alシステムを構築する上で様々なスキルをもった人かが必要になる
	→マネージャ、デザイナー、プログラマー、
	データサイエンティスト など組織を組むことが大切

	→データサイエンティスト
	対象データの分析、モデルの構築などを行う仕事である


	・プライバシー・バイ・デザイン
	  プライバシーに配慮した設計・運用等を行うこと
	  →あらかじめプライバシーに配慮したシステムを開発することで利用者から信頼を得やすくなる

	・セキュリティ・バイ・デザイン
	  セキュリティに配慮した設計・運用等を行うこと

	・バリュー・センシティブ・デザイン
	  プライバシーやセキュリティなど価値全般に配慮した設計・運用等を行うこと

AI 開発を進める上でどのような規範に則っていくのかということは,政府のみならず民間でも議論が進められている.
例えば先進的な取り組みとして,2008 年に 【AAAI】 では"Presidential Panel on Long-Term AI Futures: 2008-2009" と題して研究者が集まり,
AI の社会的課題について検討するサブグループにて予想される様々な倫理的問題,法律的課題について議論がなされた.
そのほかにも,イーロン・マスクが 2015 年に非営利団体として設立した 【OpenAI】 は,AI 研究の為のオープンソースソフトウェアを提供するなど様々な形で
AI 研究を推進・サポートするとともに,技術的な取り組みを超えて AI の安全性に関する課題への対応策として,「失敗モデルへの五つの対応」を取りまとめて公開共有している.
また AI 研究に力を入れる主要企業でもこうした議論はなされている.著名なものとしては,Microsoft 社が 2016 年に発表した「AI 開発原則」などが有名である.

AAAI: 米国人工知能学会
AI100: One Hundred Year Study on AI
IEEE: 米国電気電子学会

Microsoft 社の AI 開発原則「AI に求められるもの」
 x人間の「置き換え」  *正しくは,人間の「置き換え」でなく「拡張するもの」
 〇透明性の確保
 〇多様性の確保
 〇プライバシーの保護
 〇説明責任の義務
 〇偏見の排除を提唱

AI の社会実装を進めていくにあたり,AI がもたらす【3．倫理的】リスクを事前に考慮しておく必要性が近年強く叫ばれている.
各国政府はそれに対応すべく様々な取り組みを行っている.
【2．米国】政府の例を取ると,【2．米国】政府は 2016 年10 月に "PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE" を発行し,
続けざまに同年 "THE NATIONAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN" ,
そして 2016 年 12 月に発行した "ARTIFICIAL INTELLIGENCE AUTOMATION, AND THE ECONOMY" などで,これから表面化するであろうリスクへの対応策を事前に協議している.

AI 研究の第一人者で,2014 年から2017 年にかけて【Baidu】 の AI 研究所所長を務めたAndrew Ng が創業した【Coursera】などは入門から上級まで様々なレベルの AI講義が開かれており,
多くの受講者を惹きつけるに至っている.

ディープラーニングの活用を進めていく必要性の高まりに対して,人材不足を解消するべく,様々な方法で AI に理解のある人材育成が試みられている.
そのような試みの一つとして,【MOOCs】は期待を寄せられている.

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
48. データの収集と法律
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
データの収集方法および利用条件の確認

	AIと法制度の課題
		学習するデータと学習済みモデルの知的財産権の保護と流通容易性が矛盾

	Al開発において質の高いデータを多く集めることが大切
	→自社だけでデータを集めることは難しい場合はオープンデータセットなどを購入する

	・オープンデータセット
	  企業などが提供しているデータセットのこと

	・画像関係
	  ImageNet、PascalVOC、MS COCO など

	・自然言語関係
	  WordNet、SQuAD、DBPedia など

	利用条件などによりオープンデータセットが使えない場合は自分たちでデータを収集する必要がある
	→コスト面を意識しながらどのようなデータを収集するのか考える
	→カメラなどセンサを活用してデータを収集していく
	センサにはX線、温度計など様々な種類のものが存在する

	データの利用条件や制約を確認していくことが大切
	・ 個人情報保護法
	・ 不正競争防止法
	・ 著作権法
	・ 特許法
	・ 個人の契約

個人情報保護法
	個人情報の利用は法律によって制限されている
	→氏名、生年月日、個人識別符号など個人を特定する情報
	生存する個人を対象にしているため亡くなった方の情報は個人情報に含まれない
	5000人分以下の取扱い事業者->平成29年5月30日以降は、個人情報を扱うすべての事業者が対象

	・個人識別符号
	  マイナンバー、 遺伝子情報など個人を識別するもの
個人データ
	個人情報データベースで管理されている個人情報

保有個人データ
	個人情報取扱事業者が、開示・修正・追加・削除など行うことができる権限をもった個人データのこと
	生命や身体、財産に危害を及ぼす妨れのあるもの などは保有個人データに分類されない
	保有個人データを取り扱っている事業者は、本人からの請求に応じて、個人情報を開示、訂正、利用停止等しなければなりません。
	従来、保有期間が6ヶ月を超えるもののみが保有個人データとして定義されていましたが、GDPRとの関連から、短期保存データもそれに該当するようになった。

個人情報＞個人データ＞保有個人データ

要配慮個人情報
	人種、犯罪歴など配慮すべき情報、本人の同意なく収集、第三者に提供することは禁止されている

不正競争防止法

	事業者間の不正競争を防止するための法律
	→コピー商品の販売、ドメイン名の不正取得、不正な手段での営業秘密、限定提供データの取得

不正競争
	学習用データ、プログラム、データベース、仕様書、及び、ＡＩによる成果物等のデータは「営業秘密」として保護されている場合がある。
	転職、社外への発表、販売、情報提供、又は、他者との共同開発等で営業秘密を扱う場合には、（【不正競争】）に該当する行為か注意する必要がある。
	例えば、（【不正競争】）となる行為は、不正取得、不正使用、又は、不正開示等がある。
	不正競争防止法では、営業秘密不正取得行為等の「不正競争」を原則禁じている（不正競争防止法第２条第１項各号）。
	不正競争防止法はG検定の試験出題範囲（シラバス）2021なのでおさえておきましょう。

	学習用データ、プログラム、データベース、仕様書、及び、ＡＩによる成果物等のデータは特許法及び（【不正競争防止法】）の法律で模倣されるのから保護できる。
	共同使用、販売又は共同開発等でオープンにする客体と、他者には秘密にする客体とで保護方法を使い分ける必要がある。
	オープンにするプログラム等は特許出願をして特許権を取得して保護を図る。
	一方で、社外へ開示しないデータ等は（【不正競争防止法】）における営業秘密で保護する方法等がある。
	不正競争防止法第２条第６項で定められている　　有用性、秘密管理性、及び、非公知性　　であり、この３つの要件を備える「営業秘密」等は不正競争防止法で差止請求等の保護が可能である。
	オープンイノベーション等といった他者と連携する場合等では互いに保有する技術を共有する場合が多いため、
	特許権等の権利を確保するか、又は、営業秘密で保護するかのどちらで保護するかを使い分ける戦略が必要である。
	不正競争防止法・特許法はG検定の試験出題範囲（シラバス）2021なのでおさえておきましょう。

営業秘密
	  秘密管理性、有用性、非公知性の3つの要件を持つ情報
	  →製造に関するノウハウ、顧客情報 など
	  不正競争防止法はG検定の試験出題範囲（シラバス）2021なのでおさえておきましょう。

	  学習用データ等のデータ、及び、プログラムは特許権又は営業秘密等で法的に保護できる場合がある。
		学習用等のデータ、プログラム、データベース、仕様書、ソースコード、及び、設計図のうち、
		営業秘密で保護できる可能性があるのは（【すべて】）である。
		営業秘密として保護の客体となるには、有用性、秘密管理性、及び、非公知性等を備える必要がある。
		ただし、不正競争防止法で保護の客体となるには、営業秘密である必要がある。
		営業秘密となる要件は不正競争防止法第２条第６項で定められている有用性、秘密管理性、及び、非公知性であり、この３つの要件を備えれば形式や書面であるか否かを問わない。
		不正競争防止法はG検定の試験出題範囲（シラバス）2021なのでおさえておきましょう。

		学習用のデータ、及び、プログラムといった無体物を扱うには法律上、有体物とは扱いの違いに注意する。
		データは民法上、所有権、占有権、用益物権、及び、担保物権等の対象にできない。
		そこで、（【営業秘密】）として不正競争防止法等で保護する配慮が必要となる。

・限定提供データ
	限定された人に提供されている電磁的方法で記録されたデータ
	学習等にデータを使用、他者と共有、又は、他者へデータを提供する場合にはデータが営業秘密等であるかに注意する必要がある。
	営業秘密、又は、（【限定提供データ】）のデータは、不正に取得されたものを使用、又は、開示すると不正競争防止法における民事上・刑事上の措置を受ける場合がある。
	（【限定提供データ】）は、例えば、データ分析事業者が提供する、船舶からリアルタイムに収集するデータ、又は、分析したデータ等である。
	この場合には、（【限定提供データ】）は、データ分析事業者から造船所、船舶機器メーカ、気象会社、又は、保険会社等に提供される。
	そして、（【限定提供データ】）の提供を受けた者は、造船技術向上、保守点検、新たなビジネス等に役立てるのに（【限定提供データ】）を用いる。

	学習等にデータを使用、他者と共有、又は、他者へデータを提供する場合にはデータが営業秘密等であるかに注意する必要がある。
	（営業秘密・限定提供データ】）のデータに対し、不正に取得されたものを使用する行為等が、不正競争防止法における民事上・刑事上の措置を受ける場合がある。
	（営業秘密・限定提供データ】）でなくとも、ガイドライン等で一定の行為が禁止されているデータもある。例えば、金融分野における「機微（センシティブ）情報」等がある。
	営業秘密は、有用性、秘密管理性、及び、非公知性の要件を備える情報である（不正競争防止法第２条第６項）。
	限定提供データは、「業として特定の者に提供する情報として電磁的方法（電子的方法、磁気的方法その他人の知覚によっては認識することができない方法をいう。
	次項において同じ。）により相当量蓄積され、及び管理されている技術上又は営業上の情報（秘密として管理されているものを除く。）」である（不正競争防止法第２条第７項）。

知的財産権

	特許権 (20年) 、実用新案権 (10) 、意匠権(20) および商標権 (10) を産業財産権と呼ぶ

著作権法
	音楽や写真・小説など著作者の権利を守るための法律
	→無断で著作物を利用することが禁止されている
	  マニュアル、プログラムコード、学習済みモデルも保護の対象
	→著者権は著作物を創作すると同時に生じる

	制限はあるものの著作物を引用の形で利用することができる

	情報解析に使う場合は著作物を利用できるとされている
	→1人1人に許可を取る必要がない
	他の先進国の場合は許可を取る必要がある

	学習用データセットなどもデータベース著作物として保護

	・データベース著作物
	  情報の選択または体系的な構成によって創造性を有するもの

	・著作権法
	  Alが自ら作成したものは著作物として認められていない★
	  Alを活用して作成したものは著作権が認められている★

	著作物とは、思想または感情を創作的に表現したものであって、文学、学術、美術、または音楽の
	範囲に属するもののこと

	ソースコードは著作物

	著作権は著作財産権と著作者人格権で成り立っている

	著作者の死後50年を経過するまでの間、存続。

	第三者の著作物を利用する場合には、原則として、著作権の承諾が必要とされます。
	しかし、著作権30条の4第2号は、情報解析のために行われるデータの利用についての権利制限を定めています。
	AI開発用データセットを作成する場合、元となるデータ等の著作権者の承諾は原則として必要とされません

	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	AI が生成した創作物はそれを生み出す過程において人間による創作的寄与がある場合において著作物性が認められる
	インターネット上のデータ等の著作物から学習用データを解析することは営利目的の場合まで含め著作権侵害には当たらない
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
特許権法

	発明者の権利を守り、産業の発展を促進させる法律
	→特許庁に出願して登録されることで権利を得る

	→日本では発明者に特許権が帰属される★

	AI の生成物における特許の保護についても見解が分かれている.
	日本においては,特許庁の調査によると,現行の特許法では発明者が自然人であることが前提とされており,
	「自然人の創作」であることが認められない限りにおいては特許の保護を受けることは難しいとされている.
	アンケート調査に基づくと,「創作」を三つのステップから構成されるものとして,そのどれかを人間が担当することが「自然人の創作」であることを満たす条件になるのではないかということ議論されている.
	他方で自然人が発明の着想者であるか否かという観点も含めるべきではないかという意見もあり,統一見解は存在していない.
	現行の特許法下で「自然人の創作」を認める上で必要とされる「創作」の三つのステップに含まるもの
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	〇課題の設定
	x独創的解決策の発見
	〇解決手段候補の選択
	〇実効性評価
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

機密情報は取得・利用または第三者提供の全てが原則禁止とされ、個人情報保護法の「要配慮個人情報」よりも厳しい取り扱いが要求されている

AI の生成物の代表的な例である,学習済みモデルをいかにして保護するのか,どのようなものが保護されうるのかということも議論の対象になっている.
特に解釈が難しいものの一つに,ネットワーク構造とパラメータがブラックボックス化してしまい人間が外からは観察できない状態のモデルを利用して他のタスクに転用する新しいモデルを作成した場合,
すなわちいわゆる 【蒸留モデル】がある.
著作権による保護は非常に困難であるとされているが,特許法については保護の可能性があるのではないかとされている.
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
49. データの収集と外部連携
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

・サンプリングバイアス
	抽出したサンプリングに偏りがあること

→日本全体の給与を知るためのAlを開発するときに
	サンプルデータがデータサイエンティストばかりなど

→現実世界の偏見などがAIに反映され偏りが生じてしまう
	→偏りが生じてしまうのではないかと考えられる

→バイアスを評価・検証するためにも、
	開発者側に説明責任が求められる (透明性)

・外部の役割と責任を明確にした連携
	Alを開発する場合、様々な専門知識が必要になるため外部と連携していくことが多い

→大学・研究機関と企業が連携する産学連携 など
	オープン・イノベーションも活発になっている
	→社外のノウハウなどを活用してイノベーションを目指すこと

多くの人がプロジェクトに関わることで
責任や役割が分かりにくくなるデメリットも存在する
	→役割・責任を明確にしていくことが大切になる
	→企業文化によりトラブルが生じてしまうケースもある

・Al・データの利用に関する契約ガイドライン (経済産業省)
	→データ契約に関して実務の実績が少ないことから
	様々なトラブルが発生してしまう可能性がある

→データ契約に関する雛形を作成し、取引費用の削減
	データ契約の普及、データの有効活用を促進する目的がある

・「データ提供型」契約
・「データ創出型」契約
・「データ共有 (プラットフォーム型) 」契約

3つ契約類型が提示されている

参考 : 経済産業省 「Al・データの利用に関する契約ガイドライン」

学習用のデータ等は大量のデータを効率よく収集する必要がある。
官民データ活用推進基本法により、（国及び地方公共団体）はオープンデータに取り組むことが義務づけされた。
オープンデータ等のデータをうまく利用するのが求められる。
官民データ活用推進基本法（平成２８年法律第１０３号）で、オープンデータへの取り組みが義務付けられ、オープンデータへの取り組み組により、
国民参加・官民協働の推進を通じた諸課題の解決、経済活性化、行政の高度化・効率化等が期待されている。
機械学習用に使えるオープンデータのデータセットを検索できるサイト等もある。
オープンデータはG検定の試験出題範囲（シラバス）2021なのでおさえておきましょう。

秘密保持義務
	学習等に用いるデータを他者から提供してもらう場合、又は、共有する場合にはデータの利用に関して契約を結ぶ。
	外部に漏洩したくない情報を扱う場合には、契約で（【秘密保持義務】）を課す。契約後、（【秘密保持義務】）がある者のみで営業秘密等を扱うように配慮する必要がある。

	営業秘密等の情報を扱う従業員等には契約で秘密保持義務を課すのが重要である（AI・データの利用に関する契約ガイドライン 1.1版）。
	秘密保持義務のある者の間で扱われる情報は、秘密の状態、すなわち、「非公知性」を確保でき、営業秘密で保護できる場合がある。
	「秘密保持義務」は「守秘義務」と呼ばれる場合もある。

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
50. データの加工・分析・学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
・データの加工
	収集したデータをそのまま使うことが難しい場合は
	データをAIが学習しやすいように加工する必要がある

	教師あり学習の場合、正解のデータやラベルなどを作成する必要がある (一貫性のあるデータ作成が大切)
	→以上のような作業をアノテーション★と呼ぶ
・プライバシーの配慮
	電子決済などにより購入履歴などのデータを取得しやすくなった
	→電子化により顧客情報を取得しやすくなった
	→データを分析し、第三者に販売することで得られるメリットも大きいが目的外の用途使う場合は同意が必要
・匿名加工情報
	個人情報を加工して、個人を特定できないように、元の個人情報を復元できないようにした情報のこと
	→同意なく目的外利用・第三者への提供が可能

	学習等にデータを使用、又は、他者へデータを提供する場合には個人情報の扱いに注意する必要がある。
	例えば、個人情報を示す元のデータを特定の個人を識別できないように加工し、個人情報を復元できない状態にした「匿名加工情報」にする必要等がある。
	匿名加工情報にするには、（【個人情報に含まれる個人識別符号の全部を削除】）が必要となる。
	匿名加工情報であれば、一定の条件下で本人同意なく事業者間でやりとりが可能となる（個人情報の保護に関する法律第２条）。
	匿名加工情報として扱うには「特定の個人を識別すること及びその作成に用いる個人情報を復元することができないようにするため」加工を行う義務がある（個人情報の保護に関する法律第３６条）。

	マスキングでは十分ではない（個人情報保護委員会ホームページ「1.匿名加工情報とは」）
	特異な記述や珍しい事例は削除が必要である（個人情報保護委員会規則第１９条（第４号））
	復元できないように加工する義務がある。

・仮加工情報
	他の情報と照合しない限り特定の個人を識別することができないように加工された個人情報
	→他の情報と照合することで個人情報を復元できる
	→目的外の利用はできるが、第三者への提供はできない
	業務委託先には提供することはできる

・ELSI (Ethical, Legal and Social lssues)
		備理的・法的・社会的な問題のこと
		→人工知能を開発すると同時にELSIについて考える必要がある

・開発・学習環境の準備

	有名なライブラリ

	・Chainer  : 自然言語処理の分野でよく使われている
	など

→XAl (説明可能なAl) の開発が進んでいる
結果だけではなく、プロセスなども説明できるAlのこと

・フィルターバブル現象
	利用者の趣味嗜好を学習し、利用者の興味のある情報を表示させ、興味のない情報を表示させない機能により泡の中に包まれたように自分の興味のある情報しか見なくなる
	→異なる考えに増える機会が少なくなり、視野が狭くなる

・FAT (Fairness、Accountability、Transparency)
	公平性、説明責任、透明性についても考えることが大切


・PoC (Proof of Concept : 概念検証)
	開発前にプロジェクトかが実現可能かどうかを検証すること


:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
51. 実装・運用・評価
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

GDPR (EU一般データ保護規則)
	→個人データ・プライバシー保護に関して規定された法令
	原則としてEU領外への個人データの持ち出しを禁止

	→日本は「個人データについて保護水準を満たす国」だとする
	十分性認定を受けおり、個人データを持ち出すことができる
	日本企業がEUにサービスを提供している場合、この規則の対象になる
	▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
	ドイツや EU においてはインターネット上のデータや著作物から学習用データを解析することは営利目的の場合まで含め著作権侵害には当たらない
	▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲

悪用へのセキュリティ対策
	・敵対的な攻撃
	  Alモデルに誤った出力をさせるように攻撃を仕掛けること
	→利用者が悪意を持って間違った情報を大量に
	入力することでモデルが変化してしまう
	→システムに侵入しモデルを書き換える など

	自動運転のAlモデルが書き変わってしまうと混乱が生じる
	→悪意のある攻撃からどのように守るかが大切になる
	→ハッキングや不正アクセスに日々対抗し続ける
	  問題が起きたときの対処法なども考えることが大切になる
ディープフェイク
	ディープラーニングを利用して、動画や画像などを
	組み合わせ、フェイクの動画・画像などを作り出すこと
	→顔を入れ替えたフェイク動画、画像も作れてしまう
	→フェイクニュースを見極めるのが難しくなっている
	・予期しない振る舞いへの対応
	  Alは進化しているものの、予期しない振る舞いをする
	→予期しない振る舞いをしたときの対応が大切

アルゴリズムバイアス
	偏見や誤ったデータを学習してしまったことで
	偏った結果を出力してしまうこと
	Amazonで使用された履歴書を評価するシステムで
	一部の女性に対して不利な結果が表示されていた

	→現場には男性が多く、男性の方が好ましいとAlが学習
	→予期しない振る舞いがあるということを知ることが大切になる

インセンティブの設計と多様な人の巻き込み
	商品・サービスを開発していく上で
	新しい技術を入れることが先行してはいけない (ビジネス)
	→ステークホルダーのニーズをもとに作ることが大切

	・ステークホルダー (利害関係者のこと)
	顧客、取引先、従業員、経営者、株主など
	開発したAlを利用してもらえるように
	インセンティブ (報酬) の設計を行うことも大切

	→多くの人を巻き込むための仕組みも大切
	→新しい技術は難しそうという印象からサービスを使ってくれない
	コストをかけてシステムを作って、誰も使っていないという現象

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
52. クライシス・マネジメント
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
体制の整備
	・クライシス・マネジメント (危機管理)
		危機は発生するものと考え、危機が生じたときに
		最小限に被害を抑えるための方法

	・リスク・マネジメント (リスク管理)
		問題が起きないように事前に策を講じること

	・コーポレートガバナンス (企業統治)
		企業経営が適切に行われているか監視する仕組みのこと
		→社外取締役、委員会の設置 など

	・内部統制の更新
		経営目標を達成するために社内で守るべきルール・仕組み
		ルール・仕組みが整っていたとしても現場の人々に
		理解してもらわないと形骸化してしまう

		→よく分からずにルールに従っている
		  ルールを知らずに守っていない    など

	・炎上対策とダイバーシティ
		データセットの偏りなどから炎上することがある
		→炎上しないためには、ダイバーシティ (多様性) 、
		インクリュージョン (包括性) を理解すること

		→性別、人種など介らないようにすることが大切

	・シリアル・ゲーム
		社会問題をゲームという形で体験し、解決するゲームのこと

		→ゲームとしてエンターテイメント性も存在する

有事に対応する
	ソーシヤルズティブを通じてクライシスが発生
	→マスメディアが取り上げることでさらに炎上するケース

	世界中でAlと安全保障・軍事技術に関する議論がなされている

	自動的に攻撃対象を定め攻撃する自律型兵器 (AWS)
	自律型致死兵器システム (LAWS) の研究開発を禁止すべきだと世界中で議論されている

	・透明性レポートの公開
		利用者のデータなどをどのように活用しているのか、
		政府などに情報提供した件数 などをまとめたレポート

	→GoogleやAmazonなど多くの個人情報を扱う企業がWebなどで公開している

	・指針の作成と議論の継続
	問題が起きた場合の指針が必要とされるようになってきた

	→アメリカのIT企業を中心にPartntership on Alが組織される

	→NPO法人であるFuture of Life Instituteが倫理的問題など
	  について2 3の原則をまとめる (アシロマAI原則)

	・指針の作成と議論の継続
		内閣府 (日本) が人間中心のAl社会原則を公開した

		→AIをどのように管理してくのかについてで世界中で議論されている
		→新しい指針が今後作られていくと考えられる

2019年に内閣府にて「人間中心のAI社会原則」が決定された。ここに示された基本理念に基づく「AI社会原則」に当てはまる選択肢を1つ選べ。
3．セキュリティを確保できること

2,4は「人間中心のAI社会原則」に記載されている「基本理念」になります。
2．多様な背景を持つ人々が多様な幸せを追求できること
4．持続性のある社会を実現できること

AI の社会実装を進めていくにあたって,あまりに大きな影響が予想されるが為に様々なリスクを想定して事前に議論を進めることがなされている.
総務省はAI の社会実装によって想定されるリスクを大きく4分類で整理している.
・AI 自身のリスク
	AI のアルゴリズムがブラックボックス化し,人間の制御ができなくなってしまう
・AI に関わる人間のリスク
	人間が AI を利用した無差別殺人を行う
・社会的負のインパクト
	AI が人々の職を奪ってしまう
・法律,社会のあり方
	自動運転車が引き起こした事故は誰が責任を負うのか

AI の活用が国の経済成長を牽引する柱の 1 つになるという共通認識から,先進国各国では国の経済成長戦略の一部に AI の研究開発戦略が盛り込まれるようになっている.
日本の経済成長戦略の組み合わせとして正しいものを 1 つ選べ.
4．新産業構造ビジョン
【解説】
正しい組み合わせは以下の通り.
日本 - 新産業構造ビジョン
英国 - RAS 2020 戦略
ドイツ - デジタル戦略2025
中国 - インターネットプラスAI3年行動実施法案
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
53. 自動運転
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

・自動運転のレベル (SAE J3016)
	0 .全て人間が運転する

	1 .システムが加速・減速制御、ハンドル操作の一方を実施

	2.システムが加速・減速制御、ハンドル操作をともに行う
		運転主体は人間になる

	3 .限定した領域でシステムが運転タスクを実施するが、
		人間が制御する必要もある
		運転主体はシステム、人間が制御している場合は人間

	4.限定した領域でシステムが全ての運転タスクを実施

	5 .領域を限定せずシステムが全ての運転タスクを実施

自動運転レベル 5 に至るには,2 つのアプローチが存在している.
1つは自動運転レベル 1 から徐々に運転自動化の範囲を広げていくアプローチ,もう 1 つは直接レベル【3】以上の自動運転を目指そうとするものである.
この時,前者のレベル1から徐々に運転自動化を目指すアプローチを採っているプレイヤーは 【Waymo】などである.

2 0 2 1 年、日本でレベル 3 の自動運転車が販売された
	→特定条件下で、システム主導による運転が可能
	(渋滞していないときの高速など)
	→道路交通法、道路運送車両法が改正されつつある

自動運転の実現に向けては,現行の法の枠組の中では公道での実験の可否や,事故が起きた際の責任の所在などの点で捉えづらい面が顕在化しており,
こうした点の解釈のすり合わせや,新たな法の策定などが求められている.
以下の自動運転走行許可の各国・各地域のスタンスに関する説明文として正しいものを選択肢から 1 つ選べ.
4．米国ネバダ州では自動運転の走行や運転免許が許可制にて認められた

人工知能
レベル 1
学術分野としては制御工学やシステム工学に対応する,単純な制御プログラム
レベル2
探索や推論を活用し,限定された環境下における問題解決を行えるプログラム
レベル3
機械学習を活用し,多量のデータをもとに推論の仕組みや知識ベースが作成されているプログラム
レベル4
特徴表現学習と呼ばれ,特徴量自体を学習することができるプログラム
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
人物
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ヨシュア＝ベンジオ		ディープラーニングを初めとする様々な手法の提案者
イアン=グッドフェロー	生成敵対ネットワークの提唱者
ジェフリー＝ヒントン　Geoffrey Hinton	誤差逆伝播法を初めとする,様々な手法の提案者
アンドリュー・ン		著名な研究者.Courseraの創始者でもある.
トロント大学のヒントン教授ら Geoffrey Hinton　ディープラーニングの第一人者
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
偏微分
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
以下の関数 z を y について偏微分したあとの式として,適切な選択肢を 1 つ選べ.
z = x^2+3x-5y^2+3y-10
正解：3
あなたの回答：4
3．
-10y+3

4．
2x-10y+6

微分
　f(x)=x^5+3x^2+3を定義に従って微分しなさい。

　=5x^4+6x

ドローンを飛ばすのに許可を必要とする空域の説明として正しいものを選択肢から 1 つ選べ.
1．150m 以上の高さの空域

ドローンの飛行規制について,正しいものを選択肢から 1 つ選べ.
3．ヒト・モノから 30m 以内の飛行の禁止
【解説】
改正航空法;平成27年12月10日施行
https://www.mlit.go.jp/common/001109793.pdf

パッケージ
	Keras,TensorFlow,PyTorch

ライブラリ
	Keras（ケラス）

フレームワーク
Chainer (チェイナー)
	日本国内においても PFN 社が提供する Chainer  PyTorch
TensorFlow
	google 社が提供
CNTK
	Microsoft 社が提供

→DeepMind社が開発 英国
	DQN (Deep Q-Network)
	AlphaGo

Kaggle
	データサイエンスのコンペティションプラットフォーム
Courserra
	スタンフォード大学のアンドリュー・エンとダフニー・コラーの二人によって設立されたMOOCs
arXiv
	機械学習などの論文が無料でダウンロード可能
