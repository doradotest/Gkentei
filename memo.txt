:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ベイズの定理
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
観測された事象（結果）が起こったもとで、背後に潜む事象（原因）がどうであるかの確率を表すのに役立つ
事後確率
事前確率　結果がまだわからない状態P(A)
尤度（ゆうど）　右辺のP(B|A)は結果に対する原因のもっともらしさ
	確率モデルにおいて、想定するパラメータが具体的な値を取る場合に観測されたデータが起こり得る確率のこと
	データからモデルのパラメータを推定する方法の１つ
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
4.人工知能の定義
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
人工知能（Artifical  Intelligence）
	推論、認識、判断など人間のような知的なふるまいをする人工的に作られた機械のこと（計算機科学の一分野）
	★ジョン・マッカーシーがダートマス会議で使った言葉（1956）ダートマス大学
人工知能のおおまかな分類
	・ルールベースの制御（制御工学）
	・古典的な人工知能（人工知能）
	・機械学習（人工知能）
	・ディープラーニング（人工知能）
	自らの判断で自律的に振る舞うエージェントに基づき分類
ルールベースの制御
	あらかじめ決められた設定に基づいてふるまう
	冷蔵庫、エアコン、炊飯器など
古典的な人工知能
	多くのデータを活用して、複雑なふるまいが可能
	→医療診断プログラム、掃除ロボット（ルンバ）　など
機械学習を取り入れた人工知能
	大量のデータからルール・関係性などを学習する
	→検索エンジン、スパムメール判定、交通渋滞予測　など
ディープラーニング（人工知能）
	何に注目すればいいか（特徴量）を自らで判断し学習する
	→画像認識、音声認識、機械翻訳など
機械学習（犬の写真）
	何に注目するべきか（特徴）を教える必要があった
	→目、口、耳など犬としての特徴を設定する必要があった
AI効果
	人工知能とみなされていた技術で課題が実現されるとそれはただの自動化であって人工知能ではないと
人工知能とロボットの違い
	ロボットは予め決められた動作を行うのが基本になる
	→産業用のロボット、運搬ロボット、などの装置・機器（体）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
5.人工知能
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
世界初の汎用コンピュータ
	アメリカで世界初の汎用コンピュータ
	【エニアック（ENIAC）】をペンシルバニア大学が開発(1946)
人工知能研究のブームと冬の時代
	第一次人工知能ブーム：1950年代後半～1960年代
	第二次人工知能ブーム：1980年代
	第三次人工知能ブーム：2000年代～
第一次人工知能ブーム（推論・探索の時代）
	迷路の謎解きなど簡単な問題（トイ・プロブレム）はコンピュータが解けるようになった
第二次人工知能ブーム（知識の時代）
	特定の専門分野の知識をもとに、専門家のように様々な推論・判断を行うシステム（エキスパートシステム）
	大量の知識を与えるコスト・管理するコストが大きかった
第三次人工知能ブーム（機械学習・特徴表現学習の時代）
	大量のデータ（ビックデータ）を用いることで人工知能が自ら知識を獲得する機械学習が実用化された
	→データの特徴を定量的に表したもの（特徴量）を人工知能が自ら学習するディープラーニングが登場

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
6. 探索・推論（探索木）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
探索木
	１つ１つの要素をノードと呼ぶ
幅優先探索
	初期のノードから見て近いノードから探索を行う
	探索したノードをすべて記憶しなければならないためメモリが必要
	最短経路を見つけ出すことができる(ステップ数が少なければ)
	深い場所に解答がある場合、途中で記憶容量がパンクする
深さ優先探索
	あるノードから行き止まりになるまで行き、行き止まりになったら、
	１つ手前のノードまで戻りまた行き止まりになるまで探索を行う
	記憶するものが少なくメモリが少なくて済む
	最短経路とは限らない
ハノイの塔
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
7. 探索・推論（行動計画とボードゲーム）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ロボットの行動計画
	プランニング
	前提条件、行動、結果を組み合わせて１つの動作を定義する自動計画を記述する手法
	プランニングシステム、【STRIPS(ストリップ)】
SHRDLU(シュルドゥル)
	テリー・ウィノグラード
	コンピュータに英語で指示を出すと端末の画面上の物体を動かしてくれる（積み木）
	研究成果はCycプロジェクトに引き継がれていく
ボードゲーム
	組み合わせの数はオセロ、チェス、将棋、囲碁の順
	コスト（ステップ数）の概念を取り入れて探索を効率化しようとする
Mini-Max法
	自分が手を打つときは自分が有利に、相手が手を打つときは相手が不利
	自分も相手も最善手を打つ仮定のもとにスコアを逆算する
	あらかじめどのような手が有利か不利かをスコア化（コスト）
	すべての手を計算すると時間がかかるデメリットがある
αβ法
	Mini-Max法において、スコアが低い手を切り捨てる
	αカット、βカット
	切り捨てた中に実は最善の手があるかもしれない
	すべてのパターンを調べる方法（ブルートフォース）の方が勝てる確率は上がる
	Mini-Max法、αβ法では「相手は自分のスコアが小さい手をとる」
	Negaとつくと、「相手が相手自身のスコアが高くなるような手をとる」
ヒューリスティックな知識
	経験による知識やルールのこと
	精度は高くないが、ある程度正しい答えを出せる方法
モンテカルロ法
	ゲームの終盤まで進んだら、あらかじめ決めたスコアを無視して、AI同士でランダムな手を打って、結局（プレイアウト）させる
	複数回プレイアウトすると、勝ちやすい手などがわかりスコアをつけていく
	人間がスコアをつけるよりも理にかなっている
	DeepMind社が開発した人工知能の囲碁プログラムAlphaGoがプロに勝った(2016年)
	ディープラーニングの技術を使っている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
8. 知識表現
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
人工無能
	チャットボットなどと呼ばれるプログラム
	人間との形式的な会話のために作られたロボット
ELIZA（イライザ）
	１９６４年、チャットボットの元祖となるELIZAが開発される
イライザ効果
	コンピュータとのやりとりを、人間とやりとりしていると錯覚する現象のこと
知識ベースの構築とエキスパートシステム
	DENDRAL（世界初のエキスパートシステム）（読-デンドラル）
	1960年代の人工知能に関するプロジェクト
	→未知の有機化合物を特定するエキスパートシステム
	【知識ベースと推論エンジン】により構成
	・【マイシン（MYCIN）】1970年代
	スタンフォード大学で開発されたエキスパートシステム
	→感染した細菌を判断するためのシステム
	専門医＞マイシン（MYCIN）＞普通医
知識獲得のボトルネック
	専門家の暗黙知を聞き出すのが困難
意味ネットワーク（セマンティックネットワーク）
	「is-a」の関係：●は●である
	「part-of」の関係：●は●の一部である
	「has-a」の関係：●は●を持っている
	・is-a　継承関係を表す
	・part-of　属性を表す。本体と部品の関係
ナレッジグラフ
　　　意味ネットワークの中でも、インターネット上などから半自動で構築しているもの
	LOD（Linked　Open Data）
		ネットワーク形式で公開されているデータセット
		地方公共団体などではこのデータを活用したサービスを展開しているのも見られる
	Mercari
		ナレッジグラフを用いてレコメンドシステムにその理由を示すために使っている
Cycプロジェクト（サイク）
	一般常識もデータベース化して、人間と同様の推論システムを構築することを目的とするプロジェクト
	→1984年に開始され、30年以上も入力作業は続いている
オントロジー
	概念化の明示的・形式的な仕様、概念を体系化する学問
	意味ネットワークの単語と単語をつなぐつなぎ方のルールのこと
	オントロジーに従った記法で統一すれば共有できる
	→意味ネットワークを作成しても作者によって書き方などが異なるため他人が作成したものを利用できない
	→いずれにせよ、記述する人によって記述の方法や記述の粒度がまちまちになると、そもそもの知識の共有化や再利用が難しくなるため、記述ルールの標準化が必要
概念間の関係
	→推移律（AとB、BとCに関係が成立するとき、AとCも成立）
	・part-isの関係：全体と部分の関係、タイヤは車の一部
	→part-isの関係には5種類以上の意味があり複雑
	本来なら分けて関係を示したいが分けられていない

オントロジーの構築
　  ヘビーウェイトオントロジー
	    人間がしっかりと考えて知識を記述していくという考え方
		Cycプロジェクトが長年行っている
　  ライトウェイトオントロジー
	   コンピュータが情報を理解して、自主的に関係性を見つけようという考え方
ライトウェイトオントロジー
　  ウェブマイニング
	  ウェブ上にあるデータ分析して有益な情報を抽出する手法
　  データマイニング
	  ビックデータを分析して有益な情報を抽出する手法
ワトソン
	IBMが開発した質問応答（QUESTION-ANSWERING）のシステム
	→2011年アメリカのクイズ番組で対戦して優勝
	→蓄積されている大量のデータから最適な回答を見つけ出している
	IBMは人工知能ではなく、拡張知能と呼んでいる
	→言葉を理解していない（ライトウェイトオントロジー）
東ロボくん
	東大の入試を突破することを目標にした人工知能
	→2011年から2016年まで続けられた
	→偏差値57を突破したが、東大入試を突破することは現在では不可能であると判断されプロジェクト終了した
	→読解力が必要であり、知識だけで合格が困難であった

セマンティック・ウエブ
	メタデータを活用して、コンピュータがウェブ情報を解釈し処理する技術や考え方のこと
	→メタデータ：データに付与するデータのこと
	「五十嵐すず」というデータに対して「著者」というデータを付与することでコンピュータが理解しやすくなる
	例）写真の情報(Exif情報)
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
9. 機械学習・深層学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
レコメンデーションエンジン
	→ユーザーが興味がありそうなものを提案するシステム　アマゾン、YouTube
スパムフィルター
	→迷惑メールを見つけ出すためのシステム
自然言語処理
	自然言語をコンピュータに処理させること
統計的自然言語処理
	確率推論や統計学を使って、自然言語を処理すること
　  コーパス（自然言語の文章を構造化したもの、対訳データ）を活用することで文章を正しく読み取れる確率が上がる
ニューラルネットワーク
	人間の脳を模倣してモデル化されており、入力層、中間層、出力層の複数ニューロンから構成される非常に複雑な構成
ディープラーニング
	ニューラルネットワークを多層化したもの
	→多層化しても学習精度が思ったように上がらなかった
	→誤差逆伝播法などを取り入れることで学習精度があがる
	自己符号化器（オートエンコーダ）、
	活性化関数などを活用することで問題がすこしずつ解決
ILSVRC（ImageNet　Large　Scale　Visual Recognition　Challenge）
	画像認識の精度を競う大会
機械学習
	データの特徴を定量的に表した特徴量を人間が設定
	→人間の経験と勘によって特徴量を設定していた
	→2015年から人間の認識率を超える精度を誇るようになる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
10. 人工知能分野の問題①
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
トイ・プロブレム
	人工知能は複雑な問題を解くことができない
	ルールがシンプルなもの（ゲーム）しか解けないという用語
フレーム問題
	処理能力が限られている人工知能では、現実に起こりうる全ての問題に対応することができないこと
	問題の枠に囚われてしまい、枠の外を考慮することはロボットには難しい
	自動運転車は事故を必ず起こさないようにプログラムすると、走行速度が遅すぎてうまく機能しない問題
チューリングテスト
	機械は思考できるのかという目的から行われるテスト
	→アラン・チューリングが提案したもの
	→人間が審査員となり、人工知能と人間と会話を行うどちらが人間なのかを当てるテスト
	→人工知能は人間っぽいふるまいを行う
ローブナーコンテスト
	最も人間に近い人工知能を決める
	チューリングテスト形式のコンテストのこと
強いAI
	人間のようなふるまい（思考・判断など）をするAI
	汎用型人工知能
弱いAI
　　特定の問題を解決AI
	特化型人工知能
ジョン・サールが論文で発表したAIの分類
	→弱いAIは現実可能でも、強いAIは不可能であると考えている
	→「中国語の部屋」という思考実験を使って説明されている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
11. 人工知能分野の問題2
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
シンボルグラウンディング問題
	シンボル（記号）を現実世界の意味とどのように結びつけるかという問題のこと
	→スティーブン・ハルナッドによって推論された
	→概念と記号を結びつけることは難しい
	人間の場合、シマウマを、縞模様の馬かもしれないと考えることができる
	→記号のシマウマと実際のシマウマが結びつく
	→コンピュータの場合、記号のシマウマと実際のシマウマを結びつけることができない
身体性
	機械が身体を持つことで高度な人工知能になるという考え方
	→身体を通して得られる情報も必要になってくる
知識獲得のボトルネック
	人間が持っている大量の一般常識をコンピュータが獲得することは極めて難しいこと
	・機械翻訳
　		①ルールベース機械翻訳：1970年代後半から
　		②統計的機械翻訳：1990年代から
　　　		性能は一気に改善されたが、意味を理解しているわけではないので翻訳が大変になる
		③ニューラル機械翻訳
			ニューラルネットワークを活用した翻訳
			→精度が一気に上がり、実務でも使われるようになってきた
			TOEIC900点以上の人間と同じくらいの翻訳力をもつ
特徴量設計
	機械学習では、データの特徴を定量的に表した特徴量の選び方が大切になる
	→家賃を予想する場合、最寄り駅までの距離や築年数などが特徴量として考えられる
■One-hotエンコーディング★★
	つくられた変数をダミー変数という
	〇One-hotエンコーディングでは、たとえば、テーブル形式のデータのカテゴリ変数の列について、取り得る値の分だけ列を増やして、
	  各行の該当する値の列のみに1を、それ以外の列には0を入力するように変換する処理をいう。

		from sklearn.preprocessing import LabelEncoder, OneHotEncoder
		# DataFrameをコピー
		df_ohe = df.copy()
		# ラベルエンコーダのインスタンス化
		le = LabelEncoder()
		# 英語のa、b、cを1、2、3に変換
		df_ohe['B'] = le.fit_transform(df_ohe['B'])
		# One-hotエンコーダのインスタンス化　
		ohe = OneHotEncoder(categorical_features=[1])##変換する列名を指定 categorical_features=[1]
		# One-hotエンコーディング
		ohe.fit_transform(df_ohe).toarray()
特徴量エンジニアリング
	取得済みデータから加工し抽出すること
特徴表現学習
	特徴量をコンピュタ自身が見つけること
	特徴量の加工・抽出も学習器にさせること
	→ディープラーニングも特徴表現学習の１つ
	→家賃などの情報を与え、機械が特徴量を抽出する
	人間は機械が何を特徴量にしたのかよくわからない
シンギュラリティ（技術的な特異点）
	人工知能の進化により、人類の知能を越える転換点のこと
	→人工知能が自分よりも賢い人工知能を作れるようになることで加速度的に人工知能の性能は飛躍すると予想
	→レイ・カーツワイルは、2045年にシンギュラリテイーが訪れることを予想している
	人工知能が人類よりも賢くなるのは2029年と予想している
	→シンギュラリティーが起きる年と人工知能が人類よりも賢くなる年は異なる
	→イーロンマスク、ビル・ゲイツは人工知能は脅威であると考えている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
12. 学習の種類
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
教師あり学習
	答えが分かってる問題を教師が教えながら学習していきます
	正解が与えられたデータをもとに、ルールやパターンを学習して手法のこと
	→分類問題、回帰問題　を解くときに使われる手法になる
	・分類問題：複数のカテゴリに分ける問題
	・回帰問題：連続値（体重、温度など）を予測する問題
分類問題
　	迷惑メールか普通のメールかを分類する　など
回帰問題
　	来店数や温度、気温からビールの売上を予測する　など
教師無し学習
	クラスタリング
		購入データから顧客層に分ける　など
	次元削減(特徴抽出)
		数学、理科を理系、社会、国語を文系にまとめる　など
　　　　扱うべきデータが減り計算、分析しやすくなる
強化学習
　	与えられた環境下で、報酬が最大となる行動を行うように学習していく手法のこと
　「ブラックボックス的な環境の中で行動するエージェントが、得られる報酬を最大化するような状態に応じた行動を学習していく手法」
	→テレビゲームで点数が高くなるような行動をするために学習していく
	（最適な行動は何か）

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
13. 教師あり学習（線形回帰・ロジスティック回帰）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
・時系列分析
線形回帰
	説明変数：説明するための変数、原因になっている変数
　	目的変数：予測したい変数、結果になる変数
	→アイスクリームの売上（目的変数）と気温（説明変数）
	説明変数の数が複数になると。直線から平面になる
	３次元以上の平面を超平面という
	正規化などの前処理を行い説明変数のスケールを揃え、回帰係数の絶対値が大きいものが重要

	from sklearn.linear_model import LinearRegression　線形回帰の実行準備
	from sklearn.datasets import load_boston
	from sklearn.model_selection import train_test_split

	# Bostonデータセットを読み込む
	boston = load_boston()
	X, y = boston.data, boston.target
	# 学習データセットとテストデータセットに分割
	★X_trainは学習用の説明変数、y_trainは学習用の目的変数である。
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
	# 線形回帰をインスタンス化
	lr = LinearRegression()
	# 学習
	lr.fit(X_train, y_train)

最小二乗法
　	実際の値と予想値との差の2乗値が最小になるような重みを決定する方法
擬似相関
　　　因果関係がないのに相関に見えること
重回帰分析の注意点
	多重共線性
　		相関係数が高い説明変数があること
		→計算が不安定になってしまう特徴がある
		説明変数同士の相関が高すぎると、単独の影響を分離したり、その効果を評価したりすることが困難になる
正則化とは過学習を防ぐために使われるテクニック・手法
　	過学習は訓練データを学習しすぎて
　	訓練データ以外のデータに対して精度が悪くなってしまうこと
------------------------------------
ロジスティック回帰（分類問題）
	回帰と言う名前がついているが分類問題に適用されるモデル
　	複数の説明変数から、ある事象が発生する【確率】を予想する手法
	目的変数は０，１
	→複数の説明変数から、再検査が必要かどうか　な
	シグモイド関数［活性化関数］を用いて計算されている
		→結果が2択の時に用いられる
	→結果が3択以上の場合はソフトマックス関数を利用する
　	複数のクラスに分類することができる（多クラス分類）
	オッズ＝事象が起こる確率／事象が起こらない確率＝ｐ／１−ｐ
ある事象が起こる確率ｐの範囲は０〜１，１−ｐも同じ
ｐ／１−ｐの範囲は０〜∞（無限大）
log(ｐ／１−ｐ)の範囲は-∞〜∞
線形判別モデル

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★14.教師あり学習（ランダムフォレスト・ブースティング）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
決定木
	データを分割するルールを次々と作成していくことにより、分類を実行するアルゴリズム
	分類木：試合の勝ち負け　など
	回帰木：住宅の価格　など
	根ノード（root）
	葉ノード（一番下、子を持たない）
	分岐が多くなりすぎると1つの葉に１つのデータが対応してしまう（過学習）
	→過学習を防ぐためにも、木の深さ・幅に気を付ける必要がある
	情報利得が最大化になるように分岐させる
	・情報利得：分割前の不純度から分割後の不純度を引いたもの
				情報利得＝親ノードでの不純度　-　子ノードでの不純度の合計
	・不純度：１つのノードに異なるクラスのサンプルがどれだけ含まれているかという割合
			　ジニ不純度★　各ノードに間違ったクラスに振り分けられてしまう確率
		エントロピー、ジニ不純度、分散誤差などによって計算される
	〇決定木でデータを分割する時は、データの分割によってどれだけ得をするかについて考える。これを情報利得と呼ぶ。
	〇決定木などで利用される木と呼ばれるデータ構造は頂点であるノード（利用回数、利用間隔）とそれらを結ぶエッジ（5回以上、5回未満）から構成される。★★
	　　木の最下部にあり子ノードを持たないノードはリーフと呼ばれる。
	Ｘ情報利得は【子ノードの不純度から親ノードの不純度を差し引いたもの】として定義される。 不純度の指標としてはジニ不純度、エントロピー、分類誤差などが用いられる。

		from sklearn.datasets import load_iris
		from sklearn.model_selection import train_test_split
		from sklearn.tree import DecisionTreeClassifier　
		# Irisデータセットを読み込む
		iris = load_iris()
		X, y = iris.data, iris.target
		# 学習データセットとテストデータに分割する
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
		# 決定木をインスタンス化する (木の最大深さ=3)
		tree = DecisionTreeClassifier(max_depth=3)
		# 学習
		tree.fit(X_train, y_train)

		決定木の描画
		　pip install pydotplus

		from pydotplus import graph_from_dot_data
		from sklearn.tree import export_graphviz
		# dot形式のデータを抽出
		dot_data = export_graphviz(tree, filled=True,
		                rounded=True,
		                class_names=['Setosa',
		                     'Versicolor',
		                     'Virigica'],
		                feature_names=['Speal Length',
		                               'Spal Width',
		                               'Petal Length',
		                               'Petal Width'],
		                out_file=None)
		# 決定木のプロットを出力
		graph = graph_from_dot_data(dot_data)
		graph.write_png('tree.png')

		# 予測
		y_pred = tree.predict(X_test)
		y_pred

ランダムフォレスト（分類と回帰）
	バギングを用いる。
	たくさんの決定木を並列で学習し、それらを統合するモデル。特徴量を重複ありでランダムでサンプリングして、各決定木で使う
　	ランダムに選ばれた訓練データで学習した決定木を複数作成し、決定木で予想された結果の多数決でクラスを決める手法
	回帰では平均値を求める
	→精度の悪いものがあったとしても複数組み合わせると精度が高いものになるという考え方にもとづく
	ランダムフォレストは、アンサンブル学習の中でもバギングを用いており、またベース学習器として決定木を用いた手法

	〇scikit-learnでランダムフォレストを実行するにはensembleモジュールのRandomForestClassifierクラスを使用する。
　		パラメータで決定木の個数を指定することもできる。
	〇★★ランダムフォレストは、ブートストラップデータを用いて決定木を構築する処理を複数回繰り返し、
　　　各木の推定結果の多数決や平均値により分類・回帰を行う手法であり、アンサンブル学習の1つである。
	〇ランダムフォレストは、データのサンプルと特徴量をランダムに選択して決定木を構築する処理を複数回繰り返し、
　		各木の推定結果の多数決や平均値により分類・回帰を行う手法である。

		from sklearn.ensemble import RandomForestClassifier
		# ランダムフォレストをインスタンス化する
		forest = RandomForestClassifier(n_estimators=100, random_state=123)
		# 学習
		forest.fit(X_train, y_train)
		# 予測
		y_pred = forest.predict(X_test)
		y_pred

アンサンブル学習
　	別々の学習器を組み合わせる学習器を作ること
	→バギング、ブースティングという手法がある
	・バギング
　　	複数のモデルを並行的に学習させていく方法（復元抽出）N個の予測値の単純平均
		→ランダムフォレストはバギングの１つで学習時間が短いのが特徴
	・ブースティング
		複数のモデルを直列的に学習させていく方法
		→前に作成したモデルを修正して新しいモデルを作成していく
		学習器を増やしすぎると過学習を起こしやすい
	・スタッキング
		あるモデルの予測値を新たなモデルのメタ特徴量とする

ブートストラップサンプリング（Bootstrap Sampling）　重複ありで同じ大きさのサンプルを得る

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★15. 教師あり学習（サポートベクターマシン・k近傍法）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
サポートベクターマシン（SVM）（分類と回帰）
	主に分類で使われるアルゴリズム
	明らかに所属クラスが分かる観測ではなく、判別境界の付近にある判別の難しい学習データに着目する分類モデル
	→マージン最大化する線を求めることで分類を行う
	マージン
　		線（決定境界）とクラスの各データの距離のこと
	サポートベクトル
		判別境界に最も近い学習データ
	ハードマージン
　		マージンの中にデータが入ることが禁止
	ソフトマージン
　		マージンの中にデータが入ることを許容
	→スラック変数：どの程度強要するか調整
	〇サポートベクタマシンは、【分類・回帰だけでなく】外れ値検出にも使えるアルゴリズムであり、直線や平面などで分離できないデータであっても、
　		高次元の空間に写して線形分離することにより分類を行うことを可能にする。
	Ｘサポートベクタマシンは、分類のみに適用できるアルゴリズムである。直線や平面で線形分離できないデータも高次元の空間に写像して線形分離することで分類を行うことを可能にする。
	Ｘサポートベクタマシンで決定境界を求める際にマージンを最小にする理由は、決定境界がサポートベクタから近くなり、多少のデータが変わっても誤った分類を行う可能性を低くできると期待できるからである。

	サポートベクタマシンの決定境界は、マージンを（「最小」ではなく）最大にすることで求めます。
		ハイパーパラメーターのCの値が小さいほど、マージンは大きくなる

カーネル法を組み合わせることで直線で分類できないものでも分類ができる
 	次元を拡張させる関数をカーネル関数という
	データを高次元に写像することで線形分離を可能にする
	→カーネルトリックと呼ばれる方法を使うことで
　		計算を楽にすることができる（次元が増えると計算が大変になる）
k近傍法（分類と回帰）
	分類に使われる手法の１つ
	特徴量のスケールを予め揃えておく必要がある
	→未知のデータが与えられたときに近くにあるデータからクラスを分類していく手法のこと
	→ｋには個数が入る
	距離の算出には、一般的にユークリッド距離(√x²+ｙ²)が使われる
	最近傍法
		ｋ＝１のこと。予測する観測は最も近いデータの値が出力
		複雑で入り組んだものになる
		ｋが大きくなるほどたくさんの観測の平均や多数決が行われるので単純で明快なものになる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★16. 教師あり学習（ニューラルネットワーク・時系列分析）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ニューラルネットワーク
	人間の神経回路（ニューロン）を模倣した数理モデル
単純パーセプトロン
　	1958年に提案されたニューラルネットワーク
	→入力層と出力層からなる
	入力値の重要度を数値化したものを重みという
	モデルの自由度を上げるためにバイアス(b)を使用する
	活性化関数 次の層に渡す値を調整する関数をという
		シグモイド関数（結果2択）
		ソフトマックス関数（結果が3択以上）
		ReLU関数(ランプ関数)　　など
多層パーセプトロン
	→非線形分類が可能になった
	→入力層、隠れ層、出力層に分かれている
	層が増えることで重みも増える
	→誤差逆伝播法を活用して重みを調整
	誤差逆伝播法
	  出力層から入力層にかけて重みを調整する方法のこと
自己回帰モデル（ARモデル）
	回帰問題に使われる手法（時系列系のデータ）
	→過去のデータを使ってある時点の値を予測するモデル
ベクトル自己回帰モデル（ＶＡＲモデル）
	自己回帰モデルを多変量に拡張したもの
勾配ブースティング
	多数の弱いモデルで強いモデルを作るアンサンブル学習であるブースティングの１つ
	簡単に言うと学習器にあまり高性能なものを使わずに、弱分類器という感じで、予測値の誤差を新しく作った弱学習器がどんどん引き継いでいきながら誤差を小さくしていく方法です
	中でも弱いモデルに決定木を利用したものは、テーブルデータを対象とした回帰問題や分類問題で高い精度を示す（勾配ブースティング木、人気）
勾配ブースティング木（GBDT）のライブラリ
	xgboost DMLC社
	lightgbm Microsoft社
	catboost ロシアの検索エンジンを運営するYandex社製
畳み込みニューラルネットワーク
	特に画像領域で用いられる
汎化能力
	未知のデータに対する対応能力
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
17. 教師なし学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

クラスタリング（クラスタ分析）
	データをグループ（クラスタ）分けする手法のこと
	ある基準を設定してデータ間の類似性を計算し、データをクラスタ（グループ）にまとめるタスク
	→大量のデータをコンピュータが自動的に分類してくれる
	グループ分けされたものは人間が見て解釈する必要がある
	Ｘクラスタリングは、ある基準を設定してデータ間の類似性を計算し、データをクラス分類するタスクであり、教師あり学習の典型的なタスクである。
	Ｘクラスタリングにおいては、得られたクラスタの妥当性に絶対的な答えがある。
	〇クラスタリングは、ある基準を設定してデータ間の類似性を計算し、データをクラスタ（グループ）にまとめるタスクであり、教師なし学習の典型的なタスクである。
	・K-means法
	・ウォード法
	・主成分分析
	・ｔ-ＳＮＥ法
	・協調フィルタリング

非階層クラスタリング
	あらかじめクラスタ数を決めなければならない手法
K-means法（階層なしクラスタリング）(k平均法)
	あらかじめ設定したクラスタの数（k個）にデータを分類方法
	→重心を活用して分類を行っていく方法
	線形分離可能なデータしか分けることができません
	〇k-meansは、【最初にランダムにクラスタ中心を割り当て】、クラスタ中心を各データとの距離を計算しながら修正し
	  、最終的なクラスタ中心が収束するまで再計算を行いクラスタリングする手法である。
	〇k-meansは、scikit-learnのclusterモジュールのKMeansクラスを用いることによって実行することができる。
	〇クラスタリングにおいては、得られたクラスタの妥当性に絶対的な答えはない。

		from sklearn.cluster import KMeans
		# クラスタの数を3とするKMeansのインスタンスを生成
		km = KMeans(n_clusters=3, init='random', n_init=10, random_state=123)　　　#n_initはk-meansの実行回数★★
		# KMeansを実行
		y_km = km.fit_predict(X)

階層クラスタリング
	〇凝集型の階層的クラスタリングは、まず似ているデータをまとめて小さなクラスタを作り、次にそのクラスタと似ているデータをまとめ、
	　最終的にデータが1つのクラスタにまとめられるまで処理を繰り返すクラスタリング手法である。
	Ｘ凝集型の階層的クラスタリングは、最初にすべてのデータが一つのクラスタに所属していると考え、順次クラスタを分割していく手法である。
	〇分割型の階層的クラスタリングは、【最初にすべてのデータが1つのクラスタに所属している】と考え、順次クラスタを分割していくアプローチであり、
	凝集型より利用頻度が多いとは言えません。
	分割型　似ていないデータ同志を分割していく手法であり計算量が多いため、凝集型より利用頻度が多いとは言えません
	Ｘ分割型の階層的クラスタリングは、最初にすべてのデータが1つのクラスタに所属していると考え、順次クラスタを分割していくアプローチであり、
　　　一般的に凝集型に比べて計算量が少なく精度が高いためよく用いられる。

		from sklearn.cluster import AgglomerativeClustering
		# 擬集型の階層クラスタリングのインスタンスを作成
		ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')
		# クラスタリングを実行し、各クラスのクラスタ番号を取得
		labels = ac.fit_predict(X)
		labels

ウォード法（階層ありクラスタリング）
	似ている組み合わせから順番にクラスター化していく方法
	途中経過を樹形図（テンドログラム）で表すことができる
デンドログラム（階層ありクラスタリング）
	ある距離を閾値として定めることでクラスタにする。
主成分分析（PCA）
　	データの特徴量の関係性（相関関係）から変数をまとめてより少ない変数を要約すること
	多変数の空間の中から、分散の大きい順に、新たな軸を第一主成分、第二主成分、・・・と見つけていく手法。
	→データを要約することを次元削減といい、計算量を減らせる
	→まとめられた特徴量を主成分と呼ぶ
	カイザー基準　使用する主成分の数を決定するための基準
				固有値が１以上である因子を採用する

	英語、理科、数学、社会、国語のテストを実行
	→それぞれの科目を分析して２つの成分にする場合、理系と文系という風に分けることができる
	高次元のデータに対して分散が大きくなる方向（データが散らばっている方向）を探して、元の次元と同じかそれよりも低い次元にデータを変換する手法
	分散・ばらつきが大きくなる方向を探して、元の次元と同じか、低い次元にデータを変換・圧縮する、次元削減の一手法
	主成分分析は、scikit-learnのdecompositonモジュールのPCAクラスを用いて実行することができる。
	2次元のデータに対して主成分分析を行い、どちらも重要であると確認できた場合には、1次元に次元削減できる可能性が高いとはいえません。
	Ｘ主成分分析とは、高次元のデータに対して標準偏差が小さくなる方向を探して、元の次元と同じかそれよりも高い次元にデータを変換する手法である。

次元削減の方法
	主成分分析以外にもｔ分布を使って次元削減する
	t-SNE法、特異値分解（SVD）、多次元尺度構成法（MDS）が有名
	低次元（小）-＞高次元（多）
　　「圧縮」するタスク
　　　計算量の削減は、（目的変数ではなく）説明変数の次元数を減らすこと
	Ｘ次元削減は、データが持っている情報をなるべく損ねることなく次元を削減してデータを展開(〇圧縮)するタスクである。
	Ｘ次元削減の主目的は目的変数の数を減らして計算量を削減することであるが、説明変数の削減を行うことは少ない。これは、モデルの精度を確保するためである。
	特徴量の次元削減、なるべくデータの情報を落とさずに、少ない特徴量でデータを表現するために用いられる
	不動産分析で、延床面積、敷地面積、間口を広さとする
協調フィルタリング
	レコメンデーションで使われる手法の１つ
	→AMAzonなどECサイトでおススメが表示される仕組みのこと
	→自分と類にしたユーザーが購入した商品を薦める
	　動画配信ならば類にしたユーザーが見た動画を薦める
	→ある程度のデータが必要になる（コールドスタート問題）
	・コンテンツベースフィルタリング
	　	商品の特徴を利用して類にした商品を薦める方法
		→コールドスタート問題を回避することができる
		→それぞれにメリットとデメリットが存在する
コールドスタート問題
	新商品、行動履歴がなくてレコメンドできない問題をどうする
	「新登場のアイテムは行動履歴がある程度付くまでレコメンドの対象にならない」というもの
トピックモデル
	クラスタリングで使われるモデルの１つ
	→K-means法などと異なり複数のクラスタに分類が可能
	　潜在的ディリクレ分配法（LDA）が有名
	→ニュース記事を複数のカテゴリに分類したりレコメンダシステムなどで活用したりできる
	・ソフトクラスタリング
	　	各データが１以上のクラスタに所属するようなクラスタリング
	・ハードクラスタリング
	　	各データが１つのクラスタに所属するようなクラスタリング
半教師あり学習
　	学習の途中まで教師あり学習で、その後、教師なし学習を行う学習
	→少量のラベル付きデータを学習し、
　		ラベルがついていない大量のデータを学習するアプローチ
	→大量のラベル付きデータを集めるのはコストがかかりすぎる
		ラベル：データに付与される正解かどうかを表す情報
オートエンコーダ
	ニューラルネットワークを用いた次元削減で、入力層と出力層の次元を同じにし、入力を再現するように出力を行わせる
	より低次元の中間層では、できるだけ情報を失わないような（出力で復元できるような）次元圧縮が行われている
敵対的生成ネットワーク(GAN)
	主に画像生成するためのネットワーク
	GANはデータから特徴を学習することで、実在しないデータを生成したり、存在するデータの特徴に沿って変換できます。
	GANのネットワーク構造は、Generator（生成ネットワーク）とDiscriminator（識別ネットワーク）の２つのネットワークから構成されており、互いに競い合わせることで精度を高めていきます。
	例えるならば、「偽物を作り出す悪人（Generator）」と「本物かどうか見破る鑑定士（Discriminator）」のような役割をネットワーク内に組み込み、競争させるような形で学習させます。（そのため「敵対的」と言われます）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★★18. 強化学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
バンディットアルゴリズム
強化学習
	コンピュータ（エージェント）が試行錯誤しながら最適な行動を学習する手法のこと
	→状態、行動、報酬が大切なキーワードになる

｛エージェント｝　←　状態　　　｛環境｝
　　　　　　　　行動　→
　　　　　　　　←　報酬
	→短期的な報酬だけで行動を決定しているわけではない
	　行動に対して毎回報酬がなくても学習可能（囲碁や将棋など）
	・探索と活用のジレンマ
　		探索：知っていること以外の行動を行う
　		活用：既存情報を使って最適な行動を行うこと
		→活用だけだと、良い行動があるかもしれないが発見できない
　		　探索だけだと、必要のいない行動が多くなり時間がかかる

	・ε-greedy方策：一定確率で探索を選ぶ
	・UCB方策：探索では今まで選択されていない行動を選択

マルコフ決定過程（MDP）モデル
	次に起こる事象の確率が、【これまでの過程】と関係なく、【現在の状態】によってのみ決定される確率過程のこと
	マルコフ性
		以前の状態に依存しない性質のことを「マルコフ性」といいます
		→本来ならば過去の状態を考慮して行動を選択するが計算が複雑になってしまう
方策
　	最適な方策を見つけ出すことは難しい
	1.	状態や行動の価値を計算して最適な行動を選択（価値ベース）
	2.	方策をパラメータのもった関数と定義して方策の価値が最大になるようにパラメータを学習させる（方策ベース）
価値関数
	将来的に見込める報酬を求める関数
	・行動価値関数
	・状態価値関数
	→未来の報酬はもらえるのかどうかわからないので割り引いて考える
	（割引率：0～1）
行動価値関数
　	ある状態において行動した結果、将来的に得られる報酬の合計を返す関数
状態価値関数
　	ある状態において、将来的に得られる報酬の合計を返す関数
Q値
　	行動価値関数から返ってきた値のこと
Q学習
　	Q値が最も高くなるように行動をするように学習するとこと。
方策勾配
　	方策勾配法
　　	方策をパラメータのもった関数と定義して
　　	方策の価値が最大になるようにパラメータを学習させていくことで最適な方策を見つけるようにする方法
　	ＲＥＩＮＦＯＲＣＥ
　　アルファ碁にも活用されているアルゴリズム
	→方策ベースの手法
	・Actor-Critic
　　	行動（ACtor）と評価するCriticで構成されている
	→Criticが行動を評価して方策を更新していく
	→価値ベースと方策ベースの手法
	→Actor-Criticを応用したA3Cがある
深層強化学習
状態行動空間の爆発　強化学習の課題で、状態と行動の組に対して定義される値を保存するための
				領域が極端に必要になってしまう
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★19. モデルの評価手法
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
→データを訓練データ・テストデータに分割して検証することを交差検証と呼ぶ
交差検証（クロスバリデーション）の種類
	・ホールドアウト検証
	・ｋ-分割交差検証
	・１つ抜き（Leave　One　Out）法
ホールドアウト検証（Hold-out法）
	準備したデータを訓練データとテストデータに分ける
	→テスト用データを活用してモデルの性能を評価
	→データが少ない場合、評価が高くなってしまうことがある
	弱点
		・学習用データが減る
		・パラメータの変更などを何度も行うことより、評価用データに過学習する恐れがある
		・データの分割の仕方によって、結果が変わる
ｋ-分割交差検証
	準備したデータを複数個に分割し、それぞれのデータを使い学習と評価を行う方法
	→データ数が少ない場合にたまたま評価が高くなってしまう可能性を防ぐために使われる検証方法になる
	9割学習用データ、1割テストデータにする処理を10回行う＝10分割交差検証

		from sklearn.svm import SVC
		from sklearn.model_selection import cross_val_score
		# サポートベクタマシンをインスタンス化
		svc = SVC()
		# 10分割の交差検証を実行
		cross_val_score(svc, X, y, cv=10, scoring='precision')　　適合率（precision）　

		＊引数cvに、10分割の交差検証を指定

１つ抜き（Leave　One　Out）法
	データ全体のうち１つをテストデータにする方法
	→訓練データとテストデータを入れ替えてテストを行う
	→データ数分繰り返し行う（ジャックナイフ法とも呼ばれる）
データリーケージ
	未知のデータが入ってしまうこと
	訓練データ・検証データ、テストデータにすべき
	→時系列データをもとにモデルを作るときは
　	未来のデータが入るのはよくない（過去のデータの学習）
二乗平均平方根誤差（RMSE）
	回帰問題の評価指標。パイ　1/n　Σ（正解ー予測）2
	大きな誤差を許容したくない場合に使う
平均二乗誤差（MSE）
	平均二乗誤差の性質として外れ値に対して敏感であることが挙げられます。
	外れ値を含むデータに平均二乗誤差を用いてモデルを構築すると、予測結果が不安定になります。
	1/n　Σ（正解ー予測）2
平均絶対誤差（MAE）
	外れ値に強い
二乗平均平方根対数誤差（RMSLE）
	目的変数と予測値の比に着目する経済指標。対数化した指標
交差エントロピー誤差（cee）
	分類問題に用います。
決定係数
	回帰問題によって求められた予測値や当てはめの値が、正解ラベルとどの程度一致しているかを表す指標
	正解ラベルと予測値の相関係数を二乗した決定係数は、ランダムな予測より良いモデルに対して0〜1の範囲をとる
	また、その数値は正解ラベルの変動のうち、モデルによって説明できる変動の割合
logloss
	分類問題の代表的な評価指標
	高い確信度をもった予測が誤ったときにペナルティが大きくなる経済指標
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
☆20.分類モデルの評価
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
→正解・不正解をまとめた表を混同行列という

画像を犬か、それ以外に分けるテストを行った
１．「犬」の画像を、正しく「犬」と判断（真陽性）TP
２．「犬」の画像を、間違って「犬以外」と判断（偽陰性）FN
３．「犬以外」の画像を、間違って「犬」と判断（偽陽性）FP
４．「犬以外」の画像を、正しく「犬以外」と判断（真陰性）TN
                    　　　　　　 　予測のクラス
			　　　　		犬と判断　　　　犬以外と判断
犬の画像（Positive）　　　1.真陽性　TP　1o　　　2.偽陰性　FN 0x
犬以外の画像（Negative）　3.偽陽性　FP　1o　　　4.真陰性　TN 0x
  本当のクラス

正解率（accuracy）＝(TP+TN) / (all)
	全データのうち正しく予測できた割合
陽性判定　正例を予測する
適合率=TP / (TP+FP)
	陽性と予測したデータのうち、正しい予測の割合
	陽性判定のうち、実際に疾患を有する人の割合
再現率=TP / (TP + FN)
	陽性データの中で、正しい予測ができた割合
	疾患を有する人のうち正しく陽性判定できた割合
F値= (２ｘ適合率ｘ再現率) / (適合率　＋　再現率)
	適合率と再現率の調和平均
評価したい項目に合わせて評価指標を選ぶことが大切
→値が高いからといって性能が高いわけではない
→不良品率0.1％を判別するモデルを作り、正解率99.9％だった
	すべてを正しい製品だと判別し、不良品を見つけていなかった

	混合行列　予測と実績のクラスレベルの組合せを集計した表
	＊混同行列で計算する適合率、再現率、F値、正解率は、（回帰ではなく）分類モデルの評価指標★★★★
	＊適合率は予想するクラスをなるべく間違えないようにしたいときに重視する指標である。★★★★★★★★★★★★
	＊正解率は、正例か負例かを問わず、予測と実績が一致したデータの割合を表す。正解率は(tp+tn)/(tp+fp+fn+tn）で計算することができる。★★★★★★★★★★★★
	＊F値は、適合率と再現率の【調和平均】として定義される。F値は2*適合率*再現率/(適合率+再現率)で計算することができる。★★★★
	Ｘ★★機械学習を用いて構築した【回帰モデルの良し悪し】を評価する指標に適合率、再現率、F値、正解率がある。これらは【混同行列】から計算する。
	〇適合率、再現率、F値、正解率は、機械学習を用いて構築した【分類モデルの良し悪し】を評価する指標であり、混同行列から計算する。
	〇一般的に適合率と再現率はトレードオフの関係にある。つまり、どちらか一方の指標を高くすると、もう一方の指標は低くなる。
	ｘ適合率は、たとえば病院の検診で病気の見逃し・取りこぼしがないようにしたい場合などに重視される、網羅性に関する指標である。
	ｘ再現率は、間違えることをできるだけ避けたい場合に重視する指標である。一般的に適合率と再現率はトレードオフの関係にある。

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★21. 過学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
過学習（overfitting）
	過学習とは、特定のデータへの対応にのみ長けてしまうことです。
	これはモデルが訓練データに対して過剰に学習したため、はずれ値やノイズまで学習してしまったと考える
	訓練に使ったデータを完全に記憶してしまうと、処理の練習になりません。結果的に、未知のデータにはまったく対処できない状態になります。
	過学習が起きる原因としてデータ数が少ない、変数が多い、パラメータが大きすぎるといったことがあります。
汎化性能
　	未知のデータに対する精度のこと（精度は汎化誤差で測る）
	測定はホールドアウト法、交差検証法
訓練誤差：訓練データに対する予測と正解の誤差
汎化誤差：未知データに対する予測と正解の誤差
過学習を防ぐテクニック
	・アンサンブル学習
	・ドロップアウト・早期終了・バッチ正規化
	→ディープラーニングの章で詳しく解説

	・データ拡張
　　	データのバリエーションを増やす（データの水増し）
	・スパース化
　　	説明変数（特徴量）を減らす
	・正則化
	など
正則化
　	過学習を防ぐために、パラメータに制限をかけること
	学習の目的関数にペナルティとなる項目を追加することで、パラメータが極端な値になることを防ぐ
	→正則化しすぎると未知のデータに対する精度が低下する
　	未学習（学習不足）になってしまうので注意が必要
	→正則化にはL1正則化、L2正則化などがある
	L1正則化／ラッソ回帰（Lasso）
		余分な説明変数を省くことを目的とした手法
　　	特定のパラメータの値を０にして、選択
		正則化パラメータを大きくするに従い、各回帰係数が０になるものが増える
	L2正則化／リッジ回帰（Ridge）
		モデルの過学習を防ぐことで精度を高めるために用いられます。
　		パラメータの大きさに応じて０に近づけて滑らかなモデルにする
	Elasti Net
　　	ラッソ回帰とリッジ回帰を組み合わせたもの
	スパースデータ　要素の０が多くなるデータのこと
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★22. ROC曲線とモデル解釈・選択
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ROC曲線
	真陽性率（TPR）と偽陽性率（FPR）の関係を表した曲線
	・真陽性率（TPR）＝TP　/　（TP＋FP）
	・偽陽性率（FPR）＝FP　/　（FP＋TN）
	→横軸をFPR、縦軸をTPR
ROC曲線とAUC
	Positiveと判断　　Negativeと判断
	Positive　　TP　　　　　　　　　FN
	Negative　　FP　　　　　　　　TN
AUC
　	ROC曲線よりも下の面積のこと
	→面積が１に近いほどモデルの性能は高いとされる
モデルの解釈
　	モデルが何かを予測したとき、どのような根拠を持って予測したのかを知ることも大切になる
	→モデルが複雑になればなるほど根拠はわかりにくくなる
	→モデルの局所的な解釈を可能にするアプローチがある
　	LIME、　SHAPなどの手法が考えられている
LIME
	複雑なモデルを解釈しやすいモデル（線形回帰）に近似させる
SHAP
　　協力ゲーム理論におけるShapley値を利用してそれぞれの特徴量が予測にどの程度を与えたかを算出
モデルの選択と情報量
	タスクに対して最適なモデルを選択していく
	→複雑なモデルを選択すれば複雑なタスクを実行できる可能性は上がるがコストも増大してしまう
	→「ある事柄を説明するためには、必要以上に多くを仮定するべきではない」（オッカムの剃刀）

・赤池情報量基準（AIC）
　　「モデルの複雑さと、データとの適合度とのバランス」を知るときに使われる指標の１つ
	→モデルをどの程度複雑にすればいいのか目安になる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★23. ニューラルネットワークとディープラーニング
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ニューロン

パーセプトロン
	パーセプトロンは入力された信号（複数あり）を処理して一つの値を出力します
	w0+w1x1+w2x2≥0　->　1　w0はバイアス、w1,w2は重み
単純パーセプトロン
　	1958年提案されたニューラルネットワーク
　	活性化関数としてステップ関数が用いられる
	例えば、身長、体重といったデータを入力として受け取り、男性であるか女性であるかを判定する分類器を作ることができます。
		1を出力したときは男性、0を出力したときは女性
	一般の問題ではデータが線形分離可能であることはまずありませんので，パーセプトロンは残念ながら実用には向いていません。
多層パーセプトロン
　	多層化したニューラルネットワークのこと
　	入力層→隠れ層→出力層
	→非線形分類が可能になった
　	単純パーセプトロンは線形分類だけ
	→入力層、隠れ層、出力層に分かれている
　	隠れ層は中間層とも呼ばれている
	多層化する場合に新たに問題になるのは、予測値と実際の値の差分である誤差を最小化する、いわゆる最適化問題が複雑化することです。
ディープラーニング
	大規模なラベル付されたデータとニューラルネットワークの構造を利用して学習する
　	隠れ層を増やしたニューラルネットワーク（深層）を活用
　	層を深くすることで解ける問題が増えると考えたが、思ったような結果が得られなかった
	→信用割当問題・勾配消失問題など様々な問題を抱えていたため
	→問題が解決されたことで解ける問題の幅は広がった
信用割当問題
	最終的な結果に対して、どのパラメーターに責任があるのか、
	どのパラメーターを修正すればいいのかという問題
	→誤差があったとき何を修正すればいいのかという問題
	→誤差逆伝播法により信用割当問題は解決されたとされている
勾配消失問題
	ニューラルネットワークを多層化すると、誤差逆伝播法においてそれぞれの層で活性化関数の微分がかかることから、
	勾配が消失しやすくなり、学習が進まなくなる問題
	・誤差逆伝播法
　　　出力された結果と実際の結果の誤差を最小化するために出力層から入力層にかけて調整を行っていく方法

	→パラメーターを修正することで正しい値を高確率で導き出せるように
　		層が増えることで、入力層付近の調整が上手くいかない
	→誤差逆伝播法でパラメータを変更するとき活性化関数を微分した値を使用する
	→シグモイド関数の特性が影響している（0～1）
	→シグモイド関数の微分した値の範囲は（0～0.25）

	層が増えることで修正する値が小さくなってしまう
	→入力層付近では実質的に修正ができない状態になってしまう
	→なぜ微分を使っているのかにちては割愛
　	数学が得意な人（大学レベル）に説明しても誤差逆伝播だけで1時間から2時間は必要になってしまう上にテストで出ない

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★24. ディープラーニングのアプローチ
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
事前学習
	ニューラルネットワークの初期値に、オートエンコーダで学習させたものを用います。事前学習を行うことで、勾配消失による学習速度低下を防止します。
	多層化したニューラルネットワークはパラメータを効果的に更新することができないことから入気がなかった
	→ジェフリー・ヒントンが問題を解決する手法を提唱した
	→オートエンコーダ（自己符号化器）と呼ばれる方法になる事前に層ごとに学習を進める手法を事前学習という
オートエンコーダ
　	可視層と隠れ層の2層から構成されるネットワーク
	オートエンコーダでは、重要度の高い情報を洗い出し、それ以外の部分を削ぎ落します。
	たとえば、体格を表すのに、身長と体重の関係を二次元グラフ上のプロットで示したり、プロット結果を近似した一本の直線（一次元）で表したりすることがあります。
	→可視層は入力層と出力層を合わせたもの
	→データが可視層（入力層）から隠れ層、隠れ層から可視層（出力層）へ伝わっていく
	→可視層よりも隠れ層の次元は少ない
		入力と出力される情報が同じになるネットワーク
	→入力された情報が隠れ層で圧縮され、出力されるときに入力時と同じ情報になる
　		簡単に言えば、情報量を減らすこと（特徴をつかむ）
　		圧縮したデータはある程度もとに戻すことができる
	→入力層から隠れ層への処理：エンコード
　		隠れ層から出力層への処理：デコード
積層オートエンコーダ
	オートエンコードを何層も重ね合わせたもの
	→一気に学習させるのではなく、入力層から近い層から順番に学習していく方法になる
	→順番に学習していくことを事前学習という
		オートエンコーダを積み重ねても出力と入力は同じ値になるようにしているため教師あり学習にはならない
	→教師なし学習の状態になっている
	→オートエンコーダを積み重ねた最後に予測を行うための層を追加することで教師あり学習ができるようになる
	分類問題では、出力は入力データがどのようなクラスに属するのかを表す確率（0〜1の範囲）となる
	分類問題を解く場合、二項分類はシグモイド関数、多項分類の場合、ソフトマックス関数の出力層が追加される
	回帰問題の場合は出力層に線形回帰層が用いられる
	このように新たに出力層を追加した場合、出力層の重みを調整するためにネットワーク全体を学習して調整するファインチューニングが必要になる
ファインチューニング
	ロジスティック層を最後に追加する
	→シグモイド関数・ソフトマックス関数による出力層
	→回帰問題ならば、線形回帰層を最後に追加する
	→教師あり学習が可能になる
		オートエンコーダを積み重ねた最後に予測を行うための層を加えモデル全体で重みについて再学習させること（重みの調整）
	→最後の仕上げのことをファインチューニングと呼ぶ
　		出力層あたりの重みを調整するため誤差伝播法が有効
	→学習済みのモデルの学習率は低く設定しておく
深層信念ネットワーク
　	オートエンコーダの代わりに制限付きボルツマンマシンを積み重ねたもの
ボルツマンマシン
　　可視層・隠れ層の各ユニットが全て互いに結合されているネットワーク
　	深層信念ネットワークは積層オートエンコーダと同様にジェフリー・ヒントンによって提唱された
	→現在は事前学習は計算量が膨大になるためネットワーク全体を一気に学習させるようになった
次元の呪い　扱うデータの次元が高くなるほど、計算量が指数関数的に増えていってしまうこと

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
25. ディープラーニングを実現するには
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ハードウエア
	人工知能の理論などは数多く存在したが、計算コストが高いことから実施できないことが多かった
	→半導体の性能が向上したことで計算コストは低くなった
ムーアの法則（ゴードン・ムーアが提唱）
	半導体の集積密度は18か月で2倍になるという経験則

CPUとGPU
　	CPU（Central　Processing　Unit）
　	パソコンの頭脳と言われており、汎用的な演算処理を行う装置
	複雑な命令を逐次計算
　	GPU（Graphics　 Processing　Unit）
　	画像処理が得意な装置（コア数が多い）
	→並列的な演算処理がCPUよりも得意

GPGPU
　	ディープラーニングでは同じような計算が大量になされている
	→GPUは並列的に処理をするのが得意であるため、ディープラーニングにとって相性が良いと考えられるが、GPUは画像処理に特化していたため改良する必要があった
	→GPUを画像処理以外の演算処理に応用するための技術が必要
	General-Purpose　Computing　on　Graptics　Processing　Unitｓ
	→NVIDIA社などがディープラーニング向けのGPUを開発している
　		汎用並列コンピューティングプラットフォーム（CUDA）を提供
　
TPU（Tensor　Processing　Unit）
　	Googleが開発した機械学習に特化した演算処理装置

ディープラーニングのデータ量
　	学習に必要なデータ量は明確に決まっていない
	→経験則としてモデルのパラメータ数の10倍は必要と言われている
　（バーニーおじさんのルール）
	→パラメータ数が1万ならば、データ量は10万が必要

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
26. 活性化関数
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
活性化関数
	モデルの表現力を増すために用いられる。活性化関数を用いて変換を行うと様々な値の出力が行えます。
	その結果として高度な識別問題を解決できるようになる
	流行り　ステップ関数→シグモイド関数→ReLU関数
	出力層で用いられる活性化関数は出力を確率で表現するために、特定の活性化関数が用いられる
	ReLU関数は出力を確率として表現できない、分類問題の出力層に用いられない
	ステップ関数(階段関数)
		ステップ関数は1と0しか出力できません。そのため、情報のロスが激しく表現力に乏しいのが欠点です。
		y′≥0 => 1
		y′<0 => 0
	シグモイド関数（Sigmoid）
　		2値分類で活用される（迷惑メールかどうか　など）。非線形な関数。
		座標点（0, 0.5）を基点（変曲点）として点対称となるS字型の滑らかな曲線で、「0」～「1」の間の値を返す
		微分できないステップ関数の代わりに、微分可能（differentiable）な「シグモイド関数」が活性化関数として採用された
		シグモイド関数はニューラルネットワークで用いられる活性化関数ですが現在はあまり使われていません。
		シグモイド関数を活性化関数として用いると勾配消失という現象が起こり、学習が停滞してしまうことがあるためです。
	ソフトマックス関数
	　	多クラス分類のときに使われる関数
		→モデルの出力の総和は１になる(0.5+0.3+0.2)
	恒等関数
	　回帰問題のときに使われる関数
	Tanh関数（ハイパボリックタンジェント関数）
　		誤差逆伝播法では活性化関数の微分を活用する
		→シグモイド関数の微分の範囲は0～0.25と値が小さいため層が深くなってしまうと学習が進まなくなってしまう
		　微分係数をかけるたびに伝播していく誤差の値は小さくなってしまう。その結果、入力に近いほど伝承すべき誤差がなくなる（勾配消失問題）
		→シグモイド関数以外の微分が可能な関数を使用すれば問題が解決されるのではないかと考えられた。
			-1から1の範囲を取る関数
		→【微分の最大値は1】であるため勾配が消失しにくい。あくまで最大値が大きいだけで根本の解決でない
		→隠れ層の活性化関数をシグモイド関数からtanh関数に変更することで勾配消失問題をマシにできる
		→シグモイド関数を【線形変形】したもの
	ReLU関数（Rectified　Linear　Unit関数）（ランプ関数）
		隠れ層の活性化関数
		Tanh関数はシグモイド関数よりも勾配消失が起きにくいが微分した値は１以下なので勾配は小さくなっていく
		→ReLU関数を活用することで勾配が消失しにくくなる
		→０以下の時は０，０以上のときは入力値をそのまま出力
		微分した値は、入力が0よりも小さい場合は0。入力が0以上の場合は1になる
		→従来の活性化関数よりも精度向上に貢献した
		y > 0 =>  y
		y <= 0 => 0
	Leaky　ReLU関数
　		ReLU関数を改良した関数の１つ
		→入力値が０より小さい場合、入力値をα倍した値を出力、０以上の場合には入力値と同じ値を出力する関数

		微分した値が０にならないため勾配消失が起きにくい
		→他にReLU関数から派生した関数も存在する
　		どれを使うかはケースバイケース

		・Parametric　ReLU：αを学習によって決める
		・Randomized　ReLU：αを範囲内でランダムに変化させる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★27. 学習の最適化（勾配降下法）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
最適化
	「関数の最小点を見つける」こと
勾配降下法
	関数の勾配にあたる微分係数に沿って降りていくことで、最小値を求める手法のこと。
	xから微分係数を引く
	xが収束する可能性がある値　大域最適解、局所最適解、停留点
	ニューラルネットワークでは、予測値と実際の値の誤差を近くするように学習をしていく
	→重みやバイアスなどのパラメータを最適化していくこと
	→誤差を誤差関数（損失関数）によって求め、誤差を最小化する
	　平均二乗誤差、交差エントロピー誤差などがよく使われる
	関数の最小の値を見つけるときに使われるのが微分
	誤差関数には多数の変数（パラメータ）が含まれているため微分ではなく、偏微分である（微分した値が勾配）
	→変数が増えると計算の難易度が一気に上がる
	→勾配が最小になる変数を見つけ出すアルゴリズムの１つが勾配降下法
	イテレーション
		パラメータを更新する回数のこと
		→学習してパラメータを更新したらイテレーション数が１
	学習率
　		パラメーターを調整する値のこと。歩くときの歩幅のようなもの。
		勾配に沿って一度にどれだけ降りていくかを設定するもの
		パラメーターz（更新後）＝パラメーターz（更新前）ー学習率＊パラメーターz（更新前）における勾配
		→学習率は大きすぎても小さすぎても良くない
		→学習率の設定は専門家が決めることが多い
		応用上ではより効率的な計算を行うため、学習の初めでは学習率を大きくし、学習が進むにつれて小さくするような工夫が行われる
	勾配降下法の種類
		１．	最急降下法
		２．	確率的勾配降下法（SGD）
バッチ学習
　	全てのデータの誤差をもとにモデルを学習
最急降下法
	学習データの全ての誤差合計からパラメータを更新する方法
	→学習データが多いと計算が多くなり時間がかかる
	→学習データが増えるたびに再計算する必要がある
	バッチ学習に相当する
確率的勾配降下法（SGD）
	ミニバッチ学習、オンライン学習に相当する
ミニバッチ学習
  （ランダムに抜き出す）データを小さなグループに分割し、グループの誤差をもとにモデルを学習
オンライン学習（逐一学習）
  （ランダムに抜き出す）１つずつのデータの誤差をもとにモデルを学習（くり返す）
	→時間がかかってしまう方法になる
モーメンタム
	SGDを改善。パラメータの更新に感性的な性質を持たせ、勾配の方向に減速・加速したり、摩擦抵抗によって減衰したりしていくようにパラメーターを更新
エポック
	全ての訓練データを学習させた回数のこと
誤差関数（損失関数）
	損失関数は予測と実際の値のズレの大きさを表す関数でモデルの予測精度を評価します。
	損失関数の値が小さければより正確なモデルと言えます。
	この損失関数の評価をもとにモデルのパラメータを算出します。ニューラルネットワークをはじめとした機械学習モデルは損失関数の値が最小となるようなパラメータを様々な方法で求めます。
	例）平均二乗誤差（MSE）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★28. 学習の最適化（問題点と対策）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
勾配降下法の問題と改善
	勾配降下法で勾配の最小値が見つからない場合も存在する
	・局所最適解
　		局所的な最小値
　		停留点の影響を受ける
	・大域最適解
　		本当の最小値
	局所最適解を防ぐ方法として、学習率の値を大きくする
	→小さな山を飛び越えることができるため、大域最適解を見つけられる可能性が高くなる
	→学習率の値が大きすぎると大域最適解を飛び越えることも
　	最適なタイミングで学習率を小さくする

	微分・偏微分を使用するため局所最適解以外にも停留点の影響で学習が進みにくくなることもある
	→停留点は局所最適解・大域最適解ではないが勾配が０になってしまうため
鞍点（あんてん）　
	パラメータが多くなると、ある次元では最小にも関わらず、別の次元から見ると最大になっている点のこと、
	３次元以上の関数に対して、勾配降下法を用いると、ある次元からみると極大点で、他の次元から見ると極小点となる点
	→鞍点の近くは平たんになっていることが多く、学習が進みにくくなってしまう
	→プラトーと呼ばれる現象
モーメンタム
	鞍点問題に対するために考えられた手法
	→慣性の考え方を適応し、学習の停滞を防ぐとされる
　		前回の更新量を、現在の更新量に反映させる
	→モーメンタムよりも効率的なアルゴリズムが考えられた

局所最適化を避ける最適化アルゴリズムの1例
　　Adam　：RMSpropを改良した手法
　　AdaBound：Adamを改良した手法
	AMSBound：AdaBoundと同様のアイデアをAMSGradに適用した手法
　	AdaGrad：SGDの改良した手法、勾配降下法で学習率を自動で更新
　	AdaDelta：AdaGradを改良した手法
	RMSprop：AdaGradを改良した手法

ハイパーパラメータ
	機械学習モデルにおいて人間があらかじめ設定するパラメータ
	→学習率、ニューラルネットワークの層の数　など
	→予測の精度に大きな影響を与える
　	人が経験で決めることが多いが、最適な設定ができるように自動化する方法が考えられている
	ランダムサーチやグリッドサーチにもとづいて実験を繰り返し学習率などを決定していく
	・ランダムサーチ
		指定された分布に従ってランダムにパラメータを抽出し学習を行って、最適なパラメータを探す方法
	・グリッドサーチ
		あらかじめパラメータの候補地を設定し、パラメータの組み合わせから学習を行って、最適なパラメータを探す方法
		→ランダムサーチよりも日値が関与できる幅が広い
		指定したパラメーターの全ての組み合わせを試す手法。組み合わせの総数分モデルの学習を行うので、探索が終わるのに時間がかかる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★ドロップアウト
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ドロップアウト
	→過学習を防止する方法としてドロップアウトがある
	→学習時に、ランダムでユニットを無効化する方法

	ランダムにユニットを無効化させて学習させていくためアンサンブル学習と同じような効果が期待できる

	→	ネットワーク全体で訓練データに最適化されすぎるのを抑えることができる方法になっている

早期終了
　	過学習が起きる前に学習を終了する方法（汎用性が高い）
	→汎化誤差が増加し始めたら学習をやめていく
	訓練誤差：訓練データに対する予測と正解の誤差
	汎化誤差：学習に使っていないデータに対する予測と正解の誤差
	・二重降下現象
　		汎化誤差が増えても、そのあと汎化誤差が減っていく現象
　		どのようなタイミングでやめるのかは難しい問題
	→早期終了は過学習を抑えるテクニックとしてかなり優秀
ノーフリーランチ定理
　	あらゆる問題を効率的に解く汎用的な方法はないということ
	→早期終了を「Beautiful FREE LUNCH」と表現している

データの正規化・重みの初期化
　	正規化
　		効率的に学習が行えるようにデータを調整する
		→各データの最大値や最小値が大きく異なってしまうとパラメータに偏りが生じてしまうとパラメータに偏りが生じてしまう
		→データのスケールを合わせていく作業になる（体重と身長）
		それぞれの特徴量を最大値で割って特徴量を0～1の値へ

　	標準化
		各特徴量の平均を０，分散が１になるように変換すること
		→正規化よりも高い効果があるとされている

	白色化
　		各特徴量を無相関化し、標準化すること
		→計算コストが大きくなるので標準化を使うケースが多い
		無相関：データ間の相関をなくすること
重みの初期値
	ディープニューラルネットワークについて
		データを正規化・標準化しても層を伝播していくうちにデータの分布が崩れてしまい良くない結果になってしまう
		→データの分布に影響が少ない重みの設定が大切
		→最適な重みはわからないのでランダムな値になってしまう
		データの分布が崩れにくい重みの初期値の設定方法が考えられるようになった

	シグモイド関数　：Xavierの初期値
	ReLU関数　：Heの初期値　が良いとされる
バッチ正規化
	隠れ層に入力する値を正規化する方法
	→過学習が起きにくくなるとされている
	→重みの初期値を工夫するよりも各層に伝わるデータを正規化しようと考え方
	→活性化関数の前にデータを正規化する

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★30. CNNの基本用語
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ニューラルネットワーク
	ニューラルネットワークは画像や時系列データなどそれぞれに特化したものが数多く存在する
	→画像データを扱う場合は
　		畳み込みニューラルネットワーク（CNN）がよく使われる
	→言語処理、音声処理などは他のニューラルネットワークが使用

ＣＮＮの基本形
	ＣＮＮ（畳み込みニューラルネットワーク）は画像処理の分野に特化したニューラルネットワークである
	→OCR、パターン認識など幅広い分野で使用されている
OCR（光学的文字認識）
　	手書きや印刷された文字を読み取りデジタル化する技術

パターン認識
　	画像や音声など様々なデータから一定のパターンを見つけ識別すること
	→音声認識、画像認識、文字認識　などで使われている
	→ＯＣＲも文字認識の１つになる
	画像は2次元データ（厳密には3次元データ）なので2次元データのまま処理をする方が望ましい
	→画像は上下左右、位置が重要な意味を持つため
		一列にしてしまうと画像の持つ意味を失ってしまう
	→画像は赤色、緑色、青色の３色の情報を持っている
	人間の視覚野に関する神経細胞をもとに作られたモデル

	→単純型細胞、複雑型細胞のはたらきをもとに、ネオコグニトロンと呼ばれるモデルが作られる（福島邦彦氏）

	単純型細胞（S細胞）：画像の特徴を抽出する
	複雑型細胞（C細胞）：物体の位置が変わっても同一の特徴とみなす

ネオコグニトロン
　	S細胞層とC細胞層を組み込んだモデル（初期のCNNモデル）
	1998年に、ニューヨーク大学のヤン・ルカンによって誤差逆伝播法を学習に用いた【LeNet】と呼ばれるCNNのモデルが発表される
	→入力層・畳み込み層・プーリング層（サブサンプリング層）・全結合層・出力層で処理

	ネオコグトロンと似た構造をしている
	・Ｓ細胞層：畳み込み層
	・Ｃ細胞層：プーリング層　に対応している
	→学習方法は少し異なる、ネオコグニトロンはadd-if silent
	ＣＮＮは誤差逆伝播法を用いる

	特定のユニット同士のみが結合している特徴
	→局所結合と呼ばれる
	→重みの共有がされておりパラメータ数が少なくて済む
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★31. 畳み込み層・プーリング層・全結合層
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
畳み込み層
	フィルタ（カーネル）を使って画像の特徴を抽出する層
	→畳み込み層と言われている
	→カーネルは基本的に画像サイズよりも小さいサイズのものを使う
		カーネルのサイズをカーネル幅と呼ぶ
	フィルタの値は学習することによって変化していく（重み）
	→全体の特徴を取り出すため、対象の位置がズレていても同じ対象として扱うことが可能になる
	→畳み込み層があることで位置のズレに強いモデルになる
	視覚野の局所受容野に対応している
	カーネルは1枚でなく、実際は複数用意する
	・ストライド
		カーネルをスライドさせる量のこと
	・パディング
		作成する特徴マップの大きさを調整するために、画像データを周りに値を入れること
プーリング層
	畳み込み層で出力した特徴マップをルールに従って小さくする
	→ダウンサンプリング、サブサンプリング　と言われる
	→プーリングの操作には
		マックスプーリング（最大値プーリング）と
		アベレージプーリング（平均値プーリング）という操作がある
	物体の位置が異なっていたとしても似た特徴量になるため位置のズレに強くなる
	→畳み込み層と異なり重みはないため学習しても変化はしない
	プーリング層では機械的に計算が行われている
全結合層
	抽出した特徴マップを一列に並べる処理を行う層
	→画像に対する答えを出力するために
		特徴を2次元から一次元に変更する必要があるため
	→シグモイド関数などを使って識別を行っていく
グローバルアベレージプーリング（GAP）
	最近では全結合層を使わず、特徴マップの平均値を使う処理がよく使われている
	→全結合層を使用するよりもパラメータの数が少なくて済み
	過学習が起きにくいとされている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★32. データの拡張と発展
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
データ拡張
	精度の高いモデルを作っていくためにはデータのバリエーションを豊富にする必要がある
	→物体の角度、光の当たり方など、集めることは難しい
	→画像から別の画像を作成していく方法がある（水増し）
	コントラストを変更、左右逆転させる　など
	Cutout
		画像の一部の値を０にする
	Random Erasing
		画像の一部をランダムな値にする
	Mixup
		2枚の画像を合成して新しい画像を作る
	CutMix
		CutoutとMixupを組み合わせたもの
	データのバリエーションを豊富にすることは大切だが現実に存在しないデータを生成しても意味がない
	→現実味のあるデータを生成して学習させることが大切
	→手書きの文字６を１８０度回転させるような変換は不適切
CNNの発展形
	画像認識では、畳み込み層とプーリング層を繰り返して精度を上げるアプローチがとられている
	→画像認識の精度を競うコンテスト（ILSVRC）でAlexNetが圧倒的な精度を出し、優勝した（トロント大学）
AlexNet
	畳み込み層→プーリング層→畳み込み層→プーリング層→畳み込み層→畳み込み層→畳み込み層→プーリング層→全結合層→全結合層→全結合層（出力層）
VGGNet
	ILSVRCで準優勝したモデル（16層、19層）
GoogLeNet
	ILSVRCで優勝したモデル（22層、Google社）
	→インセプションモジュールを採用している
インセプションモジュール
	複数のフィルターサイズで畳み込む処理を並列で行う処理
	層を増やすと誤差が逆伝播しにくくなっていった
ResNet
	ILSVRCで優勝したモデル（Microsoft）
	→スキップコネクションを加えることで層が深くなっても学習が上手くいくようになった
	様々な長さのネットワークが存在するため、アンサンブル学習になっている
MoblieNet
	層が増えることでパラメータの数が増えた計算量が増加
	→スマートフォンなどでも利用できるようにパラメータ数などを削減するために作られたモデル
	→Depthwise　Separable　Convolutionという手法が使われている
Neural　Architecture　Search（NAS）
	ネットワーク構造を自動的に最適化すること
	→カーネル、カーネルのサイズなど専門家が決めていたものを深層強化学習などを用いて最適な値を見つけ出す
	→人が最適なネットワーク構造を探し出すのは困難
	学習によってネットワーク構造を探し出す
NASNet
	ResNetのResidual　BlockをベースにしたNAS
MnasNet
	モバイル機器での計算量も考えられたNAS
EfficientNet
	Google社が発表したモデル
	→ネットワークの幅・深さ・解像度を１つの複合係数を使い少ないパラメータ数で精度の高い処理を行う
転移学習とファインチューニング
	・転移学習
		ある領域で学習させたモデルを他の領域で活用する方法
		→学習済みのモデルの後ろに新しいモデルを追加する　など
		→０からモデルを構築するのはコストがかかりすぎるため
		すでに学習済みのモデルを活用する方が効率的
		入力層に近い中間層では全体的な特徴を捉え
		出力層に近い中間層は個別の特徴を捉える
		→入力層に近い中間層ではタスクごとに大きな差は生まれにくいため転移学習が可能になる
	・ファインチューニング
		学習済みのモデルに新しいモデルを追加したとき、パラメータを調整するために学習すること
		→画像認識などの分野では学習済みのモデルが公開されている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★33. 深層生成モデル
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
生成モデルの考え方
	・識別モデル
		画像などを分類するモデルのこと
	・生成モデル
		画像など新しいデータを生成するモデルのこと
		→新しいデータを作り出すタスクを生成タスクと呼ぶ
深層生成モデル
	ディープラーニングを取り入れた生成モデル
	→ディープラーニング以前から考え方は存在したが、
	 複雑なデータを生成することは難しかった
	→変分オートエンコーダ（VAE）、
	 敵対的生成ネットワーク（GAN）が代表的なモデルになる
画像の生成は訓練データから画像が持っている潜在空間を学習し、ベクトルに格納
	・潜在空間
		画像の特徴量を分布させた空間のこと
変分オートエンコーダ（VAE）
	オートエンコーダを活用した深層生成モデル
	→統計分布に変換するのが特徴（平均と分散で表現）
	→入力データをエンコードが乱数などを使って潜在変数に変換、
		潜在変数をもとに新しいデータを生成する
敵対的生成ネットワーク（GAN）「教師なし学習」
	生成モデルであり、データの特徴を抽出して学習し、実在しないデータを生成できる。
	変分オートエンコーダやボルツマンマシンより鮮明な画像を生成
	ジェネレータとディスクリミネ－タで構成される生成モデル
	・ジェネレータ ：ランダムなベクトルを入力し、画像データを生成する
	・ディスクリミネータ：画像の真偽を予測
ジェネレータとディスクリミネ－タで競い合わせて、より精度の高い画像を生成する

・DCGAN 畳み込みニューラルネットワークを利用したモデル
・Pix2Pix
	ベクトルではなく、画像データを入力して条件にもとづいて別の画像に変換する
	→元の画像、変換した画像、実際の画像（正解とする画像）から真偽を判定する（多く画像が必要になる）
・Cycle GAN
	2組の画像を使い、一方の画像から他方の画像を生成し、他方の画像から一方の画像に戻した時に（サイクルした時に）精度が高くなるように学習させます。
	→変換した画像の真偽を判定する
	 元の画像と再変換した画像が一致するように学習
	→画像のペアが必要のない方法
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★★34. 画像認識分野①
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
画像認識分野
	入力した画像を識別する以外にも一般物体検出・画像セグメンテーション・姿勢推定などがある
物体認識タスク
　	入力した画像が何かを識別するタスクのこと
	→確信度が高い物体クラスを結果として出力する
	→犬の画像を入れた場合、
　		犬の確率80％、オオカミの確率10％のように確信度が出力される
	２０１２年ILSVRCでディープラーニングを活用したモデル（AlexNet）が圧倒的な成績を残し優勝した
		→コンテストではImageNetを使っている
		→ImageNetには数百万の画像データがあるが、全てのラベルが正しいわけではない、中には誤りのラベルも
ResNet（Microsoft）
　	人間の識別精度を超えた（限られた条件）
	→カーネル数を増やしたWide　ResNet、スキップコネクション（スキップ結合）を改良したDenseNetなどが有名なResNetの派生モデルである
SEＮet(Squeeze-and-Excitation Networks)
	畳み込み層が出力した特徴マップに重み付けする
　　Attention機構を導入したモデル
	→Attention機構は汎用的なアイディアであり、VGG,ResNetなど多くのモデルに導入が可能
物体検出タスク
　	画像に写っている物体を検出するタスクのこと
	→物体が写っている領域を囲み（矩形領域、四角形）
　	  物体クラスを識別するタスク
	→ディープラーニングにより、複数の物体クラスを同時に見つけることが可能になった
	・一般物体検出の代表的な手法
　		・R-CNN
　		・Fast　R-CNN
　		・Faster R-CNN
 　		・FPN
		・YOLO
		・SSD
R-CNN
　	セレクティブ・サーチと呼ばれる手法を活用し画像から物体候補領域を抽出する
	→候補領域の画像を一定の画像サイズにしてCNNに入力して特徴マップを出力
	→特徴マップをSVMによりクラスの識別を行う

Fast　R-CNN
	R-CNNより高速化されたモデル
	→R-CNNと異なり画像全体をCNNに入力して特徴マップを作成、特徴マップごとにクラスを識別
	→候補領域の数だけCNN処理するのは時間がかかっていた

Faster R-CNN
	Faster R-CNNでは候補領域を抽出する方法としてセレクティブ・サーチを利用している
	→Region　ProPosal　Network（CNN）に置き換えたモデル
	→処理がさらに高速化された

YOLO
	候補領域を見つけ出し、物体クラスを予測する方法だった
	→画像をCNNに入力するだけで、物体の認識と物体領域の切りだしを同時に行う

SSD
	YOLOと同じように物体の認識と物体領域の切りだしを同時に行う手法になる
	→特徴マップを上手いこと活用することで精度を高めている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
★★★★35. 画像認識分野②
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
セグメンテーションタスク
	画像を画素単位で複数の領域に分類・識別を行うタスクのこと
	・セマンティックセグメンテーション
		画像上のすべてを対象にする
		→同一クラスの物体は１つにまつめられる
		→猫が複数匹写っていても、個々の物体として区別されない
　			クラスごとに識別される

	・インスタンスセグメンテーション
		画像に写っている物体（検出した物体）を対象にする
		→同一クラスでも、別の物体として識別することができる
		→猫が複数匹写っていたら、個々の物体として区別できる

	・パノプティックセグメンテーション
		ビルや道路にはセマンティックセグメンテーション
		人などにはインスタンスセグメンテーション　を行う
セグメンテーションの代表的な手法
	・セマンティックセグメンテーション
		FCN、Segnet、U-net、PSPNet　など

	・インスタンスセグメンテーション
		Mask　R-CNN　など
FCN（完全畳み込みネットワーク）
	一般的なCNNは全結合層を用いるが、全結合層を畳み込み層に置き換えたネットワーク
	→畳み込み層により特徴マップは小さくなるため入力画像サイズまで拡大する処理が必要（アンサンプリンク）
セグネット（Segnet）
	全結合層を持たず、畳み込み層で構成されている
	→特徴マップを小さくするエンコードと大きくしていくデコーダが対照的に配置されている
U-net
	エンコーダとデコーダ構造のネットワーク
	→デコーダで特徴マップを拡大するとき対応するエンコーダで作成される特徴マップを利用する
PSPNet
	エンコードとデコードの間にPyramind　Pooling　Moduleが設けられたモデル
	→特徴マップを複数の解像度でプーリングする

DeepLad
	Atrous　convolution (Dilated convolution)を導入したモデル
	・Atrous　convolution
　	入力画像のピクセルの間隔をあけて畳み込み処理を行う
	→計算量、パラメータが少なく広い範囲の情報を集約できる
DeepLad　V3＋
　　ASPPを採用したモデル

ASPP
　　PSPNetのように複数解像度の特徴マップを使う手法

Mask　R-CNN
	物体検出とセグメンテーションを同時に行うことができる
	→物体検出はFaster　R-CNNを活用し、セグメンテーションはFCNを活用している
　	インスタンスセグメンテーションの手法になる
	→１つのモデルが複数のタスクをこなすことをマルチタスク
姿勢推定
	人間の頭や足などの関節位置を推定すること
	→監視カメラの映像から不審な行動を発見する　など
	→Open　Poseが代表的なモデルになる
Open　Pose
	複数の人間の関節位置を推定できる（口や鼻なども分かる）
	Parts　Affinity　Fieldsと呼ばれる処理がなされている
マルチタスク学習
	マルチタスク
　	複数のタスクを単一のモデルで課題を解決すること
	・Mask　R-CNN
　　物体検出とセグメンテーションを同時に行うことができる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
36. 音声データの扱い方
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
データの扱い方（時系列データ）
	時系列データ
	気温、株価の動き、人口動態など時間的に変化した情報を保有しているデータのこと
	→ある時点の情報は、その時点よりも前の情報に影響を受ける
　	データを時系列にそって学習させる必要がある
データの扱い方（音声データ）
	音声認識
　		人間が発したアナログなデータをデジタルデータを変換
		→アナログデータからデジタルデータに変換することを
　		　アナログデジタル変換（A-D変換）
		→一般的にパルス符号変調（PCM）でデジタルに変換
		　音声データから音声認識で必要な特徴量を取り出す
		→高速フーリエ変換（FFT）により
　		　音声信号を周波数スペクトルに変換する
		スペクトル：複雑な信号を成分に分解したもの
		音色は音の違いを認識する上で大切
		→同じ高さの音でも音色によって違う音と感じる（楽器）
		→音色の違いは周波数スペクトルのスペクトル包絡の違いと解釈
　		　スペクトル包絡を知ることが大切とされる

		・メル周波数ケプストラム係数（MFCC）
　			スペクトル包絡を求めるために使われる方法

		・メル尺度
　			人間が感じる高音の変化の尺度のこと
			→人間は高温になればなるほど音の高さの変化を感じにくい

		・フォルマント
　			周波数スペクトルに現れる周波数のピークのこと
			→その周波数をフォルマント周波数と呼ぶ
			→音韻（意味の区別を示す音）が同じならばフォルマント周波数は近くなる
		・音韻
			同じ音として識別するグループのこと
			→言語によって音韻は異なる
			→日本人は「lock」「rock」の違いが判らない
			同じ「ロック」として認識してしまう（音韻が異なるため）

		・隠れマルコフモデル
　			音声認識でよく使われていたモデル
			→音素を利用して単語を捜索（音素と単語を対応）
		・音素
　　		個別言語の中で同じとみなさせる音の集まり
　　		さんま、テント
			→同じ「ん」だが実は発音は異なっているが日本語では同一の「ん」として扱う
			→個別言語の中で同じとみなされる音の集まり
			音韻よりも小さい概念とされている

		・WaveNet
　			DeepMind社が開発した音声認識・音声合成を行う
			→ディープラーニングを活用したモデル
			→人間に近い自然な話し方をする
　			スマホやスマートスピカ―で活用されている
