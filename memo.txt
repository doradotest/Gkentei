:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
4.人工知能の定義
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
人工知能（Artifical  Intelligence）
	推論、認識、判断など人間のような知的なふるまいをする人工的に作られた機械のこと（計算機科学の一分野）
	★ジョン・マッカーシーがダートマス会議で使った言葉（1956）ダートマス大学
人工知能のおおまかな分類
	・ルールベースの制御（制御工学）
	・古典的な人工知能（人工知能）
	・機械学習（人工知能）
	・ディープラーニング（人工知能）
	自らの判断で自律的に振る舞うエージェントに基づき分類
ルールベースの制御
	あらかじめ決められた設定に基づいてふるまう
	冷蔵庫、エアコン、炊飯器など
古典的な人工知能
	多くのデータを活用して、複雑なふるまいが可能
	→医療診断プログラム、掃除ロボット（ルンバ）　など
機械学習を取り入れた人工知能
	大量のデータからルール・関係性などを学習する
	→検索エンジン、スパムメール判定、交通渋滞予測　など
ディープラーニング（人工知能）
	何に注目すればいいか（特徴量）を自らで判断し学習する
	→画像認識、音声認識、機械翻訳など
機械学習（犬の写真）
	何に注目するべきか（特徴）を教える必要があった
	→目、口、耳など犬としての特徴を設定する必要があった
AI効果
	人工知能とみなされていた技術で課題が実現されるとそれはただの自動化であって人工知能ではないと
人工知能とロボットの違い
	ロボットは予め決められた動作を行うのが基本になる
	→産業用のロボット、運搬ロボット、などの装置・機器（体）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
5.人工知能
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
世界初の汎用コンピュータ
	アメリカで世界初の汎用コンピュータ
	【エニアック（ENIAC）】をペンシルバニア大学が開発(1946)
人工知能研究のブームと冬の時代
	第一次人工知能ブーム：1950年代後半～1960年代
	第二次人工知能ブーム：1980年代
	第三次人工知能ブーム：2000年代～
第一次人工知能ブーム（推論・探索の時代）
	迷路の謎解きなど簡単な問題（トイ・プロブレム）はコンピュータが解けるようになった
第二次人工知能ブーム（知識の時代）
	特定の専門分野の知識をもとに、専門家のように様々な推論・判断を行うシステム（エキスパートシステム）
	大量の知識を与えるコスト・管理するコストが大きかった
第三次人工知能ブーム（機械学習・特徴表現学習の時代）
	大量のデータ（ビックデータ）を用いることで人工知能が自ら知識を獲得する機械学習が実用化された
	→データの特徴を定量的に表したもの（特徴量）を人工知能が自ら学習するディープラーニングが登場

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
6. 探索・推論（探索木）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
探索木
	１つ１つの要素をノードと呼ぶ
幅優先探索
	初期のノードから見て近いノードから探索を行う
	探索したノードをすべて記憶しなければならないためメモリが必要
	最短経路を見つけ出すことができる(ステップ数が少なければ)
	深い場所に解答がある場合、途中で記憶容量がパンクする
深さ優先探索
	あるノードから行き止まりになるまで行き、行き止まりになったら、
	１つ手前のノードまで戻りまた行き止まりになるまで探索を行う
	記憶するものが少なくメモリが少なくて済む
	最短経路とは限らない
ハノイの塔
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
7. 探索・推論（行動計画とボードゲーム）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ロボットの行動計画
	プランニング
	前提条件、行動、結果を組み合わせて１つの動作を定義する自動計画を記述する手法
	プランニングシステム、【STRIPS(ストリップ)】
SHRDLU(シュルドゥル)
	テリー・ウィノグラード
	コンピュータに英語で指示を出すと端末の画面上の物体を動かしてくれる（積み木）
	研究成果はCycプロジェクトに引き継がれていく
ボードゲーム
	組み合わせの数はオセロ、チェス、将棋、囲碁の順
	コスト（ステップ数）の概念を取り入れて探索を効率化しようとする
Mini-Max法
	自分が手を打つときは自分が有利に、相手が手を打つときは相手が不利
	自分も相手も最善手を打つ仮定のもとにスコアを逆算する
	あらかじめどのような手が有利か不利かをスコア化（コスト）
	すべての手を計算すると時間がかかるデメリットがある
αβ法
	Mini-Max法において、スコアが低い手を切り捨てる
	αカット、βカット
	切り捨てた中に実は最善の手があるかもしれない
	すべてのパターンを調べる方法（ブルートフォース）の方が勝てる確率は上がる
	Mini-Max法、αβ法では「相手は自分のスコアが小さい手をとる」
	Negaとつくと、「相手が相手自身のスコアが高くなるような手をとる」
ヒューリスティックな知識
	経験による知識やルールのこと
	精度は高くないが、ある程度正しい答えを出せる方法
モンテカルロ法
	ゲームの終盤まで進んだら、あらかじめ決めたスコアを無視して、AI同士でランダムな手を打って、結局（プレイアウト）させる
	複数回プレイアウトすると、勝ちやすい手などがわかりスコアをつけていく
	人間がスコアをつけるよりも理にかなっている
	DeepMind社が開発した人工知能の囲碁プログラムAlphaGoがプロに勝った(2016年)
	ディープラーニングの技術を使っている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
8. 知識表現
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
人工無能
	チャットボットなどと呼ばれるプログラム
	人間との形式的な会話のために作られたロボット
ELIZA（イライザ）
	１９６４年、チャットボットの元祖となるELIZAが開発される
イライザ効果
	コンピュータとのやりとりを、人間とやりとりしていると錯覚する現象のこと
知識ベースの構築とエキスパートシステム
	DENDRAL（世界初のエキスパートシステム）（読-デンドラル）
	1960年代の人工知能に関するプロジェクト
	→未知の有機化合物を特定するエキスパートシステム
	【知識ベースと推論エンジン】により構成
	・【マイシン（MYCIN）】1970年代
	スタンフォード大学で開発されたエキスパートシステム
	→感染した細菌を判断するためのシステム
	専門医＞マイシン（MYCIN）＞普通医
知識獲得のボトルネック
	専門家の暗黙知を聞き出すのが困難
意味ネットワーク（セマンティックネットワーク）
	「is-a」の関係：●は●である
	「part-of」の関係：●は●の一部である
	「has-a」の関係：●は●を持っている
	・is-a　継承関係を表す
	・part-of　属性を表す。本体と部品の関係
ナレッジグラフ
　　　意味ネットワークの中でも、インターネット上などから半自動で構築しているもの
	LOD（Linked　Open Data）
		ネットワーク形式で公開されているデータセット
		地方公共団体などではこのデータを活用したサービスを展開しているのも見られる
	Mercari
		ナレッジグラフを用いてレコメンドシステムにその理由を示すために使っている
Cycプロジェクト（サイク）
	一般常識もデータベース化して、人間と同様の推論システムを構築することを目的とするプロジェクト
	→1984年に開始され、30年以上も入力作業は続いている
オントロジー
	概念化の明示的・形式的な仕様、概念を体系化する学問
	意味ネットワークの単語と単語をつなぐつなぎ方のルールのこと
	オントロジーに従った記法で統一すれば共有できる
	→意味ネットワークを作成しても作者によって書き方などが異なるため他人が作成したものを利用できない
	→いずれにせよ、記述する人によって記述の方法や記述の粒度がまちまちになると、そもそもの知識の共有化や再利用が難しくなるため、記述ルールの標準化が必要
概念間の関係
	→推移律（AとB、BとCに関係が成立するとき、AとCも成立）
	・part-isの関係：全体と部分の関係、タイヤは車の一部
	→part-isの関係には5種類以上の意味があり複雑
	本来なら分けて関係を示したいが分けられていない

オントロジーの構築
　  ヘビーウェイトオントロジー
	    人間がしっかりと考えて知識を記述していくという考え方
		Cycプロジェクトが長年行っている
　  ライトウェイトオントロジー
	   コンピュータが情報を理解して、自主的に関係性を見つけようという考え方
ライトウェイトオントロジー
　  ウェブマイニング
	  ウェブ上にあるデータ分析して有益な情報を抽出する手法
　  データマイニング
	  ビックデータを分析して有益な情報を抽出する手法
ワトソン
	IBMが開発した質問応答（QUESTION-ANSWERING）のシステム
	→2011年アメリカのクイズ番組で対戦して優勝
	→蓄積されている大量のデータから最適な回答を見つけ出している
	IBMは人工知能ではなく、拡張知能と呼んでいる
	→言葉を理解していない（ライトウェイトオントロジー）
東ロボくん
	東大の入試を突破することを目標にした人工知能
	→2011年から2016年まで続けられた
	→偏差値57を突破したが、東大入試を突破することは現在では不可能であると判断されプロジェクト終了した
	→読解力が必要であり、知識だけで合格が困難であった

セマンティック・ウエブ
	メタデータを活用して、コンピュータがウェブ情報を解釈し処理する技術や考え方のこと
	→メタデータ：データに付与するデータのこと
	「五十嵐すず」というデータに対して「著者」というデータを付与することでコンピュータが理解しやすくなる
	例）写真の情報(Exif情報)
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
9. 機械学習・深層学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
レコメンデーションエンジン
	→ユーザーが興味がありそうなものを提案するシステム　アマゾン、YouTube
スパムフィルター
	→迷惑メールを見つけ出すためのシステム
自然言語処理
	自然言語をコンピュータに処理させること
統計的自然言語処理
	確率推論や統計学を使って、自然言語を処理すること
　  コーパス（自然言語の文章を構造化したもの、対訳データ）を活用することで文章を正しく読み取れる確率が上がる
ニューラルネットワーク
	人間の脳を模倣してモデル化されており、入力層、中間層、出力層の複数ニューロンから構成される非常に複雑な構成
ディープラーニング
	ニューラルネットワークを多層化したもの
	→多層化しても学習精度が思ったように上がらなかった
	→誤差逆伝播法などを取り入れることで学習精度があがる
	自己符号化器（オートエンコーダ）、
	活性化関数などを活用することで問題がすこしずつ解決
ILSVRC（ImageNet　Large　Scale　Visual Recognition　Challenge）
	画像認識の精度を競う大会
機械学習
	データの特徴を定量的に表した特徴量を人間が設定
	→人間の経験と勘によって特徴量を設定していた
	→2015年から人間の認識率を超える精度を誇るようになる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
10. 人工知能分野の問題①
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
トイ・プロブレム
	人工知能は複雑な問題を解くことができない
	ルールがシンプルなもの（ゲーム）しか解けないという用語
フレーム問題
	処理能力が限られている人工知能では、現実に起こりうる全ての問題に対応することができないこと
	問題の枠に囚われてしまい、枠の外を考慮することはロボットには難しい
	自動運転車は事故を必ず起こさないようにプログラムすると、走行速度が遅すぎてうまく機能しない問題
チューリングテスト
	機械は思考できるのかという目的から行われるテスト
	→アラン・チューリングが提案したもの
	→人間が審査員となり、人工知能と人間と会話を行うどちらが人間なのかを当てるテスト
	→人工知能は人間っぽいふるまいを行う
ローブナーコンテスト
	最も人間に近い人工知能を決める
	チューリングテスト形式のコンテストのこと
強いAI
	人間のようなふるまい（思考・判断など）をするAI
	汎用型人工知能
弱いAI
　　特定の問題を解決AI
	特化型人工知能
ジョン・サールが論文で発表したAIの分類
	→弱いAIは現実可能でも、強いAIは不可能であると考えている
	→「中国語の部屋」という思考実験を使って説明されている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
11. 人工知能分野の問題2
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
シンボルグラウンディング問題
	シンボル（記号）を現実世界の意味とどのように結びつけるかという問題のこと
	→スティーブン・ハルナッドによって推論された
	→概念と記号を結びつけることは難しい
	人間の場合、シマウマを、縞模様の馬かもしれないと考えることができる
	→記号のシマウマと実際のシマウマが結びつく
	→コンピュータの場合、記号のシマウマと実際のシマウマを結びつけることができない
身体性
	機械が身体を持つことで高度な人工知能になるという考え方
	→身体を通して得られる情報も必要になってくる
知識獲得のボトルネック
	人間が持っている大量の一般常識をコンピュータが獲得することは極めて難しいこと
	・機械翻訳
　		①ルールベース機械翻訳：1970年代後半から
　		②統計的機械翻訳：1990年代から
　　　		性能は一気に改善されたが、意味を理解しているわけではないので翻訳が大変になる
		③ニューラル機械翻訳
			ニューラルネットワークを活用した翻訳
			→精度が一気に上がり、実務でも使われるようになってきた
			TOEIC900点以上の人間と同じくらいの翻訳力をもつ
特徴量設計
	機械学習では、データの特徴を定量的に表した特徴量の選び方が大切になる
	→家賃を予想する場合、最寄り駅までの距離や築年数などが特徴量として考えられる
特徴量エンジニアリング
	取得済みデータから加工し抽出すること
特徴表現学習
	特徴量をコンピュタ自身が見つけること
	特徴量の加工・抽出も学習器にさせること
	→ディープラーニングも特徴表現学習の１つ
	→家賃などの情報を与え、機械が特徴量を抽出する
	人間は機械が何を特徴量にしたのかよくわからない
シンギュラリティ（技術的な特異点）
	人工知能の進化により、人類の知能を越える転換点のこと
	→人工知能が自分よりも賢い人工知能を作れるようになることで加速度的に人工知能の性能は飛躍すると予想
	→レイ・カーツワイルは、2045年にシンギュラリテイーが訪れることを予想している
	人工知能が人類よりも賢くなるのは2029年と予想している
	→シンギュラリティーが起きる年と人工知能が人類よりも賢くなる年は異なる
	→イーロンマスク、ビル・ゲイツは人工知能は脅威であると考えている
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
12. 学習の種類
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
教師あり学習
	正解が与えられたデータをもとに、ルールやパターンを学習して手法のこと
	→分類問題、回帰問題　を解くときに使われる手法になる
	・分類問題：複数のカテゴリに分ける問題
	・回帰問題：連続値（体重、温度など）を予測する問題
分類問題
　	迷惑メールか普通のメールかを分類する　など
回帰問題
　	来店数や温度、気温からビールの売上を予測する　など
教師無し学習
	クラスタリング
		購入データから顧客層に分ける　など
	次元削減
		数学、理科を理系、社会、国語を文系にまとめる　など
　　　　扱うべきデータが減り計算、分析しやすくなる
強化学習
　	与えられた環境下で、報酬が最大となる行動を行うように学習していく手法のこと
　「ブラックボックス的な環境の中で行動するエージェントが、得られる報酬を最大化するような状態に応じた行動を学習していく手法」
	→テレビゲームで点数が高くなるような行動をするために学習していく
	（最適な行動は何か）

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
13. 教師あり学習（線形回帰・ロジスティック回帰）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
・時系列分析
線形回帰
	説明変数：説明するための変数、原因になっている変数
　	目的変数：予測したい変数、結果になる変数
	→アイスクリームの売上（目的変数）と気温（説明変数）
	説明変数の数が複数になると。直線から平面になる
	３次元以上の平面を超平面という
	正規化などの前処理を行い説明変数のスケールを揃え、回帰係数の絶対値が大きいものが重要
多重共線性
	説明変数同士の相関が高すぎると、単独の影響を分離したり、その効果を評価したりすることが困難になる
	from sklearn.linear_model import LinearRegression　線形回帰の実行準備
	from sklearn.datasets import load_boston
	from sklearn.model_selection import train_test_split

	# Bostonデータセットを読み込む
	boston = load_boston()
	X, y = boston.data, boston.target
	# 学習データセットとテストデータセットに分割
	★X_trainは学習用の説明変数、y_trainは学習用の目的変数である。
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
	# 線形回帰をインスタンス化
	lr = LinearRegression()
	# 学習
	lr.fit(X_train, y_train)

最小二乗法
　	実際の値と予想値との差の2乗値が最小になるような重みを決定する方法
擬似相関
　　　因果関係がないのに相関に見えること
重回帰分析の注意点
	多重共線性
　		相関係数が高い説明変数があること
		→計算が不安定になってしまう特徴がある
正則化とは過学習を防ぐために使われるテクニック・手法
　	過学習は訓練データを学習しすぎて
　	訓練データ以外のデータに対して精度が悪くなってしまうこと
------------------------------------
ロジスティック回帰（分類問題）
	回帰と言う名前がついているが分類問題に適用されるモデル
　	複数の説明変数から、ある事象が発生する確率を予想する手法
	目的変数は０，１
	→複数の説明変数から、再検査が必要かどうか　な
	ジグモイド関数を用いて計算されている
		→結果が2択の時に用いられる
	→結果が3択以上の場合はソフトマックス関数を利用する
　	複数のクラスに分類することができる（多クラス分類）
	オッズ＝事象が起こる確率／事象が起こらない確率＝ｐ／１−ｐ
ある事象が起こる確率ｐの範囲は０〜１，１−ｐも同じ
ｐ／１−ｐの範囲は０〜∞（無限大）
log(ｐ／１−ｐ)の範囲は-∞〜∞
線形判別モデル

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
14.教師あり学習（ランダムフォレスト・ブースティング）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
決定木
	データを分割するルールを次々と作成していくことにより、分類を実行するアルゴリズム
	分類木：試合の勝ち負け　など
	回帰木：住宅の価格　など
	根ノード（root）
	葉ノード（一番下、子を持たない）
	分岐が多くなりすぎると1つの葉に１つのデータが対応してしまう（過学習）
	→過学習を防ぐためにも、木の深さ・幅に気を付ける必要がある
	情報利得が最大化になるように分岐させる
	・情報利得：分割前の不純度から分割後の不純度を引いたもの
				情報利得＝親ノードでの不純度　-　子ノードでの不純度の合計
	・不純度：１つのノードに異なるクラスのサンプルがどれだけ含まれているかという割合
			　ジニ不純度★　各ノードに間違ったクラスに振り分けられてしまう確率
		エントロピー、ジニ不純度、分散誤差などによって計算される
	〇決定木でデータを分割する時は、データの分割によってどれだけ得をするかについて考える。これを情報利得と呼ぶ。
	　　情報利得は親ノードの不純度から子ノードの不純度を差し引いたものとして定義される。
	〇決定木などで利用される木と呼ばれるデータ構造は頂点であるノード（利用回数、利用間隔）とそれらを結ぶエッジ（5回以上、5回未満）から構成される。★★
	　　木の最下部にあり子ノードを持たないノードはリーフと呼ばれる。
	Ｘ情報利得は【子ノードの不純度から親ノードの不純度を差し引いたもの】として定義される。 不純度の指標としてはジニ不純度、エントロピー、分類誤差などが用いられる。

		from sklearn.datasets import load_iris
		from sklearn.model_selection import train_test_split
		from sklearn.tree import DecisionTreeClassifier　
		# Irisデータセットを読み込む
		iris = load_iris()
		X, y = iris.data, iris.target
		# 学習データセットとテストデータに分割する
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
		# 決定木をインスタンス化する (木の最大深さ=3)
		tree = DecisionTreeClassifier(max_depth=3)
		# 学習
		tree.fit(X_train, y_train)

		決定木の描画
		　pip install pydotplus

		from pydotplus import graph_from_dot_data
		from sklearn.tree import export_graphviz
		# dot形式のデータを抽出
		dot_data = export_graphviz(tree, filled=True,
		                rounded=True,
		                class_names=['Setosa',
		                     'Versicolor',
		                     'Virigica'],
		                feature_names=['Speal Length',
		                               'Spal Width',
		                               'Petal Length',
		                               'Petal Width'],
		                out_file=None)
		# 決定木のプロットを出力
		graph = graph_from_dot_data(dot_data)
		graph.write_png('tree.png')

		# 予測
		y_pred = tree.predict(X_test)
		y_pred

ランダムフォレスト（分類と回帰）
　	ランダムに選ばれた訓練データで学習した決定木を複数作成し、決定木で予想された結果の多数決でクラスを決める手法
	→回帰では平均値を求める
	→精度の悪いものがあったとしても複数組み合わせると精度が高いものになるという考え方にもとづく
	〇scikit-learnでランダムフォレストを実行するにはensembleモジュールのRandomForestClassifierクラスを使用する。
　		パラメータで決定木の個数を指定することもできる。
	〇★★ランダムフォレストは、ブートストラップデータを用いて決定木を構築する処理を複数回繰り返し、
　　　各木の推定結果の多数決や平均値により分類・回帰を行う手法であり、アンサンブル学習の1つである。
	〇ランダムフォレストは、データのサンプルと特徴量をランダムに選択して決定木を構築する処理を複数回繰り返し、
　		各木の推定結果の多数決や平均値により分類・回帰を行う手法である。

		from sklearn.ensemble import RandomForestClassifier
		# ランダムフォレストをインスタンス化する
		forest = RandomForestClassifier(n_estimators=100, random_state=123)
		# 学習
		forest.fit(X_train, y_train)
		# 予測
		y_pred = forest.predict(X_test)
		y_pred

アンサンブル学習
　	別々の学習器を組み合わせる学習器を作ること
	→バギング、ブースティングという手法がある
	・バギング
　　	複数のモデルを並行的に学習させていく方法（復元抽出）
		→ランダムフォレストはバギングの１つで学習時間が短いのが特徴
	・ブースティング
		複数のモデルを直列的に学習させていく方法
		→前に作成したモデルを修正して新しいモデルを作成していく
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
15. 教師あり学習（サポートベクターマシン・k近傍法）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
サポートベクターマシン（SVM）（分類と回帰）
	主に分類で使われるアルゴリズム
	明らかに所属クラスが分かる観測ではなく、判別境界の付近にある判別の難しい学習データに着目する分類モデル
	→マージン最大化する線を求めることで分類を行う
	マージン
　		線（決定境界）とクラスの各データの距離のこと
	サポートベクトル
		判別境界に最も近い学習データ
	ハードマージン
　		マージンの中にデータが入ることが禁止
	ソフトマージン
　		マージンの中にデータが入ることを許容
	→スラック変数：どの程度強要するか調整
	〇サポートベクタマシンは、【分類・回帰だけでなく】外れ値検出にも使えるアルゴリズムであり、直線や平面などで分離できないデータであっても、
　		高次元の空間に写して線形分離することにより分類を行うことを可能にする。
	Ｘサポートベクタマシンは、分類のみに適用できるアルゴリズムである。直線や平面で線形分離できないデータも高次元の空間に写像して線形分離することで分類を行うことを可能にする。
	Ｘサポートベクタマシンで決定境界を求める際にマージンを最小にする理由は、決定境界がサポートベクタから近くなり、多少のデータが変わっても誤った分類を行う可能性を低くできると期待できるからである。

	サポートベクタマシンの決定境界は、マージンを（「最小」ではなく）最大にすることで求めます。
		ハイパーパラメーターのCの値が小さいほど、マージンは大きくなる

カーネル法を組み合わせることで直線で分類できないものでも分類ができる
 	次元を拡張させる関数をカーネル関数という
	データを高次元に写像することで線形分離を可能にする
	→カーネルトリックと呼ばれる方法を使うことで
　		計算を楽にすることができる（次元が増えると計算が大変になる）
k近傍法（分類と回帰）
	分類に使われる手法の１つ
	特徴量のスケールを予め揃えておく必要がある
	→未知のデータが与えられたときに近くにあるデータからクラスを分類していく手法のこと
	→ｋには個数が入る
	最近傍法
		ｋ＝１のこと。予測する観測は最も近いデータの値が出力
		複雑で入り組んだものになる
		ｋが大きくなるほどたくさんの観測の平均や多数決が行われるので単純で明快なものになる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
16. 教師あり学習（ニューラルネットワーク・時系列分析）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ニューラルネットワーク
	人間の神経回路（ニューロン）を模倣した数理モデル
単純パーセプトロン
　	1958年に提案されたニューラルネットワーク
	→入力層と出力層からなる
	入力値の重要度を数値化したものを重みという
	モデルの自由度を上げるためにバイアス(b)を使用する
	活性化関数 次の層に渡す値を調整する関数をという
		シグモイド関数（結果2択）
		ソフトマックス関数（結果が3択以上）
		ReLU関数　　など
多層パーセプトロン
	→非線形分類が可能になった
	→入力層、隠れ層、出力層に分かれている
	層が増えることで重みも増える
	→誤差逆伝播法を活用して重みを調整
	誤差逆伝播法
	出力層から入力層にかけて重みを調整する方法のこと
自己回帰モデル（ARモデル）
	回帰問題に使われる手法（時系列系のデータ）
	→過去のデータを使ってある時点の値を予測するモデル
ベクトル自己回帰モデル（ＶＡＲモデル）
	自己回帰モデルを多変量に拡張したもの
勾配ブースティング
	多数の弱いモデルで強いモデルを作るアンサンブル学習であるブースティングの１つ
	中でも弱いモデルに決定木を利用したものは、テーブルデータを対象とした回帰問題や分類問題で高い精度を示す
畳み込みニューラルネットワーク
	特に画像領域で用いられる
汎化能力
	未知のデータに対する対応能力
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
17. 教師なし学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

クラスタリング（クラスタ分析）
	データをグループ（クラスタ）分けする手法のこと
	ある基準を設定してデータ間の類似性を計算し、データをクラスタ（グループ）にまとめるタスク
	→大量のデータをコンピュータが自動的に分類してくれる
	グループ分けされたものは人間が見て解釈する必要がある
	Ｘクラスタリングは、ある基準を設定してデータ間の類似性を計算し、データをクラス分類するタスクであり、教師あり学習の典型的なタスクである。
	Ｘクラスタリングにおいては、得られたクラスタの妥当性に絶対的な答えがある。
	〇クラスタリングは、ある基準を設定してデータ間の類似性を計算し、データをクラスタ（グループ）にまとめるタスクであり、教師なし学習の典型的なタスクである。
	・K-means法
	・ウォード法
	・主成分分析
	・ｔ-ＳＮＥ法
	・協調フィルタリング
K-means法（階層なしクラスタリング）
	あらかじめ設定したクラスタの数（k個）にデータを分類方法
	→重心を活用して分類を行っていく方法
	〇k-meansは、【最初にランダムにクラスタ中心を割り当て】、クラスタ中心を各データとの距離を計算しながら修正し
	  、最終的なクラスタ中心が収束するまで再計算を行いクラスタリングする手法である。
	〇k-meansは、scikit-learnのclusterモジュールのKMeansクラスを用いることによって実行することができる。
	〇クラスタリングにおいては、得られたクラスタの妥当性に絶対的な答えはない。

		from sklearn.cluster import KMeans
		# クラスタの数を3とするKMeansのインスタンスを生成
		km = KMeans(n_clusters=3, init='random', n_init=10, random_state=123)　　　#n_initはk-meansの実行回数★★
		# KMeansを実行
		y_km = km.fit_predict(X)

ウォード法（階層ありクラスタリング）
	似ている組み合わせから順番にクラスター化していく方法
	途中経過を樹形図（テンドログラム）で表すことができる

階層クラスタリング
	〇凝集型の階層的クラスタリングは、まず似ているデータをまとめて小さなクラスタを作り、次にそのクラスタと似ているデータをまとめ、
	　最終的にデータが1つのクラスタにまとめられるまで処理を繰り返すクラスタリング手法である。
	Ｘ凝集型の階層的クラスタリングは、最初にすべてのデータが一つのクラスタに所属していると考え、順次クラスタを分割していく手法である。
	〇分割型の階層的クラスタリングは、【最初にすべてのデータが1つのクラスタに所属している】と考え、順次クラスタを分割していくアプローチであり、
	凝集型より利用頻度が多いとは言えません。
	分割型　似ていないデータ同志を分割していく手法であり計算量が多いため、凝集型より利用頻度が多いとは言えません
	Ｘ分割型の階層的クラスタリングは、最初にすべてのデータが1つのクラスタに所属していると考え、順次クラスタを分割していくアプローチであり、
　　　一般的に凝集型に比べて計算量が少なく精度が高いためよく用いられる。

		from sklearn.cluster import AgglomerativeClustering
		# 擬集型の階層クラスタリングのインスタンスを作成
		ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')
		# クラスタリングを実行し、各クラスのクラスタ番号を取得
		labels = ac.fit_predict(X)
		labels

主成分分析（PCA）
　	データの特徴量の関係性（相関関係）から変数をまとめてより少ない変数を要約すること
	→データを要約することを次元削減といい、計算量を減らせる
	→まとめられた特徴量を主成分と呼ぶ

	英語、理科、数学、社会、国語のテストを実行
	→それぞれの科目を分析して２つの成分にする場合、理系と文系という風に分けることができる
	高次元のデータに対して分散が大きくなる方向（データが散らばっている方向）を探して、元の次元と同じかそれよりも低い次元にデータを変換する手法
	分散・ばらつきが大きくなる方向を探して、元の次元と同じか、低い次元にデータを変換・圧縮する、次元削減の一手法
	主成分分析は、scikit-learnのdecompositonモジュールのPCAクラスを用いて実行することができる。
	2次元のデータに対して主成分分析を行い、どちらも重要であると確認できた場合には、1次元に次元削減できる可能性が高いとはいえません。
	Ｘ主成分分析とは、高次元のデータに対して標準偏差が小さくなる方向を探して、元の次元と同じかそれよりも高い次元にデータを変換する手法である。

次元削減の方法
	主成分分析以外にもｔ分布を使って次元削減する
	t-SNE法、特異値分解（SVD）、多次元尺度構成法（MDS）が有名
	低次元（小）-＞高次元（多）
　　「圧縮」するタスク
　　　計算量の削減は、（目的変数ではなく）説明変数の次元数を減らすこと
	Ｘ次元削減は、データが持っている情報をなるべく損ねることなく次元を削減してデータを展開(〇圧縮)するタスクである。
	Ｘ次元削減の主目的は目的変数の数を減らして計算量を削減することであるが、説明変数の削減を行うことは少ない。これは、モデルの精度を確保するためである。
	特徴量の次元削減、なるべくデータの情報を落とさずに、少ない特徴量でデータを表現するために用いられる
	不動産分析で、延床面積、敷地面積、間口を広さとする
協調フィルタリング
	レコメンデーションで使われる手法の１つ
	→AMAzonなどECサイトでおススメが表示される仕組みのこと
	→自分と類にしたユーザーが購入した商品を薦める
	　動画配信ならば類にしたユーザーが見た動画を薦める
	→ある程度のデータが必要になる（コールドスタート問題）
	・コンテンツベースフィルタリング
	　	商品の特徴を利用して類にした商品を薦める方法
		→コールドスタート問題を回避することができる
		→それぞれにメリットとデメリットが存在する
コールドスタート問題
	新商品、行動履歴がなくてレコメンドできない問題をどうする
	「新登場のアイテムは行動履歴がある程度付くまでレコメンドの対象にならない」というもの
トピックモデル
	クラスタリングで使われるモデルの１つ
	→K-means法などと異なり複数のクラスタに分類が可能
	　潜在的ディリクレ分配法（LDA）が有名
	→ニュース記事を複数のカテゴリに分類したりレコメンダシステムなどで活用したりできる
	・ソフトクラスタリング
	　	各データが１以上のクラスタに所属するようなクラスタリング
	・ハードクラスタリング
	　	各データが１つのクラスタに所属するようなクラスタリング
半教師あり学習
　	学習の途中まで教師あり学習で、その後、教師なし学習を行う学習
	→少量のラベル付きデータを学習し、
　		ラベルがついていない大量のデータを学習するアプローチ
	→大量のラベル付きデータを集めるのはコストがかかりすぎる
		ラベル：データに付与される正解かどうかを表す情報
オートエンコーダ
	ニューラルネットワークを用いた次元削減で、入力層と出力層の次元を同じにし、入力を再現するように出力を行わせる
	より低次元の中間層では、できるだけ情報を失わないような（出力で復元できるような）次元圧縮が行われている
敵対的生成ネットワーク
	主に画像生成するためのネットワーク
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
18. 強化学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
バンディットアルゴリズム
強化学習
	コンピュータ（エージェント）が試行錯誤しながら最適な行動を学習する手法のこと
	→状態、行動、報酬が大切なキーワードになる

｛エージェント｝　←　状態　　　｛環境｝
　　　　　　　　行動　→
　　　　　　　　←　報酬
	→短期的な報酬だけで行動を決定しているわけではない
	　行動に対して毎回報酬がなくても学習可能（囲碁やあ将棋など）
	・探索と活用のジレンマ
　		探索：知っていること以外の行動を行う
　		活用：既存情報を使って最適な行動を行うこと
		→活用だけだと、良い行動があるかもしれないが発見できない
　		　探索だけだと、必要のいない行動が多くなり時間がかかる

	・ε-greedy方策：一定確率で探索を選ぶ
	・UCB方策：探索では今まで選択されていない行動を選択

マルコフ決定過程モデル
	遷移過程がマルコフ性を満たすモデルのこと
	マルコフ性
		状態が遷移するとき、過去の状態に依存しない特定のこと
		→本来ならば過去の状態を考慮して行動を選択するが計算が複雑になってしまう
方策
　	最適な方策を見つけ出すことは難しい
	1.	状態や行動の価値を計算して最適な行動を選択（価値ベース）
	2.	方策をパラメータのもった関数と定義して方策の価値が最大になるようにパラメータを学習させる（方策ベース）
価値関数
	将来的に見込める報酬を求める関数
	・行動価値関数
	・状態価値関数
	→未来の報酬はもらえるのかどうかわからないので割り引いて考える
	（割引率：0～1）
行動価値関数
　	ある状態において行動した結果、将来的に得られる報酬の合計を返す関数
状態価値関数
　	ある状態において、将来的に得られる報酬の合計を返す関数
	Q値
　		行動価値関数から返ってきた値のこと
	Q学習
　		Q値が最も高くなるように行動をするように学習するとこと。
方策勾配
　	方策勾配法
　　	方策をパラメータのもった関数と定義して
　　	方策の価値が最大になるようにパラメータを学習させていくことで最適な方策を見つけるようにする方法
　	ＲＥＩＮＦＯＲＣＥ
　　アルファ碁にも活用されているアルゴリズム
	→方策ベースの手法
	・Actor-Critic
　　	行動（ACtor）と評価するCriticで構成されている
	→Criticが行動を評価して方策を更新していく
	→価値ベースと方策ベースの手法
	→Actor-Criticを応用したA3Cがある
深層強化学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
19. モデルの評価手法
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
→データを訓練データ・テストデータに分割して検証することを交差検証と呼ぶ
交差検証（クロスバリデーション）の種類
	・ホールドアウト検証
	・ｋ-分割交差検証
	・１つ抜き（Leave　One　Out）法
ホールドアウト検証
	準備したデータを訓練データとテストデータに分ける
	→テスト用データを活用してモデルの性能を評価
	→データが少ない場合、評価が高くなってしまうことがある
ｋ-分割交差検証
	準備したデータを複数個に分割し、それぞれのデータを使い学習と評価を行う方法
	→データ数が少ない場合にたまたま評価が高くなってしまう可能性を防ぐために使われる検証方法になる
	9割学習用データ、1割テストデータにする処理を10回行う＝10分割交差検証

		from sklearn.svm import SVC
		from sklearn.model_selection import cross_val_score
		# サポートベクタマシンをインスタンス化
		svc = SVC()
		# 10分割の交差検証を実行
		cross_val_score(svc, X, y, cv=10, scoring='precision')　　適合率（precision）　

		＊引数cvに、10分割の交差検証を指定

１つ抜き（Leave　One　Out）法
	データ全体のうち１つをテストデータにする方法
	→訓練データとテストデータを入れ替えてテストを行う
	→データ数分繰り返し行う（ジャックナイフ法とも呼ばれる）
データリーケージ
	訓練データ・検証データにテストデータにすべき
	未知のデータが入ってしまうこと
	→時系列データをもとにモデルを作るときは
　	未来のデータが入るのはよくない（過去のデータの学習）
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
20.分類モデルの評価
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
→正解・不正解をまとめた表を混同行列という

画像を犬か、それ以外に分けるテストを行った
１．「犬」の画像を、正しく「犬」と判断（真陽性）
２．「犬」の画像を、間違って「犬以外」と判断（偽陰性）
３．「犬以外」の画像を、間違って「犬」と判断（偽陽性）
４．「犬以外」の画像を、正しく「犬以外」と判断（真陰性）
					犬と判断　　　　犬以外と判断
犬（Positive）　　　真陽性　TP　　　　偽陰性　FN
犬以外（Negative）　偽陽性　FP　　　　真陰性　TN

正解率＝(TP+TN) / (all)
	全データのうち正しく予測できた割合

適合率=TP / (TP+FP)
	陽性と予測したデータのうち、正しい予測の割合

再現率=TP / (TP + FN)
	陽性データの中で、正しい予測ができた割合

F値= (２ｘ適合率ｘ再現率) / (適合率　＋　再現率)
	適合率と再現率の調和平均
正解率（accuracy）予測と実績が一致したもの　(tp+tn)/(全体)
評価したい項目に合わせて評価指標を選ぶことが大切
→値が高いからといって性能が高いわけではない
→不良品率0.1％を判別するモデルを作り、正解率99.9％だった
→すべてを正しい製品だと判別し、不良品を見つけていなかった

	混合行列　予測と実績のクラスレベルの組合せを集計した表
	＊混同行列で計算する適合率、再現率、F値、正解率は、（回帰ではなく）分類モデルの評価指標★★★★
	＊適合率は予想するクラスをなるべく間違えないようにしたいときに重視する指標である。★★★★★★★★★★★★
	＊正解率は、正例か負例かを問わず、予測と実績が一致したデータの割合を表す。正解率は(tp+tn)/(tp+fp+fn+tn）で計算することができる。★★★★★★★★★★★★
	＊F値は、適合率と再現率の【調和平均【として定義される。F値は2*適合率*再現率/(適合率+再現率)で計算することができる。★★★★
	Ｘ★★機械学習を用いて構築した【回帰モデルの良し悪し】を評価する指標に適合率、再現率、F値、正解率がある。これらは【混同行列】から計算する。
	〇適合率、再現率、F値、正解率は、機械学習を用いて構築した【分類モデルの良し悪し】を評価する指標であり、混同行列から計算する。
	〇一般的に適合率と再現率はトレードオフの関係にある。つまり、どちらか一方の指標を高くすると、もう一方の指標は低くなる。
	ｘ適合率は、たとえば病院の検診で病気の見逃し・取りこぼしがないようにしたい場合などに重視される、網羅性に関する指標である。
	ｘ再現率は、間違えることをできるだけ避けたい場合に重視する指標である。一般的に適合率と再現率はトレードオフの関係にある。

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
21. 過学習
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
汎化性能
　未知のデータに対する精度のこと（精度は汎化誤差で測る）
訓練誤差：訓練データに対する予測と正解の誤差
汎化誤差：未知データに対する予測と正解の誤差
過学習を防ぐテクニック
	・アンサンブル学習
	・ドロップアウト・早期終了・バッチ正規化
	→ディープラーニングの章で詳しく解説

	・データ拡張
　　	データのバリエーションを増やす（データの水増し）
	・スパース化
　　	説明変数（特徴量）を減らす
	・正則化
	など
正則化
　	過学習を防ぐために、パラメータに制限をかけること
	学習の目的関数にペナルティとなる項目を追加することで、パラメータが極端な値になることを防ぐ
	→正則化しすぎると未知のデータに対する精度が低下する
　	未学習（学習不足）になってしまうので注意が必要
	→正則化にはL1正則化、L2正則化などがある
	L1正則化／ラッソ回帰（Lasso）
　　	特定のパラメータの値を０にして、選択
		正則化パラメータを大きくするに従い、各回帰係数が０になるものが増える
	L2正則化／リッジ回帰（Ridge）
　		パラメータの大きさに応じて０に近づけて滑らかなモデルにする
	Elasti Net
　　	ラッソ回帰とリッジ回帰を組み合わせたもの
	スパースデータ　要素の０が多くなるデータのこと
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
22. ROC曲線とモデル解釈・選択
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ROC曲線
	真陽性率（TPR）と偽陽性率（FPR）の関係を表した曲線
	・真陽性率（TPR）＝TP　/　（TP＋FP）
	・偽陽性率（FPR）＝FP　/　（FP＋TN）
	→横軸をFPR、縦軸をTPR
ROC曲線とAUC
	Positiveと判断　　Negativeと判断
	Positive　　TP　　　　　　　　　FN
	Negative　　FP　　　　　　　　TN
AUC
　	ROC曲線よりも下の面積のこと
	→面積が１に近いほどモデルの性能は高いとされる
モデルの解釈
　	モデルが何かを予測したとき、どのような根拠を持って予測したのかを知ることも大切になる
	→モデルが複雑になればなるほど根拠はわかりにくくなる
	→モデルの局所的な解釈を可能にするアプローチがある
　	LIME、　SHAPなどの手法が考えられている
LIME
	複雑なモデルを解釈しやすいモデル（線形回帰）に近似させる
SHAP
　　協力ゲーム理論におけるShapley値を利用してそれぞれの特徴量が予測にどの程度を与えたかを算出
モデルの選択と情報量
	タスクに対して最適なモデルを選択していく
	→複雑なモデルを選択すれば複雑なタスクを実行できる可能性は上がるがコストも増大してしまう
	→「ある事柄を説明するためには、必要以上に多くを仮定するべきではない」（オッカムの剃刀）

・赤池情報量基準（AIC）
　　「モデルの複雑さと、データとの適合度とのバランス」を知るときに使われる指標の１つ
	→モデルをどの程度複雑にすればいいのか目安になる
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
23. ニューラルネットワークとディープラーニング
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
単純パーセプトロン
　	1958年提案されたニューラルネットワーク
　	活性化関数としてステップ関数が用いられる
多層パーセプトロン
　	多層化したニューラルネットワークのこと
　	入力層→隠れ層→出力層
	→非線形分類が可能になった
　	単純パーセプトロンは線形分類だけ
	→入力層、隠れ層、出力層に分かれている
　	隠れ層は中間層とも呼ばれている
ディープラーニング
　	隠れ層を増やしたニューラルネットワーク（深層）を活用
　	層を深くすることで解ける問題が増えると考えたが、思ったような結果が得られなかった
	→信用割当問題・勾配消失問題など様々な問題を抱えていたため
	→問題が解決されたことで解ける問題の幅は広がった
信用割当問題
	最終的な結果に対して、どのパラメーターに責任があるのか、
	どのパラメーターを修正すればいいのかという問題
	→誤差があったとき何を修正すればいいのかという問題
	→誤差逆伝播法により信用割当問題は解決されたとされている
勾配消失問題
　	・誤差逆伝播法
　　　出力された結果と実際の結果の誤差を最小化するために出力層から入力層にかけて調整を行っていく方法

	→パラメーターを修正することで正しい値を高確率で導き出せるように
　		層が増えることで、入力層付近の調整が上手くいかない
	→誤差逆伝播法でパラメータを変更するとき活性化関数を微分した値を使用する
	→シグモイド関数の特性が影響している（0～1）
	→シグモイド関数の微分した値の範囲は（0～0.25）

	層が増えることで修正する値が小さくなってしまう
	→入力層付近では実質的に修正ができない状態になってしまう
	→なぜ微分を使っているのかにちては割愛
　	数学が得意な人（大学レベル）に説明しても誤差逆伝播だけで1時間から2時間は必要になってしまう上にテストで出ない


